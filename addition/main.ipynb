{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from numba import njit, prange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from data import addition_dataset\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "from preprocessing import OneHot, Encoding\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import to_automaton_history, reduce_automaton, to_automaton\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "from visualization.automata import AutomatonAnimation, display_automata\n",
    "from visualization.epochs import EpochAnimation\n",
    "\n",
    "import publication\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [str(n) for n in range(10)] + [\"+\", \" \"]\n",
    "onehot = OneHot(symbols)\n",
    "\n",
    "\n",
    "class SequenceEncoder(Encoding):\n",
    "    def __init__(self, single_encoder: Encoding):\n",
    "        self._decoding = lambda x: x\n",
    "        self.single_encoder = single_encoder\n",
    "\n",
    "    # @property\n",
    "    # def symbols(self) -> list:\n",
    "    #     return None\n",
    "\n",
    "    # def _update_decoding(self, encoding):\n",
    "    #     pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        encoded = [self.single_encoder(char) for char in data]\n",
    "        encoded = encoded[::-1]  # decoded in reverse for better computation\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, enc_data):\n",
    "        enc_data = enc_data.to(\"cpu\").detach()\n",
    "        decoded = [self.single_encoder.decode(vec) for vec in enc_data]\n",
    "        decoded = \"\".join(decoded)\n",
    "        decoded = decoded[::-1]\n",
    "        return decoded\n",
    "\n",
    "\n",
    "encoder = SequenceEncoder(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "full_length = 100\n",
    "max_train_length = 20\n",
    "test_length = 30\n",
    "\n",
    "test_data = addition_dataset(\n",
    "    device, encoder, n_datapoints=100, seq_len=[test_length], full_length=full_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm_encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm_decoder = nn.LSTM(\n",
    "            input_size=hidden_size * 2,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h = self.lstm_encoder(x)\n",
    "        x, _ = self.lstm_decoder(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=-1)\n",
    "        return x, h\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    device,\n",
    "    input_size=len(symbols),\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    output_size=len(symbols),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, model):\n",
    "    out = model(input)[0]\n",
    "    pred = torch.argmax(out, axis=1).int()\n",
    "    string = \"\".join([symbols[index] for index in pred])\n",
    "\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "\n",
    "\n",
    "def score(dataset, model):\n",
    "    count = 0\n",
    "    n_datapoints = len(dataset)\n",
    "    for n in prange(n_datapoints):\n",
    "        x, y = dataset[n]\n",
    "        target = encoder.decode(y)\n",
    "        prediction = predict(x, model)\n",
    "        if target == prediction:\n",
    "            count += 1\n",
    "    acc = count / len(dataset)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses, train_scores, test_scores = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 0.2420, test RMSE 0.2444, train score 0.79, test score 0.32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Train\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Generate data\n",
    "    train_data = addition_dataset(\n",
    "        device,\n",
    "        encoder,\n",
    "        n_datapoints=1000,\n",
    "        seq_len=[x for x in range(3, max_train_length) if x != test_length],\n",
    "        full_length=full_length,\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=32)\n",
    "\n",
    "    # Validation\n",
    "    if epoch % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(train_data.tensors[0])[0]\n",
    "            train_rmse = np.sqrt(loss_fn(y_pred, train_data.tensors[1]).cpu())\n",
    "            y_pred = model(test_data.tensors[0])[0]\n",
    "            test_rmse = np.sqrt(loss_fn(y_pred, test_data.tensors[1]).cpu())\n",
    "            train_score = score(\n",
    "                torch.utils.data.Subset(train_data, np.arange(100)), model\n",
    "            )\n",
    "            test_score = score(test_data, model)\n",
    "            train_losses.append(train_rmse)\n",
    "            test_losses.append(test_rmse)\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "        print(\n",
    "            f\"Epoch {epoch}: train RMSE {train_rmse:.4f}, test RMSE {test_rmse:.4f}, train score {train_score:.2f}, test score {test_score:.2f}\"\n",
    "        )\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)[0]\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss curves\n",
    "c = 1\n",
    "# publication.set_color_mixed()\n",
    "plt.plot(c * np.array(train_losses), label=\"Train loss\")\n",
    "plt.plot(c * np.array(test_losses), label=\"Test loss\")\n",
    "plt.plot(train_scores, label=\"Train score\")\n",
    "plt.plot(test_scores, label=\"Test score\")\n",
    "plt.legend()\n",
    "# publication.plt_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example length: 16\n",
      "input: 788778+878787878                                                                                                    \n",
      "target: 879576656\n",
      "prediction: 789656666                                                                                                           \n",
      "Wrong!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3572487/3063543233.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1716578970623/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  encoded = torch.tensor(encoder(example), device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "int_a = 788778\n",
    "int_b = 878787878\n",
    "target = int_a + int_b\n",
    "example = f\"{int_a}+{int_b}\"\n",
    "\n",
    "print(f\"Example length: {len(example)}\")\n",
    "\n",
    "computation_length = len(example) + full_length\n",
    "example = example + \" \" * (computation_length - len(example))\n",
    "encoded = torch.tensor(encoder(example), device=device, dtype=torch.float32)\n",
    "prediction = predict(encoded, model)\n",
    "\n",
    "print(f\"input: {example}\")\n",
    "print(f\"target: {target}\")\n",
    "print(f\"prediction: {prediction}\")\n",
    "print(\"Correct!\" if int(prediction) == target else \"Wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for large sequence length generalization\n",
    "\n",
    "# Evaluate model generalization for sequences of varying lengths\n",
    "N = 100\n",
    "val_scores = []\n",
    "for n in trange(3, N + 1):\n",
    "    val_data = addition_dataset(\n",
    "        device, encoder, n_datapoints=10, seq_len=[n], full_length=full_length\n",
    "    )\n",
    "    val_score = score(val_data, model)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# Visualize validation error for varying sequence lengths\n",
    "# publication.set_color_mixed()\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.bar(np.arange(3, N + 1), val_scores, color=\"skyblue\")\n",
    "ax.set_xlabel(\"Sequence lengths\")\n",
    "ax.set_ylabel(\"Validation error\")\n",
    "ax.set_title(\"Model Generalization Across Sequence Lengths\")\n",
    "ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "# publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "model(torch.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2, 100), indices imply (100, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m onehot\n\u001b[1;32m      3\u001b[0m initial_hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,model\u001b[38;5;241m.\u001b[39mlstm_encoder\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mto_automaton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_hidden\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rnn_structure/addition/../source/data_analysis/automata.py:318\u001b[0m, in \u001b[0;36mto_automaton\u001b[0;34m(model, datasets, initial_hidden)\u001b[0m\n\u001b[1;32m    311\u001b[0m hidden_tracker \u001b[38;5;241m=\u001b[39m ActivationTracker(\n\u001b[1;32m    312\u001b[0m     model,\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    314\u001b[0m     datasets,\n\u001b[1;32m    315\u001b[0m     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: initial_hidden,\n\u001b[1;32m    316\u001b[0m )\n\u001b[1;32m    317\u001b[0m out_tracker \u001b[38;5;241m=\u001b[39m ActivationTracker(model, \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m0\u001b[39m], datasets)\n\u001b[0;32m--> 318\u001b[0m \u001b[43mhidden_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m out_tracker\u001b[38;5;241m.\u001b[39mtrack()\n\u001b[1;32m    320\u001b[0m data_hid \u001b[38;5;241m=\u001b[39m hidden_tracker\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/projects/rnn_structure/addition/../source/compilation.py:122\u001b[0m, in \u001b[0;36mActivationTracker.track\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store the data of this epoch. Should be called each epoch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     act_this_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         initial_hidden \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    131\u001b[0m             index\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m\"\u001b[39m])],\n\u001b[1;32m    132\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/rnn_structure/addition/../source/activations.py:60\u001b[0m, in \u001b[0;36mget_activations\u001b[0;34m(datasets, output_function, encoding)\u001b[0m\n\u001b[1;32m     57\u001b[0m         activations_this_dataset \u001b[38;5;241m=\u001b[39m activations_this_dataset\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# Store activations in DataFrame\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m         activation_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations_this_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         activations\u001b[38;5;241m.\u001b[39mappend(activation_df)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Combine activations into a single DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (2, 100), indices imply (100, 100)"
     ]
    }
   ],
   "source": [
    "\n",
    "model.encoding = onehot\n",
    "\n",
    "initial_hidden = torch.zeros(size=(1,model.lstm_encoder.hidden_size))\n",
    "\n",
    "to_automaton(model, train_data, initial_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_structure",
   "language": "python",
   "name": "rnn_structure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
