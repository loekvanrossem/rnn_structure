{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import trange\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "\n",
    "source = \"/home/loek/projects/rnn/source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data, grid_data\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, Tracker, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import to_automaton_history\n",
    "from data_analysis.visualization.animation import SliderAnimation\n",
    "from data_analysis.visualization.activations import ActivationsAnimation\n",
    "from data_analysis.visualization.automata import AutomatonAnimation\n",
    "from data_analysis.visualization.epochs import EpochAnimation\n",
    "\n",
    "from model import Model\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "symbols = [0, 1]\n",
    "encoding = OneHot(symbols)\n",
    "xor_problem = lambda seq: np.sum(seq) % 2\n",
    "par_3_problem = lambda seq: int(np.sum(seq) % 3 == 2)\n",
    "\n",
    "problem = xor_problem\n",
    "\n",
    "N = 15\n",
    "\n",
    "train_datasets = [\n",
    "    seq_data(device, problem, encoding, seq_len=1, n_datapoints=N),\n",
    "    seq_data(device, problem, encoding, seq_len=2, n_datapoints=N),\n",
    "    seq_data(device, problem, encoding, seq_len=3, n_datapoints=N),\n",
    "    seq_data(device, problem, encoding, seq_len=4, n_datapoints=N),\n",
    "    # seq_data(device, problem, encoding, seq_len=5, n_datapoints=N),\n",
    "    # seq_data(device, problem, encoding, seq_len=6, n_datapoints=N),\n",
    "    # seq_data(device, problem, encoding, seq_len=7, n_datapoints=N),\n",
    "    # seq_data(device, problem, encoding, seq_len=8, n_datapoints=N),\n",
    "    # seq_data(device, problem, encoding, seq_len=9, n_datapoints=N),\n",
    "]\n",
    "\n",
    "val_dataset = [seq_data(device, problem, encoding, n_datapoints=50, seq_len=10)]\n",
    "tracked_datasets = val_dataset + train_datasets\n",
    "\n",
    "\n",
    "grid = grid_data(device, dim=2, output_dim=2, n=20, bounds=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_datasets[0]:\n",
    "    print(x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate model\n",
    "model = Model(\n",
    "    encoding=encoding,\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    hidden_dim=100,\n",
    "    n_layers=1,\n",
    "    device=device,\n",
    "    init_std=0.03,\n",
    "    # output_noise=0.01,\n",
    ")\n",
    "\n",
    "## TODO: Try starting with large weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as spatial\n",
    "\n",
    "\n",
    "def dir_to_closest(X: pd.DataFrame):\n",
    "    \"\"\"Compute vectors from the closest vector to each vector.\"\"\"\n",
    "    dist = spatial.distance_matrix(X, X)\n",
    "    dist[dist == 0] = np.inf\n",
    "    closest = np.argmin(dist, axis=1)\n",
    "    closest_loc = X.to_numpy()[closest]\n",
    "    closest_vecs = X - closest_loc\n",
    "    return closest_vecs\n",
    "\n",
    "\n",
    "def track_function(inputs):\n",
    "    # hidden_act = model(inputs)[0].detach().numpy()\n",
    "    # hidden = pd.DataFrame(hidden_act, index=labels)\n",
    "    # d = dir_to_closest(hidden)  # Not normalized\n",
    "    grad = jacobian(lambda x: model(x)[0], inputs)\n",
    "    # print(grad.shape)\n",
    "    # print(\"Expect only n_datapoints, output_dim, input_dim\")\n",
    "    # Need to think about with respect to what we take derivative of.\n",
    "    # print(np.linalg.det(grad))\n",
    "\n",
    "    # theta = np.tensordot(grad, d, axes=(1, 1))\n",
    "    # print(theta.shape)\n",
    "\n",
    "    dets = torch.Tensor(\n",
    "        [\n",
    "            np.sum(\n",
    "                [\n",
    "                    torch.det(grad[input, :, input, char, :])\n",
    "                    for char in range(grad.shape[3])\n",
    "                ]\n",
    "            )\n",
    "            for input in range(grad.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    return dets\n",
    "\n",
    "\n",
    "class WeightChangeTracker(Tracker):\n",
    "    \"\"\"Tracks how much weights have changed in the last epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.last_weights = None\n",
    "        self.model = model\n",
    "        super().__init__()\n",
    "\n",
    "    def track(self, *args):\n",
    "        new_weights = [\n",
    "            torch.Tensor(params[1]).clone() for params in self.model.named_parameters()\n",
    "        ]\n",
    "\n",
    "        change = 0.0\n",
    "        if self.last_weights is not None:\n",
    "            old_weights = self.last_weights\n",
    "            for parameter_new, parameter_old in zip(new_weights, old_weights):\n",
    "                change += float(torch.norm(parameter_new - parameter_old))\n",
    "        self._trace.append(pd.DataFrame([change], [\"Weight change\"]))\n",
    "\n",
    "        self.last_weights = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup compiler\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 400\n",
    "lr = 0.003\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "compiler = Compiler(model, criterion, optimizer)\n",
    "initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "compiler.trackers = {\n",
    "    \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    \"hidden\": ActivationTracker(\n",
    "        model, lambda inputs: model(inputs)[1][-1], initial=lambda: initial_hidden\n",
    "    ),\n",
    "    \"output\": ActivationTracker(model, lambda inputs: model(inputs)[0]),\n",
    "    # \"output_grad\": ActivationTracker(\n",
    "    #     model, lambda inputs: jacobian(lambda x: model(x)[0], inputs)\n",
    "    # ),\n",
    "    # \"theta\": ActivationTracker(model, track_function),\n",
    "    \"weight change\": WeightChangeTracker(model),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training run\n",
    "compiler.training_run(\n",
    "    train_datasets, tracked_datasets, n_epochs=n_epochs, batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize automaton dynamics\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "query = \"Epoch >= 0\"\n",
    "query += \"and (Dataset != 0)\"\n",
    "query += \"and (Dataset <= 5)\"\n",
    "data_hid = data_hid.query(query).copy()\n",
    "data_output = data_output.query(query).copy()\n",
    "\n",
    "std = float(np.linalg.norm(data_hid.std()))\n",
    "\n",
    "automaton_history = to_automaton_history(\n",
    "    data_hid, data_output, merge_distance=0.05 * std\n",
    ")\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "\n",
    "# mergers = np.array(automaton_history.get_state_changes()[0])\n",
    "# splits = np.array(automaton_history.get_state_changes()[1])\n",
    "n_states = np.array([len(automaton.states) for automaton in automaton_history])\n",
    "# n_state_changes = np.abs(n_states[1:] - n_states[:-1])\n",
    "# n_state_changes = scipy.signal.savgol_filter(n_state_changes, 31, 2)\n",
    "# n_state_changes[n_state_changes < 0] = 0\n",
    "\n",
    "# dets = compiler.trackers[\"theta\"].get_trace()\n",
    "# theta = dets.groupby(\"Epoch\").sum()[0].to_numpy()\n",
    "weight_change = compiler.trackers[\"weight change\"].get_trace().to_numpy().reshape(-1)\n",
    "\n",
    "animation = SliderAnimation(\n",
    "    [\n",
    "        ActivationsAnimation(data_hid, transform=\"PCA\", n_labels=0),\n",
    "        ActivationsAnimation(\n",
    "            data_output, transform=\"none\", n_labels=0, fixed_points=encoding.encoding\n",
    "        ),\n",
    "        AutomatonAnimation(automaton_history),\n",
    "        EpochAnimation(\n",
    "            graphs={\n",
    "                \"Training loss\": train_loss,\n",
    "                \"Validation loss\": val_loss,\n",
    "            },\n",
    "            unitless_graphs={\n",
    "                \"Number of states\": n_states,\n",
    "                # \"average distance\": av_distances,\n",
    "                # \"theta\": theta,\n",
    "                # \"weight change\": weight_change,\n",
    "                # \"state changes\": n_state_changes,\n",
    "            },\n",
    "            # x_bounds=(0, 800),\n",
    "            y_bounds=(0, 1),\n",
    "        ),\n",
    "    ],\n",
    "    parameters=list(set(data_hid.index.get_level_values(\"Epoch\"))),\n",
    "    parameter_name=\"Epoch\",\n",
    "    fig_size=4,\n",
    ")\n",
    "\n",
    "\n",
    "# TODO Plot map A(h)\n",
    "# TODO Plot pairwise distances\n",
    "# TODO Plot total theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = SliderAnimation(\n",
    "    [\n",
    "        ActivationsAnimation(\n",
    "            data_output, transform=\"none\", n_labels=0, fixed_points=encoding.encoding\n",
    "        ),\n",
    "    ],\n",
    "    parameters=list(set(data_hid.index.get_level_values(\"Epoch\"))),\n",
    "    parameter_name=\"Epoch\",\n",
    "    fig_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation.to_gif(\"animation\", step_size=int(len(train_loss) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute d\n",
    "\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "hid = data_hid.loc[0]\n",
    "\n",
    "\n",
    "av_distances = []\n",
    "for epoch in range(n_epochs):\n",
    "    vecs = dir_to_closest(data_hid.loc[epoch])\n",
    "    av_distance = np.mean([np.sqrt(np.dot(vec, vec)) for vec in vecs.to_numpy()])\n",
    "    av_distances.append(av_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "dataset = train_datasets[5]\n",
    "trainloader = data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "for batch in trainloader:\n",
    "    inputs, outputs = batch\n",
    "\n",
    "    # Get labels\n",
    "    labels = []\n",
    "    for input in inputs:\n",
    "        try:\n",
    "            decoding = model.encoding.decode(input.cpu())\n",
    "            label = \"\".join(str(int(char)) for char in decoding)\n",
    "        except KeyError:\n",
    "            label = tuple(np.squeeze(input.cpu()).numpy())\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "grad = jacobian(lambda x: model(x)[0], inputs)\n",
    "\n",
    "dets = torch.Tensor(\n",
    "    [\n",
    "        np.sum(\n",
    "            [\n",
    "                torch.abs(torch.det(grad[input, :, input, char, :]))\n",
    "                for char in range(grad.shape[3])\n",
    "            ]\n",
    "        )\n",
    "        for input in range(grad.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "act_this_dataset = dets\n",
    "act_this_dataset = torch.squeeze(act_this_dataset).cpu().detach().numpy()\n",
    "act_this_dataset = pd.DataFrame(act_this_dataset, labels)\n",
    "act_this_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for generalization\n",
    "N = 100\n",
    "val_data = []\n",
    "for n in trange(1, N + 1):\n",
    "    val_data.append(seq_data(device, problem, encoding, n_datapoints=100, seq_len=n))\n",
    "val_err = compiler.validation(val_data).to_numpy()[:, 0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.bar(np.arange(1, N + 1), val_err)\n",
    "ax.set_xlabel(\"Sequence lengths\")\n",
    "ax.set_ylabel(\"Validation error\")\n",
    "ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions\n",
    "\n",
    "\n",
    "def predict(model, sequence):\n",
    "    input = torch.from_numpy(np.array([encoding(sequence)], dtype=np.float32)).to(\n",
    "        device\n",
    "    )\n",
    "    out, hidden = model(input)\n",
    "    prediction = out.cpu().detach().numpy()\n",
    "    return prediction\n",
    "\n",
    "\n",
    "test_seq = [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
    "print(f\"Prediction: {predict(model, test_seq)}, True output: {problem(test_seq)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e02af9847f8f14625728f2f7147d07d87bda9043f1b0a8cf0822fa7c64756065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
