{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loek/miniconda3/envs/rnn/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "\n",
    "def XOR_data(n_datapoints=100, seq_len=4):\n",
    "    inputs = np.zeros([n_datapoints, seq_len, 1], dtype=np.float32)\n",
    "    outputs = np.zeros([n_datapoints, 1], dtype=np.float32)\n",
    "    for i in range(n_datapoints):\n",
    "        # Generate input sequences\n",
    "        for j in range(seq_len):\n",
    "            bit = np.random.choice([0, 1])\n",
    "            inputs[i, j, 0] = bit\n",
    "\n",
    "        # Compute output\n",
    "        xor = np.nansum(inputs[i]) % 2  # Code with 0,1\n",
    "        # xor = (-1) ** (1 + np.nansum(inputs[i]) % 2)  # Code with -1,1\n",
    "        outputs[i] = xor\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Defining the layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, hidden_dim, n_layers, batch_first=True, nonlinearity=\"relu\"\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out[:, -1]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate model\n",
    "model = Model(input_size=1, output_size=1, hidden_dim=30, n_layers=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "inputs_1, outputs_1 = XOR_data(n_datapoints=1000, seq_len=5)\n",
    "inputs_2, outputs_2 = XOR_data(n_datapoints=1000, seq_len=4)\n",
    "inputs_1 = torch.from_numpy(inputs_1).to(device)\n",
    "outputs_1 = torch.from_numpy(outputs_1).to(device)\n",
    "inputs_2 = torch.from_numpy(inputs_2).to(device)\n",
    "outputs_2 = torch.from_numpy(outputs_2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/3000............. Loss: 0.2505\n",
      "Epoch: 20/3000............. Loss: 0.2498\n",
      "Epoch: 30/3000............. Loss: 0.2496\n",
      "Epoch: 40/3000............. Loss: 0.2496\n",
      "Epoch: 50/3000............. Loss: 0.2496\n",
      "Epoch: 60/3000............. Loss: 0.2496\n",
      "Epoch: 70/3000............. Loss: 0.2496\n",
      "Epoch: 80/3000............. Loss: 0.2496\n",
      "Epoch: 90/3000............. Loss: 0.2496\n",
      "Epoch: 100/3000............. Loss: 0.2496\n",
      "Epoch: 110/3000............. Loss: 0.2496\n",
      "Epoch: 120/3000............. Loss: 0.2496\n",
      "Epoch: 130/3000............. Loss: 0.2496\n",
      "Epoch: 140/3000............. Loss: 0.2496\n",
      "Epoch: 150/3000............. Loss: 0.2495\n",
      "Epoch: 160/3000............. Loss: 0.2495\n",
      "Epoch: 170/3000............. Loss: 0.2495\n",
      "Epoch: 180/3000............. Loss: 0.2495\n",
      "Epoch: 190/3000............. Loss: 0.2495\n",
      "Epoch: 200/3000............. Loss: 0.2495\n",
      "Epoch: 210/3000............. Loss: 0.2495\n",
      "Epoch: 220/3000............. Loss: 0.2495\n",
      "Epoch: 230/3000............. Loss: 0.2495\n",
      "Epoch: 240/3000............. Loss: 0.2495\n",
      "Epoch: 250/3000............. Loss: 0.2495\n",
      "Epoch: 260/3000............. Loss: 0.2495\n",
      "Epoch: 270/3000............. Loss: 0.2495\n",
      "Epoch: 280/3000............. Loss: 0.2495\n",
      "Epoch: 290/3000............. Loss: 0.2495\n",
      "Epoch: 300/3000............. Loss: 0.2495\n",
      "Epoch: 310/3000............. Loss: 0.2495\n",
      "Epoch: 320/3000............. Loss: 0.2495\n",
      "Epoch: 330/3000............. Loss: 0.2495\n",
      "Epoch: 340/3000............. Loss: 0.2495\n",
      "Epoch: 350/3000............. Loss: 0.2495\n",
      "Epoch: 360/3000............. Loss: 0.2495\n",
      "Epoch: 370/3000............. Loss: 0.2495\n",
      "Epoch: 380/3000............. Loss: 0.2495\n",
      "Epoch: 390/3000............. Loss: 0.2495\n",
      "Epoch: 400/3000............. Loss: 0.2495\n",
      "Epoch: 410/3000............. Loss: 0.2495\n",
      "Epoch: 420/3000............. Loss: 0.2495\n",
      "Epoch: 430/3000............. Loss: 0.2495\n",
      "Epoch: 440/3000............. Loss: 0.2495\n",
      "Epoch: 450/3000............. Loss: 0.2495\n",
      "Epoch: 460/3000............. Loss: 0.2495\n",
      "Epoch: 470/3000............. Loss: 0.2495\n",
      "Epoch: 480/3000............. Loss: 0.2495\n",
      "Epoch: 490/3000............. Loss: 0.2495\n",
      "Epoch: 500/3000............. Loss: 0.2495\n",
      "Epoch: 510/3000............. Loss: 0.2495\n",
      "Epoch: 520/3000............. Loss: 0.2495\n",
      "Epoch: 530/3000............. Loss: 0.2495\n",
      "Epoch: 540/3000............. Loss: 0.2495\n",
      "Epoch: 550/3000............. Loss: 0.2495\n",
      "Epoch: 560/3000............. Loss: 0.2495\n",
      "Epoch: 570/3000............. Loss: 0.2495\n",
      "Epoch: 580/3000............. Loss: 0.2495\n",
      "Epoch: 590/3000............. Loss: 0.2495\n",
      "Epoch: 600/3000............. Loss: 0.2495\n",
      "Epoch: 610/3000............. Loss: 0.2495\n",
      "Epoch: 620/3000............. Loss: 0.2495\n",
      "Epoch: 630/3000............. Loss: 0.2495\n",
      "Epoch: 640/3000............. Loss: 0.2495\n",
      "Epoch: 650/3000............. Loss: 0.2495\n",
      "Epoch: 660/3000............. Loss: 0.2495\n",
      "Epoch: 670/3000............. Loss: 0.2495\n",
      "Epoch: 680/3000............. Loss: 0.2495\n",
      "Epoch: 690/3000............. Loss: 0.2495\n",
      "Epoch: 700/3000............. Loss: 0.2495\n",
      "Epoch: 710/3000............. Loss: 0.2495\n",
      "Epoch: 720/3000............. Loss: 0.2495\n",
      "Epoch: 730/3000............. Loss: 0.2495\n",
      "Epoch: 740/3000............. Loss: 0.2495\n",
      "Epoch: 750/3000............. Loss: 0.2495\n",
      "Epoch: 760/3000............. Loss: 0.2495\n",
      "Epoch: 770/3000............. Loss: 0.2495\n",
      "Epoch: 780/3000............. Loss: 0.2495\n",
      "Epoch: 790/3000............. Loss: 0.2495\n",
      "Epoch: 800/3000............. Loss: 0.2495\n",
      "Epoch: 810/3000............. Loss: 0.2495\n",
      "Epoch: 820/3000............. Loss: 0.2495\n",
      "Epoch: 830/3000............. Loss: 0.2495\n",
      "Epoch: 840/3000............. Loss: 0.2495\n",
      "Epoch: 850/3000............. Loss: 0.2495\n",
      "Epoch: 860/3000............. Loss: 0.2495\n",
      "Epoch: 870/3000............. Loss: 0.2495\n",
      "Epoch: 880/3000............. Loss: 0.2495\n",
      "Epoch: 890/3000............. Loss: 0.2495\n",
      "Epoch: 900/3000............. Loss: 0.2495\n",
      "Epoch: 910/3000............. Loss: 0.2495\n",
      "Epoch: 920/3000............. Loss: 0.2495\n",
      "Epoch: 930/3000............. Loss: 0.2495\n",
      "Epoch: 940/3000............. Loss: 0.2495\n",
      "Epoch: 950/3000............. Loss: 0.2495\n",
      "Epoch: 960/3000............. Loss: 0.2495\n",
      "Epoch: 970/3000............. Loss: 0.2495\n",
      "Epoch: 980/3000............. Loss: 0.2495\n",
      "Epoch: 990/3000............. Loss: 0.2495\n",
      "Epoch: 1000/3000............. Loss: 0.2495\n",
      "Epoch: 1010/3000............. Loss: 0.2495\n",
      "Epoch: 1020/3000............. Loss: 0.2495\n",
      "Epoch: 1030/3000............. Loss: 0.2495\n",
      "Epoch: 1040/3000............. Loss: 0.2495\n",
      "Epoch: 1050/3000............. Loss: 0.2495\n",
      "Epoch: 1060/3000............. Loss: 0.2495\n",
      "Epoch: 1070/3000............. Loss: 0.2495\n",
      "Epoch: 1080/3000............. Loss: 0.2495\n",
      "Epoch: 1090/3000............. Loss: 0.2495\n",
      "Epoch: 1100/3000............. Loss: 0.2495\n",
      "Epoch: 1110/3000............. Loss: 0.2495\n",
      "Epoch: 1120/3000............. Loss: 0.2495\n",
      "Epoch: 1130/3000............. Loss: 0.2495\n",
      "Epoch: 1140/3000............. Loss: 0.2495\n",
      "Epoch: 1150/3000............. Loss: 0.2495\n",
      "Epoch: 1160/3000............. Loss: 0.2495\n",
      "Epoch: 1170/3000............. Loss: 0.2495\n",
      "Epoch: 1180/3000............. Loss: 0.2495\n",
      "Epoch: 1190/3000............. Loss: 0.2495\n",
      "Epoch: 1200/3000............. Loss: 0.2495\n",
      "Epoch: 1210/3000............. Loss: 0.2495\n",
      "Epoch: 1220/3000............. Loss: 0.2495\n",
      "Epoch: 1230/3000............. Loss: 0.2495\n",
      "Epoch: 1240/3000............. Loss: 0.2495\n",
      "Epoch: 1250/3000............. Loss: 0.2495\n",
      "Epoch: 1260/3000............. Loss: 0.2495\n",
      "Epoch: 1270/3000............. Loss: 0.2495\n",
      "Epoch: 1280/3000............. Loss: 0.2495\n",
      "Epoch: 1290/3000............. Loss: 0.2495\n",
      "Epoch: 1300/3000............. Loss: 0.2495\n",
      "Epoch: 1310/3000............. Loss: 0.2495\n",
      "Epoch: 1320/3000............. Loss: 0.2495\n",
      "Epoch: 1330/3000............. Loss: 0.2495\n",
      "Epoch: 1340/3000............. Loss: 0.2495\n",
      "Epoch: 1350/3000............. Loss: 0.2495\n",
      "Epoch: 1360/3000............. Loss: 0.2495\n",
      "Epoch: 1370/3000............. Loss: 0.2495\n",
      "Epoch: 1380/3000............. Loss: 0.2495\n",
      "Epoch: 1390/3000............. Loss: 0.2495\n",
      "Epoch: 1400/3000............. Loss: 0.2495\n",
      "Epoch: 1410/3000............. Loss: 0.2495\n",
      "Epoch: 1420/3000............. Loss: 0.2495\n",
      "Epoch: 1430/3000............. Loss: 0.2495\n",
      "Epoch: 1440/3000............. Loss: 0.2495\n",
      "Epoch: 1450/3000............. Loss: 0.2495\n",
      "Epoch: 1460/3000............. Loss: 0.2495\n",
      "Epoch: 1470/3000............. Loss: 0.2495\n",
      "Epoch: 1480/3000............. Loss: 0.2495\n",
      "Epoch: 1490/3000............. Loss: 0.2495\n",
      "Epoch: 1500/3000............. Loss: 0.2495\n",
      "Epoch: 1510/3000............. Loss: 0.2495\n",
      "Epoch: 1520/3000............. Loss: 0.2495\n",
      "Epoch: 1530/3000............. Loss: 0.2495\n",
      "Epoch: 1540/3000............. Loss: 0.2495\n",
      "Epoch: 1550/3000............. Loss: 0.2495\n",
      "Epoch: 1560/3000............. Loss: 0.2495\n",
      "Epoch: 1570/3000............. Loss: 0.2495\n",
      "Epoch: 1580/3000............. Loss: 0.2495\n",
      "Epoch: 1590/3000............. Loss: 0.2495\n",
      "Epoch: 1600/3000............. Loss: 0.2495\n",
      "Epoch: 1610/3000............. Loss: 0.2495\n",
      "Epoch: 1620/3000............. Loss: 0.2495\n",
      "Epoch: 1630/3000............. Loss: 0.2495\n",
      "Epoch: 1640/3000............. Loss: 0.2495\n",
      "Epoch: 1650/3000............. Loss: 0.2495\n",
      "Epoch: 1660/3000............. Loss: 0.2495\n",
      "Epoch: 1670/3000............. Loss: 0.2495\n",
      "Epoch: 1680/3000............. Loss: 0.2495\n",
      "Epoch: 1690/3000............. Loss: 0.2495\n",
      "Epoch: 1700/3000............. Loss: 0.2495\n",
      "Epoch: 1710/3000............. Loss: 0.2495\n",
      "Epoch: 1720/3000............. Loss: 0.2495\n",
      "Epoch: 1730/3000............. Loss: 0.2495\n",
      "Epoch: 1740/3000............. Loss: 0.2495\n",
      "Epoch: 1750/3000............. Loss: 0.2495\n",
      "Epoch: 1760/3000............. Loss: 0.2495\n",
      "Epoch: 1770/3000............. Loss: 0.2495\n",
      "Epoch: 1780/3000............. Loss: 0.2495\n",
      "Epoch: 1790/3000............. Loss: 0.2495\n",
      "Epoch: 1800/3000............. Loss: 0.2495\n",
      "Epoch: 1810/3000............. Loss: 0.2495\n",
      "Epoch: 1820/3000............. Loss: 0.2495\n",
      "Epoch: 1830/3000............. Loss: 0.2495\n",
      "Epoch: 1840/3000............. Loss: 0.2495\n",
      "Epoch: 1850/3000............. Loss: 0.2495\n",
      "Epoch: 1860/3000............. Loss: 0.2495\n",
      "Epoch: 1870/3000............. Loss: 0.2495\n",
      "Epoch: 1880/3000............. Loss: 0.2495\n",
      "Epoch: 1890/3000............. Loss: 0.2495\n",
      "Epoch: 1900/3000............. Loss: 0.2495\n",
      "Epoch: 1910/3000............. Loss: 0.2495\n",
      "Epoch: 1920/3000............. Loss: 0.2495\n",
      "Epoch: 1930/3000............. Loss: 0.2495\n",
      "Epoch: 1940/3000............. Loss: 0.2495\n",
      "Epoch: 1950/3000............. Loss: 0.2495\n",
      "Epoch: 1960/3000............. Loss: 0.2495\n",
      "Epoch: 1970/3000............. Loss: 0.2495\n",
      "Epoch: 1980/3000............. Loss: 0.2495\n",
      "Epoch: 1990/3000............. Loss: 0.2495\n",
      "Epoch: 2000/3000............. Loss: 0.2495\n",
      "Epoch: 2010/3000............. Loss: 0.2495\n",
      "Epoch: 2020/3000............. Loss: 0.2495\n",
      "Epoch: 2030/3000............. Loss: 0.2495\n",
      "Epoch: 2040/3000............. Loss: 0.2495\n",
      "Epoch: 2050/3000............. Loss: 0.2495\n",
      "Epoch: 2060/3000............. Loss: 0.2495\n",
      "Epoch: 2070/3000............. Loss: 0.2495\n",
      "Epoch: 2080/3000............. Loss: 0.2495\n",
      "Epoch: 2090/3000............. Loss: 0.2495\n",
      "Epoch: 2100/3000............. Loss: 0.2495\n",
      "Epoch: 2110/3000............. Loss: 0.2495\n",
      "Epoch: 2120/3000............. Loss: 0.2495\n",
      "Epoch: 2130/3000............. Loss: 0.2495\n",
      "Epoch: 2140/3000............. Loss: 0.2495\n",
      "Epoch: 2150/3000............. Loss: 0.2495\n",
      "Epoch: 2160/3000............. Loss: 0.2495\n",
      "Epoch: 2170/3000............. Loss: 0.2495\n",
      "Epoch: 2180/3000............. Loss: 0.2495\n",
      "Epoch: 2190/3000............. Loss: 0.2495\n",
      "Epoch: 2200/3000............. Loss: 0.2495\n",
      "Epoch: 2210/3000............. Loss: 0.2495\n",
      "Epoch: 2220/3000............. Loss: 0.2495\n",
      "Epoch: 2230/3000............. Loss: 0.2495\n",
      "Epoch: 2240/3000............. Loss: 0.2495\n",
      "Epoch: 2250/3000............. Loss: 0.2495\n",
      "Epoch: 2260/3000............. Loss: 0.2495\n",
      "Epoch: 2270/3000............. Loss: 0.2495\n",
      "Epoch: 2280/3000............. Loss: 0.2495\n",
      "Epoch: 2290/3000............. Loss: 0.2495\n",
      "Epoch: 2300/3000............. Loss: 0.2495\n",
      "Epoch: 2310/3000............. Loss: 0.2495\n",
      "Epoch: 2320/3000............. Loss: 0.2495\n",
      "Epoch: 2330/3000............. Loss: 0.2495\n",
      "Epoch: 2340/3000............. Loss: 0.2495\n",
      "Epoch: 2350/3000............. Loss: 0.2495\n",
      "Epoch: 2360/3000............. Loss: 0.2495\n",
      "Epoch: 2370/3000............. Loss: 0.2495\n",
      "Epoch: 2380/3000............. Loss: 0.2495\n",
      "Epoch: 2390/3000............. Loss: 0.2495\n",
      "Epoch: 2400/3000............. Loss: 0.2495\n",
      "Epoch: 2410/3000............. Loss: 0.2495\n",
      "Epoch: 2420/3000............. Loss: 0.2495\n",
      "Epoch: 2430/3000............. Loss: 0.2495\n",
      "Epoch: 2440/3000............. Loss: 0.2495\n",
      "Epoch: 2450/3000............. Loss: 0.2495\n",
      "Epoch: 2460/3000............. Loss: 0.2495\n",
      "Epoch: 2470/3000............. Loss: 0.2495\n",
      "Epoch: 2480/3000............. Loss: 0.2495\n",
      "Epoch: 2490/3000............. Loss: 0.2495\n",
      "Epoch: 2500/3000............. Loss: 0.2495\n",
      "Epoch: 2510/3000............. Loss: 0.2495\n",
      "Epoch: 2520/3000............. Loss: 0.2495\n",
      "Epoch: 2530/3000............. Loss: 0.2495\n",
      "Epoch: 2540/3000............. Loss: 0.2495\n",
      "Epoch: 2550/3000............. Loss: 0.2495\n",
      "Epoch: 2560/3000............. Loss: 0.2495\n",
      "Epoch: 2570/3000............. Loss: 0.2495\n",
      "Epoch: 2580/3000............. Loss: 0.2495\n",
      "Epoch: 2590/3000............. Loss: 0.2495\n",
      "Epoch: 2600/3000............. Loss: 0.2495\n",
      "Epoch: 2610/3000............. Loss: 0.2495\n",
      "Epoch: 2620/3000............. Loss: 0.2495\n",
      "Epoch: 2630/3000............. Loss: 0.2495\n",
      "Epoch: 2640/3000............. Loss: 0.2495\n",
      "Epoch: 2650/3000............. Loss: 0.2495\n",
      "Epoch: 2660/3000............. Loss: 0.2495\n",
      "Epoch: 2670/3000............. Loss: 0.2495\n",
      "Epoch: 2680/3000............. Loss: 0.2495\n",
      "Epoch: 2690/3000............. Loss: 0.2495\n",
      "Epoch: 2700/3000............. Loss: 0.2495\n",
      "Epoch: 2710/3000............. Loss: 0.2495\n",
      "Epoch: 2720/3000............. Loss: 0.2495\n",
      "Epoch: 2730/3000............. Loss: 0.2495\n",
      "Epoch: 2740/3000............. Loss: 0.2495\n",
      "Epoch: 2750/3000............. Loss: 0.2495\n",
      "Epoch: 2760/3000............. Loss: 0.2495\n",
      "Epoch: 2770/3000............. Loss: 0.2495\n",
      "Epoch: 2780/3000............. Loss: 0.2495\n",
      "Epoch: 2790/3000............. Loss: 0.2495\n",
      "Epoch: 2800/3000............. Loss: 0.2495\n",
      "Epoch: 2810/3000............. Loss: 0.2495\n",
      "Epoch: 2820/3000............. Loss: 0.2495\n",
      "Epoch: 2830/3000............. Loss: 0.2495\n",
      "Epoch: 2840/3000............. Loss: 0.2495\n",
      "Epoch: 2850/3000............. Loss: 0.2495\n",
      "Epoch: 2860/3000............. Loss: 0.2495\n",
      "Epoch: 2870/3000............. Loss: 0.2495\n",
      "Epoch: 2880/3000............. Loss: 0.2495\n",
      "Epoch: 2890/3000............. Loss: 0.2495\n",
      "Epoch: 2900/3000............. Loss: 0.2495\n",
      "Epoch: 2910/3000............. Loss: 0.2495\n",
      "Epoch: 2920/3000............. Loss: 0.2495\n",
      "Epoch: 2930/3000............. Loss: 0.2495\n",
      "Epoch: 2940/3000............. Loss: 0.2495\n",
      "Epoch: 2950/3000............. Loss: 0.2495\n",
      "Epoch: 2960/3000............. Loss: 0.2495\n",
      "Epoch: 2970/3000............. Loss: 0.2495\n",
      "Epoch: 2980/3000............. Loss: 0.2495\n",
      "Epoch: 2990/3000............. Loss: 0.2495\n",
      "Epoch: 3000/3000............. Loss: 0.2495\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 3000\n",
    "lr = 0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()  # Clears existing gradients from previous epoch\n",
    "    output, hidden = model(inputs_1)\n",
    "    loss = criterion(torch.squeeze(output), torch.squeeze(outputs_1))\n",
    "    loss.backward()  # Does backpropagation and calculates gradients\n",
    "    optimizer.step()  # Updates the weights accordingly\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(inputs_2)\n",
    "    loss = criterion(torch.squeeze(output), torch.squeeze(outputs_2))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {}/{}.............\".format(epoch, n_epochs), end=\" \")\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sequence):\n",
    "    input = torch.unsqueeze(\n",
    "        torch.from_numpy(np.array([sequence], dtype=np.float32)), dim=2\n",
    "    ).to(device)\n",
    "    out, hidden = model(input)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]], device='cuda:0')\n",
      "Output:1.0\n",
      "Prediction:0.4834793508052826\n",
      "tensor(0.2668, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Predict on dataset\n",
    "index = 1\n",
    "input = inputs_1[index]\n",
    "output = torch.squeeze(outputs_1[index])\n",
    "prediction, _ = model(torch.unsqueeze(input, dim=0))\n",
    "prediction = torch.squeeze(prediction)\n",
    "print(f\"Input:{input}\")\n",
    "print(f\"Output:{output}\")\n",
    "print(f\"Prediction:{prediction}\")\n",
    "print(criterion(prediction, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5019]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(predict(model, [1, 1, 0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:00<00:01, 40.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/loek/projects/rnn/XOR.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m val_err \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(N \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m trange(\u001b[39m1\u001b[39m, N \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     val_inputs, val_outputs \u001b[39m=\u001b[39m XOR_data(n_datapoints\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, seq_len\u001b[39m=\u001b[39;49mn)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     val_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(val_inputs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     val_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(val_outputs)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/home/loek/projects/rnn/XOR.ipynb Cell 10\u001b[0m in \u001b[0;36mXOR_data\u001b[0;34m(n_datapoints, seq_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_datapoints):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Generate input sequences\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         bit \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice([\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         inputs[i, j, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m bit\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/XOR.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Compute output\u001b[39;00m\n",
      "File \u001b[0;32mmtrand.pyx:962\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:748\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_bounded_integers.pyx:1228\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "val_err = np.zeros(N + 1)\n",
    "for n in trange(1, N + 1):\n",
    "    val_inputs, val_outputs = XOR_data(n_datapoints=200, seq_len=n)\n",
    "    val_inputs = torch.from_numpy(val_inputs).to(device)\n",
    "    val_outputs = torch.from_numpy(val_outputs).to(device)\n",
    "    prediction, _ = model(val_inputs)\n",
    "    val_err[n] = criterion(torch.squeeze(prediction), torch.squeeze(val_outputs))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.bar(np.arange(N + 1), val_err)\n",
    "ax.set_xlabel(\"Sequence lengths\")\n",
    "ax.set_ylabel(\"Validation error\")\n",
    "ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "plt.show()\n",
    "print(val_err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e02af9847f8f14625728f2f7147d07d87bda9043f1b0a8cf0822fa7c64756065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
