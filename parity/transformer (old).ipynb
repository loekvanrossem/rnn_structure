{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc4d73d5a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import to_automaton_history, reduce_automaton, to_automaton\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "from visualization.automata import AutomatonAnimation, display_automata\n",
    "from visualization.epochs import EpochAnimation\n",
    "\n",
    "from model import Model\n",
    "import publication\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.081581163465542)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [2, 3], [4, 5]])\n",
    "a = np.mean(np.linalg.norm(a, axis=1))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load settings\n",
    "settings = \"rich\"\n",
    "\n",
    "(nonlinearity, gain, lr, P, L, n_epochs, max_seq_len) = (\n",
    "    pd.read_csv(\"model settings/rnn.txt\", sep=\" \", header=0).loc[settings].to_numpy()\n",
    ")\n",
    "P, L, n_epochs, max_seq_len = int(P), int(L), int(n_epochs), int(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "# Define problem and data encoding\n",
    "symbols = [0, 1]\n",
    "encoding = OneHot(symbols)\n",
    "problem = lambda seq: np.sum(seq) % 2  # XOR problem\n",
    "# problem = lambda seq: (np.sum(seq) % 3) % 2\n",
    "\n",
    "# Define sequence lengths for training and validation datasets\n",
    "train_seq_lengths = list(range(1, max_seq_len + 1))\n",
    "analysis_seq_lengths = train_seq_lengths\n",
    "# analysis_seq_lengths = list(range(1, 8 + 1))\n",
    "val_seq_length = 50\n",
    "val_datapoints = 100\n",
    "\n",
    "# Generate datasets\n",
    "training_datasets = [\n",
    "    seq_data(device, problem, encoding, seq_len=length) for length in train_seq_lengths\n",
    "]\n",
    "validation_datasets = [\n",
    "    seq_data(\n",
    "        device, problem, encoding, n_datapoints=val_datapoints, seq_len=val_seq_length\n",
    "    )\n",
    "]\n",
    "analysis_data = [\n",
    "    seq_data(device, problem, encoding, seq_len=length)\n",
    "    for length in analysis_seq_lengths\n",
    "]\n",
    "tracked_datasets = validation_datasets + analysis_data + training_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loek/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import Encoding\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    encoding : Encoding\n",
    "        An encoding from input symbols to neural activities\n",
    "    input_size : int\n",
    "        The size of one input symbol\n",
    "    output_size : int\n",
    "        The size of the output\n",
    "    hidden_dim : int\n",
    "        The number of hidden neurons\n",
    "    n_layers : int\n",
    "        The number of RNN layers\n",
    "    device : device\n",
    "        The device to put the model on\n",
    "    nonlinearity : str\n",
    "        Nonlinearity used in recurrent layer\n",
    "    gain : float, default 0.1\n",
    "        The gain of the initial rnn weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding: Encoding,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        device: torch.device,\n",
    "        nonlinearity: str = \"tanh\",\n",
    "        gain: float = 0.1,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.encoding = encoding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Defining the layers\n",
    "        # self.rnn = nn.RNN(\n",
    "        #     input_size,\n",
    "        #     hidden_dim,\n",
    "        #     n_layers,\n",
    "        #     batch_first=True,\n",
    "        #     nonlinearity=nonlinearity,\n",
    "        # )\n",
    "        self.fc_in = nn.Linear(input_size, hidden_dim, bias=True)\n",
    "        self.ReLU = nn.LeakyReLU()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim, nhead=50, num_encoder_layers=4, bias=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size, bias=True)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=50)\n",
    "\n",
    "        # for par in self.parameters():\n",
    "        #     if len(par.shape) == 2:\n",
    "        #         nn.init.xavier_normal_(par, gain=gain)\n",
    "        #     if len(par.shape) == 1:\n",
    "        #         nn.init.zeros_(par)\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if \"weight\" in name and param.data.dim() == 2:\n",
    "        #         nn.init.kaiming_uniform_(param)\n",
    "        # if param.data.dim() == 2:\n",
    "        #     nn.init.xavier_normal_(param, gain=1)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the self and obtaining outputs\n",
    "        # y = torch.stack([y * 0] * (x.shape[1] - 1) + [y], axis=1)  # pad y with zeros\n",
    "        # out = self.transformer(x, y)\n",
    "\n",
    "        out = self.ReLU(self.fc_in(x))\n",
    "        # out = self.transformer.encoder(out)\n",
    "        out = self.encoder(out)\n",
    "        out = out.max(dim=1)[0]\n",
    "        # out = out[:, -1, :]\n",
    "        out = self.ReLU(self.fc_out(out))\n",
    "\n",
    "        hidden = out\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def predict(self, x):\n",
    "        out = self(x, x)[0]\n",
    "        # out = out[:, -1, :]\n",
    "        prediction = torch.argmax(out).item()\n",
    "        return prediction\n",
    "\n",
    "    def train_step(self, optimizer: Optimizer, criterion, dataloader):\n",
    "        self.train()\n",
    "        av_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, outputs = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = self(inputs, outputs)\n",
    "\n",
    "            # output = output[:, -1, :]\n",
    "\n",
    "            # print(output)\n",
    "            # print(outputs)\n",
    "\n",
    "            loss = criterion(torch.squeeze(output), torch.squeeze(outputs))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            av_loss += loss / len(dataloader)\n",
    "        return av_loss\n",
    "\n",
    "\n",
    "P = 1000\n",
    "\n",
    "model = Transformer(\n",
    "    encoding=encoding,\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    hidden_dim=P,\n",
    "    n_layers=L,\n",
    "    device=device,\n",
    "    nonlinearity=nonlinearity,\n",
    "    gain=gain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup compiler\n",
    "lr = 0.001\n",
    "n_epochs = 2000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True, weight_decay=0.00)\n",
    "compiler = Compiler(model, criterion, optimizer)\n",
    "# initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "# hidden_function = lambda inputs: model(inputs)[1][-1]\n",
    "output_function = lambda inputs: model(inputs)[0]\n",
    "compiler.trackers = {\n",
    "    \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    # \"hidden\": ActivationTracker(\n",
    "    #     encoding,\n",
    "    #     hidden_function,\n",
    "    #     analysis_data,\n",
    "    #     initial=lambda: initial_hidden,\n",
    "    # ),\n",
    "    # \"output\": ActivationTracker(encoding, output_function, analysis_data),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.25481206178665\n",
      "4.282965421676636\n",
      "4.2422007620334625\n",
      "4.117201209068298\n",
      "3.37539079785347\n",
      "3.6433010399341583\n",
      "4.00490614771843\n",
      "4.015331953763962\n",
      "3.9927879571914673\n",
      "3.9492194056510925\n",
      "3.908480077981949\n",
      "3.8771691024303436\n",
      "3.850974678993225\n",
      "3.818611055612564\n",
      "3.432559549808502\n",
      "3.915164202451706\n",
      "4.015902400016785\n",
      "4.028731733560562\n",
      "4.015877604484558\n",
      "3.9855583906173706\n",
      "3.9576510190963745\n",
      "3.929815858602524\n",
      "3.8960157334804535\n",
      "3.869011163711548\n",
      "3.842617690563202\n",
      "3.60829296708107\n",
      "2.925633519887924\n",
      "2.9971530735492706\n",
      "2.76036736369133\n",
      "2.9205084145069122\n",
      "3.5057222843170166\n",
      "3.897083818912506\n",
      "3.9387514889240265\n",
      "3.929310977458954\n",
      "3.8985400199890137\n",
      "3.560104876756668\n",
      "3.2168162167072296\n",
      "3.8298839330673218\n",
      "3.836890161037445\n",
      "3.8293874859809875\n",
      "3.8291940093040466\n",
      "3.807600259780884\n",
      "3.7944977581501007\n",
      "3.727127730846405\n",
      "3.5320861786603928\n",
      "3.3247769474983215\n",
      "3.8479703068733215\n",
      "3.8814777731895447\n",
      "3.880414217710495\n",
      "3.8731228709220886\n",
      "3.8608287572860718\n",
      "3.8493199944496155\n",
      "3.8367453813552856\n",
      "3.832404315471649\n",
      "3.8148270547389984\n",
      "3.800005078315735\n",
      "3.4125714898109436\n",
      "3.5520573258399963\n",
      "3.1112326979637146\n",
      "3.473676860332489\n",
      "2.955254316329956\n",
      "3.6098060607910156\n",
      "3.8618109226226807\n",
      "3.349959522485733\n",
      "3.5861547589302063\n",
      "3.1274945437908173\n",
      "3.479865849018097\n",
      "2.9429351687431335\n",
      "3.066151648759842\n",
      "3.0405702590942383\n",
      "3.039473831653595\n",
      "3.161980777978897\n",
      "3.0675192922353745\n",
      "3.072793483734131\n",
      "3.0935693085193634\n",
      "3.0538779199123383\n",
      "2.9702555388212204\n",
      "2.918368011713028\n",
      "2.935901716351509\n",
      "2.9090394377708435\n",
      "2.8803137093782425\n",
      "2.8443654775619507\n",
      "2.9914406836032867\n",
      "2.882145807147026\n",
      "2.9018896222114563\n",
      "2.875001698732376\n",
      "2.8506507873535156\n",
      "2.5339024513959885\n",
      "2.200241431593895\n",
      "2.0781075060367584\n",
      "2.0543229430913925\n",
      "2.1096194088459015\n",
      "1.9562683999538422\n",
      "2.0906071811914444\n",
      "2.0431352704763412\n",
      "2.0241085588932037\n",
      "2.0352413207292557\n",
      "1.9508910030126572\n",
      "2.089263066649437\n",
      "2.1578121185302734\n",
      "2.000780627131462\n",
      "2.0828766524791718\n",
      "1.9478338807821274\n",
      "1.9691122472286224\n",
      "2.0174102932214737\n",
      "1.9922249913215637\n",
      "2.213453322649002\n",
      "1.908345714211464\n",
      "1.870744302868843\n",
      "2.100456953048706\n",
      "1.8987396508455276\n",
      "1.9754092395305634\n",
      "1.978796124458313\n",
      "1.9122285693883896\n",
      "1.8329568654298782\n",
      "2.0666086971759796\n",
      "1.9506551921367645\n",
      "1.8637598156929016\n",
      "1.8451473414897919\n",
      "1.8073734045028687\n",
      "1.7952163368463516\n",
      "1.9394182711839676\n",
      "1.843953900039196\n",
      "1.858831886202097\n",
      "1.7054138742387295\n",
      "1.9598732739686966\n",
      "1.9884154945611954\n",
      "1.8718459531664848\n",
      "2.0015225782990456\n",
      "1.8224545158445835\n",
      "1.9839086756110191\n",
      "1.8494872450828552\n",
      "1.6283744387328625\n",
      "1.9583066403865814\n",
      "1.9106846749782562\n",
      "1.8825705386698246\n",
      "1.7565901819616556\n",
      "1.6855176463723183\n",
      "1.7923230677843094\n",
      "1.6502317264676094\n",
      "1.5638201292604208\n",
      "1.5055149663239717\n",
      "1.6534871384501457\n",
      "1.5894680619239807\n",
      "1.5755194444209337\n",
      "1.620070869103074\n",
      "1.6088934615254402\n",
      "1.5695094428956509\n",
      "1.5588475903496146\n",
      "1.5740248747169971\n",
      "1.5475476421415806\n",
      "1.593543067574501\n",
      "1.6758926510810852\n",
      "1.6771778389811516\n",
      "1.533410094678402\n",
      "1.5546748898923397\n",
      "1.551360972225666\n",
      "1.523284550756216\n",
      "1.6018923111259937\n",
      "1.6005181446671486\n",
      "1.5276318974792957\n",
      "1.4898846112191677\n",
      "1.659666322171688\n",
      "1.5626080110669136\n",
      "1.6441561505198479\n",
      "1.5897599682211876\n",
      "1.5299640260636806\n",
      "1.5525845289230347\n",
      "1.5560858733952045\n",
      "1.5695513598620892\n",
      "1.5379389524459839\n",
      "1.5684882868081331\n",
      "1.458841685205698\n",
      "1.5820868015289307\n",
      "1.4977054633200169\n",
      "1.6262428537011147\n",
      "1.6043144017457962\n",
      "1.5197594799101353\n",
      "1.6567009538412094\n",
      "1.4905865862965584\n",
      "1.5828476324677467\n",
      "1.5544906742870808\n",
      "1.5690049845725298\n",
      "1.49138188874349\n",
      "1.647214725613594\n",
      "1.6526982560753822\n",
      "1.7576976362615824\n",
      "1.5880235037766397\n",
      "1.5734745487570763\n",
      "1.7062928974628448\n",
      "1.6126278582960367\n",
      "1.5435217320919037\n",
      "1.5708366855978966\n",
      "1.56306803971529\n",
      "1.6297288872301579\n",
      "1.4638437628746033\n",
      "1.560104725882411\n",
      "1.502530962228775\n",
      "1.575936745852232\n",
      "1.5484438128769398\n",
      "1.5442325323820114\n",
      "1.5468220598995686\n",
      "1.5349616159219295\n",
      "1.5896337740123272\n",
      "1.5667828768491745\n",
      "1.5041396552696824\n",
      "1.554335517808795\n",
      "1.5566379949450493\n",
      "1.5320314951241016\n",
      "1.6086179800331593\n",
      "1.5623455066233873\n",
      "1.5081897117197514\n",
      "1.545771062374115\n",
      "1.5030693374574184\n",
      "1.4874746445566416\n",
      "1.475904431194067\n",
      "1.5100529566407204\n",
      "1.514950968325138\n",
      "1.5304526779800653\n",
      "1.4891248941421509\n",
      "1.5501535255461931\n",
      "1.5639700405299664\n",
      "1.4864656887948513\n",
      "1.5171376690268517\n",
      "1.523586830124259\n",
      "1.4652967806905508\n",
      "1.4906840100884438\n",
      "1.4438434173353016\n",
      "1.5054516531527042\n",
      "1.4263680092990398\n",
      "1.5413739308714867\n",
      "1.5112764462828636\n",
      "1.5686223087832332\n",
      "1.5234290473163128\n",
      "1.4734993800520897\n",
      "1.5555103048682213\n",
      "1.5720446724444628\n",
      "1.591246877796948\n",
      "1.557814810425043\n",
      "1.5340220853686333\n",
      "1.585420809686184\n",
      "1.525095303542912\n",
      "1.478215180337429\n",
      "1.4592461287975311\n",
      "1.5235671531409025\n",
      "1.431296403054148\n",
      "1.4695303961634636\n",
      "1.5700942501425743\n",
      "1.5019690147601068\n",
      "1.5154098700731993\n",
      "1.5482445862144232\n",
      "1.4960436709225178\n",
      "1.5437389090657234\n",
      "1.5645212195813656\n",
      "1.5757893398404121\n",
      "1.5164260752499104\n",
      "1.5228260420262814\n",
      "1.4919144287705421\n",
      "1.5465478040277958\n",
      "1.55223198980093\n",
      "1.430295579135418\n",
      "1.545867057517171\n",
      "1.4528679810464382\n",
      "1.5501207448542118\n",
      "1.4926851354539394\n",
      "1.568426065146923\n",
      "1.4201136976480484\n",
      "1.452279407531023\n",
      "1.4720410890877247\n",
      "1.502218745648861\n",
      "1.5169844552874565\n",
      "1.5122446939349174\n",
      "1.4931019265204668\n",
      "1.4685680456459522\n",
      "1.457806296646595\n",
      "1.5968447923660278\n",
      "1.5254961140453815\n",
      "1.4708710424602032\n",
      "1.4816406853497028\n",
      "1.5395986661314964\n",
      "1.4540648274123669\n",
      "1.564443714916706\n",
      "1.4731416841968894\n",
      "1.520595908164978\n",
      "1.536512166261673\n",
      "1.5081242509186268\n",
      "1.5610723495483398\n",
      "1.4856445640325546\n",
      "1.5299179665744305\n",
      "1.5026070810854435\n",
      "1.4435296654701233\n",
      "1.5171682424843311\n",
      "1.4339474141597748\n",
      "1.5261528119444847\n",
      "1.4470087084919214\n",
      "1.5167794795706868\n",
      "1.4860504381358624\n",
      "1.5704911053180695\n",
      "1.482702486217022\n",
      "1.4832049943506718\n",
      "1.4816536381840706\n",
      "1.5068050250411034\n",
      "1.5358786880970001\n",
      "1.4818146266043186\n",
      "1.4788332413882017\n",
      "1.4236611779779196\n",
      "1.5259310007095337\n",
      "1.488815177232027\n",
      "1.5326926056295633\n",
      "1.5157560426741838\n",
      "1.5181160867214203\n",
      "1.4642605856060982\n",
      "1.4604284195229411\n",
      "1.592883586883545\n",
      "1.4959896393120289\n",
      "1.4767561629414558\n",
      "1.4787955861538649\n",
      "1.4931730702519417\n",
      "1.466069770976901\n",
      "1.4961457699537277\n",
      "1.4895162056200206\n",
      "1.4772401675581932\n",
      "1.469912363216281\n",
      "1.433714684098959\n",
      "1.4622657159343362\n",
      "1.423592558130622\n",
      "1.5513706430792809\n",
      "1.4988321512937546\n",
      "1.5565542466938496\n",
      "1.5063281022012234\n",
      "1.469718374311924\n",
      "1.5274076964706182\n",
      "1.411648128181696\n",
      "1.4356690077111125\n",
      "1.4461475983262062\n",
      "1.4860470965504646\n",
      "1.5007095262408257\n",
      "1.5723242685198784\n",
      "1.4583766423165798\n",
      "1.5031790863722563\n",
      "1.4550679167732596\n",
      "1.4722207449376583\n",
      "1.4822402000427246\n",
      "1.5343078607693315\n",
      "1.4333279971033335\n",
      "1.442970521748066\n",
      "1.5464910818263888\n",
      "1.4682598374783993\n",
      "1.4944573789834976\n",
      "1.6127099990844727\n",
      "1.4632801373954862\n",
      "1.5183806046843529\n",
      "1.48752099275589\n",
      "1.5137958070263267\n",
      "1.392289187759161\n",
      "1.4914326593279839\n",
      "1.489974670112133\n",
      "1.472513284534216\n",
      "1.4227418000809848\n",
      "1.4745148345828056\n",
      "1.5956571102142334\n",
      "1.5383972493000329\n",
      "1.42902344558388\n",
      "1.4677783735096455\n",
      "1.4837777465581894\n",
      "1.4819573340937495\n",
      "1.5300433000084013\n",
      "1.5159880481660366\n",
      "1.3715884983539581\n",
      "1.3981654774397612\n",
      "1.4632435203529894\n",
      "1.456751050427556\n",
      "1.4861352369189262\n",
      "1.4681002739816904\n",
      "1.4901204407215118\n",
      "1.4681500075384974\n",
      "1.4987766118720174\n",
      "1.4781350754201412\n",
      "1.490084394812584\n",
      "1.4685794338583946\n",
      "1.44773660460487\n",
      "1.5071174558252096\n",
      "1.4330088272690773\n",
      "1.5244939010590315\n",
      "1.514373586513102\n",
      "1.4554907567799091\n",
      "1.4701553769409657\n",
      "1.4397305697202682\n",
      "1.4858738202601671\n",
      "1.5236885342746973\n",
      "1.4317939579486847\n",
      "1.51523969322443\n",
      "1.469115444459021\n",
      "1.4526354279369116\n",
      "1.4646980799734592\n",
      "1.495027994737029\n",
      "1.441531628370285\n",
      "1.4303471446037292\n",
      "1.473534937016666\n",
      "1.535455659031868\n",
      "1.4726539887487888\n",
      "1.4018102940171957\n",
      "1.4316950775682926\n",
      "1.4114604778587818\n",
      "1.5333801489323378\n",
      "1.4630434662103653\n",
      "1.4092265889048576\n",
      "1.4347757417708635\n",
      "1.4972609877586365\n",
      "1.423182550817728\n",
      "1.4924197234213352\n",
      "1.4188505113124847\n",
      "1.5045946827158332\n",
      "1.4705663658678532\n",
      "1.484202105551958\n",
      "1.5142921973019838\n",
      "1.5295607130974531\n",
      "1.4116971995681524\n",
      "1.5501881912350655\n",
      "1.443449154496193\n",
      "1.4958857111632824\n",
      "1.4068602044135332\n",
      "1.5610063821077347\n",
      "1.4136883625760674\n",
      "1.4849492004141212\n",
      "1.576600730419159\n",
      "1.4534015096724033\n",
      "1.432731345295906\n",
      "1.508594922721386\n",
      "1.5152961369603872\n",
      "1.458649918437004\n",
      "1.5314637385308743\n",
      "1.5517735108733177\n",
      "1.512590310536325\n",
      "1.4634053241461515\n",
      "1.468219481408596\n",
      "1.4982600510120392\n",
      "1.4860374928102829\n",
      "1.4418548792600632\n",
      "1.4878372242674232\n",
      "1.5540156904608011\n",
      "1.4631551075726748\n",
      "1.434569075703621\n",
      "1.3854303807020187\n",
      "1.46503022685647\n",
      "1.4144294536672533\n",
      "1.4563537910580635\n",
      "1.44436838850379\n",
      "1.4370799735188484\n",
      "1.4350246647372842\n",
      "1.425437898375094\n",
      "1.4812628664076328\n",
      "1.466281272470951\n",
      "1.4940640054410324\n",
      "1.4333745753392577\n",
      "1.5290443822741508\n",
      "1.41163966152817\n",
      "1.459045355208218\n",
      "1.4637009352445602\n",
      "1.440653495490551\n",
      "1.4941267264075577\n",
      "1.4216062697814777\n",
      "1.430282961577177\n",
      "1.3607014156877995\n",
      "1.472624721005559\n",
      "1.4639857923611999\n",
      "1.4986613392829895\n",
      "1.4780849888920784\n",
      "1.4403000120073557\n",
      "1.4440934211015701\n",
      "1.4937343737110496\n",
      "1.452047923579812\n",
      "1.5113576268777251\n",
      "1.483926210552454\n",
      "1.5046518854796886\n",
      "1.4294504951685667\n",
      "1.4845936223864555\n",
      "1.4469182528555393\n",
      "1.4718236699700356\n",
      "1.412315116263926\n",
      "1.4684132412075996\n",
      "1.4935319609940052\n",
      "1.4971060082316399\n",
      "1.4717447608709335\n",
      "1.4381408672779799\n",
      "1.4294225042685866\n",
      "1.5153635959140956\n",
      "1.4278702065348625\n",
      "1.4715831349603832\n",
      "1.4751868774183095\n",
      "1.5397618785500526\n",
      "1.5207374580204487\n",
      "1.5030657723546028\n",
      "1.4553740359842777\n",
      "1.5249979346990585\n",
      "1.410979631356895\n",
      "1.474387526512146\n",
      "1.4687122702598572\n",
      "1.40947544015944\n",
      "1.4759515523910522\n",
      "1.4863034337759018\n",
      "1.4473400190472603\n",
      "1.493492721579969\n",
      "1.5063054049387574\n",
      "1.5189326144754887\n",
      "1.4469373598694801\n",
      "1.5571105666458607\n",
      "1.5541454104240984\n",
      "1.4272946566343307\n",
      "1.4388544149696827\n",
      "1.407840522006154\n",
      "1.4597776317968965\n",
      "1.4258197024464607\n",
      "1.4685539342463017\n",
      "1.4853961020708084\n",
      "1.4274460561573505\n",
      "1.4851889982819557\n",
      "1.558865960687399\n",
      "1.4926618188619614\n",
      "1.4683001041412354\n",
      "1.457098700106144\n",
      "1.5149262072518468\n",
      "1.4430125057697296\n",
      "1.3900980688631535\n",
      "1.488779567182064\n",
      "1.4981272732838988\n",
      "1.4001052659004927\n",
      "1.4520901292562485\n",
      "1.4710222287103534\n",
      "1.4969237996265292\n",
      "1.4909581318497658\n",
      "1.366817869246006\n",
      "1.6169537529349327\n",
      "1.4930127151310444\n",
      "1.4365424010902643\n",
      "1.4665254950523376\n",
      "1.4854135578498244\n",
      "1.4569062776863575\n",
      "1.4944820553064346\n",
      "1.5014557987451553\n",
      "1.4121354296803474\n",
      "1.4585915761999786\n",
      "1.415061417967081\n",
      "1.4354427549988031\n",
      "1.386938437819481\n",
      "1.4507523085922003\n",
      "1.4173756828531623\n",
      "1.4034491525962949\n",
      "1.4931495301425457\n",
      "1.504182830452919\n",
      "1.4672729643061757\n",
      "1.4216868355870247\n",
      "1.4695347070228308\n",
      "1.4298335574567318\n",
      "1.4094529375433922\n",
      "1.4305762071162462\n",
      "1.4204106633551419\n",
      "1.4795499593019485\n",
      "1.487472228705883\n",
      "1.3785014357417822\n",
      "1.4680029675364494\n",
      "1.483117789030075\n",
      "1.438451498746872\n",
      "1.4930693041533232\n",
      "1.4160228334367275\n",
      "1.4798805229365826\n",
      "1.5298891738057137\n",
      "1.4326558124739677\n",
      "1.485274288803339\n",
      "1.4868105435743928\n",
      "1.4126674737781286\n",
      "1.4764657318592072\n",
      "1.4584312494844198\n",
      "1.4786574468016624\n",
      "1.5395595245063305\n",
      "1.4858228154480457\n",
      "1.4161976538598537\n",
      "1.5763178169727325\n",
      "1.5159306405112147\n",
      "1.4826058149337769\n",
      "1.4772190246731043\n",
      "1.4371367692947388\n",
      "1.432542324066162\n",
      "1.459645631723106\n",
      "1.3555905129760504\n",
      "1.4235997218638659\n",
      "1.42120873183012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m training_datasets[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     av_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(av_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    av_loss = 0\n",
    "    for dataset in training_datasets[::-1]:\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "        loss = model.train_step(optimizer, criterion, dataloader).item()\n",
    "        av_loss += loss / len(dataloader)\n",
    "    print(av_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.46it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Check for large sequence length generalization\n",
    "\n",
    "# Evaluate model generalization for sequences of varying lengths\n",
    "N = 100\n",
    "stepsize = 1\n",
    "val_acc = []\n",
    "for n in trange(1, N + 1, stepsize):\n",
    "    val_data = seq_data(device, problem, encoding, n_datapoints=10, seq_len=n)\n",
    "    dataloader = torch.utils.data.DataLoader(val_data, batch_size=1)\n",
    "    count = 0\n",
    "    for x, y in dataloader:\n",
    "        target = torch.argmax(y).item()\n",
    "        # out = model.transformer.encoder(x)[0][:, -1, :]\n",
    "        # prediction = torch.argmax(out).item()\n",
    "        prediction = model.predict(x)\n",
    "        if prediction == target:\n",
    "            count += 1\n",
    "\n",
    "    accuracy = count / len(dataloader)\n",
    "    val_acc.append(accuracy)\n",
    "\n",
    "# Visualize validation error for varying sequence lengths\n",
    "publication.set_color_mixed()\n",
    "fig = plt.figure(figsize=(6, 2))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.bar(np.arange(1, N + 1, stepsize), val_acc, color=\"skyblue\")\n",
    "ax.set_xlabel(\"Sequence lengths\")\n",
    "ax.set_ylabel(\"Validation accuracy\")\n",
    "# ax.set_title(\"Model Generalization Across Sequence Lengths\")\n",
    "ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "if save:\n",
    "    publication.pub_show(save_path=f\"plots/automaton_dynamics/{settings}_validation\")\n",
    "else:\n",
    "    publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8025, 0.2037]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8025, 0.2037]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7082, 0.3869]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7082, 0.3869]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9391, 0.1279]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9391, 0.1279]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5576, 0.5253]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5576, 0.5253]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 9.9233e-01, -9.4285e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.9233e-01, -9.4285e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5986, 0.3060]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5986, 0.3060]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3755, -0.0054]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3755, -0.0054]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3127, -0.0035]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3127, -0.0035]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4923, 0.5684]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4923, 0.5684]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1662, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1662, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8461, 0.1071]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8461, 0.1071]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5006, 0.6429]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5006, 0.6429]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7064, 0.3622]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7064, 0.3622]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5922, 0.4213]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5922, 0.4213]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2512, -0.0031]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2512, -0.0031]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2572, -0.0028]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2572, -0.0028]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7282, 0.3617]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7282, 0.3617]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6314, 0.3570]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6314, 0.3570]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6818, 0.3755]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6818, 0.3755]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4936, 0.5853]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4936, 0.5853]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7360, 0.2565]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7360, 0.2565]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9513, 0.0683]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9513, 0.0683]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6553, 0.3735]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6553, 0.3735]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8056, 0.2132]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8056, 0.2132]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6173, 0.3742]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6173, 0.3742]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6188, 0.4744]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6188, 0.4744]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9030, 0.0119]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9030, 0.0119]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9208, 0.0617]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9208, 0.0617]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8002, 0.2700]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8002, 0.2700]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7913, 0.1720]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7913, 0.1720]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1278, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1278, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6742, 0.4088]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6742, 0.4088]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.3117, -0.0033]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3117, -0.0033]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7397, 0.2722]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7397, 0.2722]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5542, 0.5312]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5542, 0.5312]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1793, -0.0035]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1793, -0.0035]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[1.0040, 0.0902]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[1.0040, 0.0902]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5460, 0.4884]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5460, 0.4884]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5070, 0.5605]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5070, 0.5605]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4404, 0.6766]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4404, 0.6766]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[1.0648, 0.0052]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[1.0648, 0.0052]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8538, 0.1436]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8538, 0.1436]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.4380, -0.0053]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.4380, -0.0053]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6482, 0.1929]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6482, 0.1929]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6680, 0.2420]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6680, 0.2420]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.3480, -0.0037]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3480, -0.0037]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0812e+00, -7.2257e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0812e+00, -7.2257e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3035, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3035, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9076, 0.0166]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9076, 0.0166]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2127, -0.0039]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2127, -0.0039]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5744, 0.4402]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5744, 0.4402]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.4626e-01, -3.2996e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.4626e-01, -3.2996e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7626, 0.1574]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7626, 0.1574]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 8.4525e-01, -1.3188e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 8.4525e-01, -1.3188e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1176, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1176, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0927, -0.0020]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0927, -0.0020]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5385, 0.5073]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5385, 0.5073]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3255, -0.0043]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3255, -0.0043]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7427, 0.3162]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7427, 0.3162]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8222, 0.2526]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8222, 0.2526]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8630, 0.0800]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8630, 0.0800]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7270, 0.3883]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7270, 0.3883]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0940, -0.0027]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0940, -0.0027]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7521, 0.1375]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7521, 0.1375]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8850, 0.0584]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8850, 0.0584]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 9.8289e-01, -4.6824e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.8289e-01, -4.6824e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.4251, 0.6688]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4251, 0.6688]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6220, 0.2128]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6220, 0.2128]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5809, 0.3320]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5809, 0.3320]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.1606e-01, -1.3161e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.1606e-01, -1.3161e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9729, 0.1743]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9729, 0.1743]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5331, 0.5726]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5331, 0.5726]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0385, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0385, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6578, 0.2021]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6578, 0.2021]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4068, 0.7317]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4068, 0.7317]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9284, 0.0393]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9284, 0.0393]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1352, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1352, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 9.5063e-01, -5.8151e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.5063e-01, -5.8151e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5344, 0.4589]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5344, 0.4589]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8162, 0.0868]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8162, 0.0868]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.3745, 0.6458]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.3745, 0.6458]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6406, 0.3696]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6406, 0.3696]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7229, 0.4643]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7229, 0.4643]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0435, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0435, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7802, 0.3955]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7802, 0.3955]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.7343e-01, -1.3563e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.7343e-01, -1.3563e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0559, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0559, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7743, 0.1781]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7743, 0.1781]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8615, 0.2329]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8615, 0.2329]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 8.9305e-01, -7.1445e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 8.9305e-01, -7.1445e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8986, 0.0362]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8986, 0.0362]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6816, 0.2572]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6816, 0.2572]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.3249e-01, -9.1858e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.3249e-01, -9.1858e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5668, 0.5315]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5668, 0.5315]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7202, 0.2473]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7202, 0.2473]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1873e+00, -9.8909e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.1873e+00, -9.8909e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[1.0612, 0.0634]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[1.0612, 0.0634]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5418, 0.5859]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5418, 0.5859]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.4519, -0.0050]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.4519, -0.0050]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.3835e-01, -7.2461e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.3835e-01, -7.2461e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9326, 0.0770]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9326, 0.0770]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6107, 0.3865]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6107, 0.3865]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3401, -0.0041]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3401, -0.0041]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6972, 0.3324]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6972, 0.3324]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6741, 0.3816]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6741, 0.3816]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6366, 0.4419]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6366, 0.4419]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6985, 0.1402]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6985, 0.1402]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.2864e-01, -6.3036e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.2864e-01, -6.3036e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7417, 0.2039]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7417, 0.2039]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0349, -0.0031]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0349, -0.0031]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.4597, 0.6207]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4597, 0.6207]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1439, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1439, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[1.0634, 0.0565]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[1.0634, 0.0565]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.4827e-01, -2.4307e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.4827e-01, -2.4307e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8166, 0.2460]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8166, 0.2460]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.5156, -0.0042]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.5156, -0.0042]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2945, -0.0052]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2945, -0.0052]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7054, 0.3389]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7054, 0.3389]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7830, 0.3204]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7830, 0.3204]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4049, 0.7310]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4049, 0.7310]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2396, -0.0037]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2396, -0.0037]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7773, 0.2427]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7773, 0.2427]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0158, -0.0011]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0158, -0.0011]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.3580, 0.6887]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.3580, 0.6887]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8510, 0.1156]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8510, 0.1156]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9505, 0.0097]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9505, 0.0097]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6919, 0.3405]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6919, 0.3405]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2190, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2190, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8052, 0.1441]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8052, 0.1441]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7294, 0.2641]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7294, 0.2641]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2325, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2325, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0452, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0452, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7468, 0.2049]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7468, 0.2049]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9321, 0.1519]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9321, 0.1519]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3594, -0.0048]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3594, -0.0048]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6853, 0.4839]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6853, 0.4839]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6500, 0.3689]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6500, 0.3689]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.3110e-01, -5.9616e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.3110e-01, -5.9616e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5851, 0.3266]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5851, 0.3266]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7642, 0.3080]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7642, 0.3080]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7334, 0.1705]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7334, 0.1705]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5678, 0.4567]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5678, 0.4567]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 8.2406e-01, -3.2632e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 8.2406e-01, -3.2632e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8462, 0.1778]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8462, 0.1778]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8370, 0.2817]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8370, 0.2817]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7558, 0.2464]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7558, 0.2464]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6615, 0.4859]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6615, 0.4859]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6586, 0.3494]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6586, 0.3494]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7469, 0.1682]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7469, 0.1682]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6214, 0.3594]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6214, 0.3594]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5637, 0.5905]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5637, 0.5905]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9248, 0.0786]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9248, 0.0786]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4712, 0.6900]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4712, 0.6900]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1179, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1179, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0774, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0774, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4703, 0.6164]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4703, 0.6164]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7101, 0.4400]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7101, 0.4400]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7691, 0.2100]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7691, 0.2100]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9697, 0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9697, 0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.3341, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3341, -0.0014]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1586, -0.0017]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1586, -0.0017]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5696, 0.4980]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5696, 0.4980]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7042, 0.3406]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7042, 0.3406]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5274, 0.5140]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5274, 0.5140]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0248, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0248, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.9854, 0.0246]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9854, 0.0246]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5650, 0.4650]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5650, 0.4650]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3724, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3724, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6780, 0.2057]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6780, 0.2057]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2266, -0.0038]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2266, -0.0038]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7420, 0.2634]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7420, 0.2634]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2279, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2279, -0.0021]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8896, 0.0193]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8896, 0.0193]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 8.7318e-01, -5.2679e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 8.7318e-01, -5.2679e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.3273, -0.0039]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3273, -0.0039]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5684, 0.5232]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5684, 0.5232]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7774, 0.2959]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7774, 0.2959]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6834, 0.2252]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6834, 0.2252]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.4683, 0.6510]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4683, 0.6510]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0701e+00, -7.6371e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0701e+00, -7.6371e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7891, 0.1902]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7891, 0.1902]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8649, 0.0186]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8649, 0.0186]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2024, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2024, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9064, 0.1785]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9064, 0.1785]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.3799, 0.6771]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.3799, 0.6771]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7737, 0.1192]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7737, 0.1192]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2856, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2856, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0082, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0082, -0.0013]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6004, 0.4765]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6004, 0.4765]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.5193e-01, -1.0690e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.5193e-01, -1.0690e-05]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0730, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0730, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8116, 0.0293]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8116, 0.0293]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.9106, 0.0148]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.9106, 0.0148]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7894, 0.0856]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7894, 0.0856]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6941, 0.2801]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6941, 0.2801]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1092, -0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1092, -0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6788, 0.2437]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6788, 0.2437]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6885, 0.3555]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6885, 0.3555]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8998, 0.1259]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8998, 0.1259]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[1.0065, 0.0434]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[1.0065, 0.0434]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8894, 0.1470]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8894, 0.1470]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7504, 0.3240]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7504, 0.3240]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.5100, -0.0068]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.5100, -0.0068]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8485, 0.0116]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8485, 0.0116]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0171e+00, -5.7792e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0171e+00, -5.7792e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1447, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1447, -0.0026]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2757, -0.0036]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2757, -0.0036]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 0.9784, -0.0010]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 0.9784, -0.0010]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 9.7172e-01, -1.4494e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.7172e-01, -1.4494e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7446, 0.2569]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7446, 0.2569]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1331e+00, -6.8559e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.1331e+00, -6.8559e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5791, 0.4290]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5791, 0.4290]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0625, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0625, -0.0015]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1182, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1182, -0.0018]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0058e+00, -7.3093e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0058e+00, -7.3093e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8402, 0.0533]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8402, 0.0533]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0926e+00, -3.2290e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0926e+00, -3.2290e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2796, -0.0022]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2796, -0.0022]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8584, 0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8584, 0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2363, -0.0028]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2363, -0.0028]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0561e+00, -7.0117e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0561e+00, -7.0117e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.8715, 0.0349]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8715, 0.0349]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2244, -0.0025]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2244, -0.0025]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2290, -0.0030]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2290, -0.0030]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.4927, 0.5075]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4927, 0.5075]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 8.9671e-01, -3.8312e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 8.9671e-01, -3.8312e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6826, 0.4803]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6826, 0.4803]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6910, 0.2867]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6910, 0.2867]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.0839, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0839, -0.0024]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7203, 0.3852]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7203, 0.3852]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1507, -0.0025]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1507, -0.0025]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.1631, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1631, -0.0016]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8514, 0.1663]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8514, 0.1663]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.5327, 0.5370]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5327, 0.5370]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6975, 0.2848]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6975, 0.2848]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.2910, -0.0046]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2910, -0.0046]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0324e+00, -6.1539e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0324e+00, -6.1539e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7005, 0.2132]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7005, 0.2132]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6147, 0.2992]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6147, 0.2992]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2930, -0.0045]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2930, -0.0045]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6719, 0.3232]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6719, 0.3232]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0079, -0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.0079, -0.0012]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[ 1.1347, -0.0023]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.1347, -0.0023]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7998, 0.0894]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7998, 0.0894]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.0356e+00, -7.5152e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.0356e+00, -7.5152e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8925, 0.0986]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8925, 0.0986]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 9.4758e-01, -2.7224e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 9.4758e-01, -2.7224e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[ 1.2704, -0.0034]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.2704, -0.0034]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6783, 0.4240]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6783, 0.4240]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.6874, 0.3966]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6874, 0.3966]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.8488, 0.3176]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.8488, 0.3176]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.4813, 0.6289]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.4813, 0.6289]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5614, 0.4242]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5614, 0.4242]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5899, 0.4649]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5899, 0.4649]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7966, 0.1306]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7966, 0.1306]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.7342, 0.3868]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7342, 0.3868]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48828125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(training_datasets[-1], batch_size=1)\n",
    "count = 0\n",
    "for x, y in dataloader:\n",
    "    target = torch.argmax(y).item()\n",
    "    print(x)\n",
    "    print(model(x, y))\n",
    "    prediction = model.predict(x)\n",
    "    if prediction == target:\n",
    "        count += 1\n",
    "\n",
    "accuracy = count / len(dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2000 [00:00<?, ?steps/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Transformer.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Training run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:228\u001b[0m, in \u001b[0;36mCompiler.training_run\u001b[0;34m(self, training_datasets, n_epochs, batch_size, progress_bar, conv_thresh)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tracker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m    231\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, trainloader)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m/\u001b[39m n_train_data)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trainloader, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(trainloaders, training_datasets)\n\u001b[1;32m    235\u001b[0m )\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:83\u001b[0m, in \u001b[0;36mScalarTracker.track\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# initial_hidden = model.init_hidden(batch_size=1)[-1]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# hidden_function = lambda inputs: model(inputs)[1][-1]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m output_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m compiler\u001b[38;5;241m.\u001b[39mtrackers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: ScalarTracker(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_datasets\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# \"hidden\": ActivationTracker(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#     encoding,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#     hidden_function,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#     analysis_data,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#     initial=lambda: initial_hidden,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# \"output\": ActivationTracker(encoding, output_function, analysis_data),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m }\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:180\u001b[0m, in \u001b[0;36mCompiler.validation\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    178\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    179\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(\n\u001b[0;32m--> 180\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39msqueeze(outputs)\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqueeze(loss_this_dataset)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([loss_this_dataset], [i])\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "## Training run\n",
    "compiler.training_run(\n",
    "    training_datasets,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute percentage merged as function of seq len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ON TRANSFORMER TEST BEHAVIOUR ON ARBITRAY LONG RANGE SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize automaton dynamics\n",
    "\n",
    "# Filter and process data for visualization\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "query = \"(Dataset <10) \"\n",
    "data_hid = data_hid.query(query).copy()\n",
    "data_output = data_output.query(query).copy()\n",
    "\n",
    "epochs = list(set(data_hid.index.get_level_values(\"Epoch\")))\n",
    "epochs.sort()\n",
    "\n",
    "std = float(np.linalg.norm(data_hid.std()))\n",
    "n_points = len(data_hid.query(\"Epoch == 0\"))\n",
    "\n",
    "automaton_history = to_automaton_history(\n",
    "    data_hid, data_output, merge_distance=0.1 * std\n",
    ")\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "n_states = np.array([len(automaton.states) for automaton in automaton_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot number of reduced states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = sum(len(dataset) for dataset in analysis_data)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "# plt.plot(n_states / n_datapoints, label=\"Number of states\", zorder=0, color=\"0.5\")\n",
    "plt.plot(n_states / n_datapoints, label=\"Number of states\", zorder=0)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "if save:\n",
    "    publication.plt_show(save_path=f\"plots/automaton_dynamics/{settings}_loss\")\n",
    "else:\n",
    "    publication.plt_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = sum(len(dataset) for dataset in training_datasets)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "if save:\n",
    "    publication.plt_show(save_path=f\"plots/automaton_dynamics/{settings}_loss_small\")\n",
    "else:\n",
    "    publication.plt_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match settings:\n",
    "    case \"rich\":\n",
    "        epoch_choices = [0, 1000, 1999]\n",
    "    case \"intermediate\":\n",
    "        epoch_choices = [0, 350, 660, 1999]\n",
    "    case \"lazy\":\n",
    "        epoch_choices = [0, 1999]\n",
    "    case \"low_data\":\n",
    "        epoch_choices = [0, 600,1999]\n",
    "\n",
    "for epoch in epoch_choices:\n",
    "    automaton = automaton_history[epoch]\n",
    "    display_automata(automaton)\n",
    "    if save:\n",
    "        publication.pub_show(\n",
    "            save_path=f\"plots/automaton_dynamics/{settings}_automaton_epoch_{epoch}\"\n",
    "        )\n",
    "    else:\n",
    "        publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automaton = to_automaton(\n",
    "    hidden_function,\n",
    "    output_function,\n",
    "    initial_hidden,\n",
    "    training_datasets,\n",
    "    encoding,\n",
    "    merge_distance_frac=0.1,\n",
    ")\n",
    "display_automata(reduce_automaton(automaton))\n",
    "if save:\n",
    "    publication.pub_show(\n",
    "        save_path=f\"plots/automaton_dynamics/{settings}_automaton_reduced\"\n",
    "    )\n",
    "else:\n",
    "    publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation\n",
    "publication.set_color_mixed()\n",
    "animation = SliderAnimation(\n",
    "    {\n",
    "        \"Hidden representations\": ActivationsAnimation(\n",
    "            data_hid, transform=\"PCA\", colors=[5] * n_points\n",
    "        ),\n",
    "        \"Output\": ActivationsAnimation(\n",
    "            data_output,\n",
    "            transform=\"none\",\n",
    "            fixed_points=encoding.encoding,\n",
    "            colors=[6] * n_points,\n",
    "        ),\n",
    "        \"Automaton\": AutomatonAnimation(automaton_history, reduce_automata=False),\n",
    "        \"Loss\": EpochAnimation(\n",
    "            graphs={\n",
    "                \"Training loss\": train_loss,\n",
    "                \"Validation loss\": val_loss,\n",
    "            },\n",
    "            unitless_graphs={\n",
    "                \"Number of states\": n_states,\n",
    "            },\n",
    "            y_bounds=(0, 1),\n",
    "        ),\n",
    "    },\n",
    "    parameters=epochs,\n",
    "    parameter_name=\"Epoch\",\n",
    "    fig_size=4,\n",
    ")\n",
    "\n",
    "# Optionally export the animation\n",
    "# if save:\n",
    "#     animation.to_gif(f\"plots/automaton_dynamics_{settings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "counts = []\n",
    "# for n, h in tqdm(data_hid.groupby(\"Epoch\")):\n",
    "#     H = h.to_numpy()\n",
    "#     if n == 0:\n",
    "#         dist = scipy.spatial.distance_matrix(H, H)\n",
    "#         threshold = 0.01 * np.max(dist)\n",
    "#     clustering = AgglomerativeClustering(\n",
    "#         n_clusters=None, distance_threshold=threshold\n",
    "#     ).fit(H)\n",
    "#     count = max(clustering.labels_)\n",
    "#     counts.append(count)\n",
    "\n",
    "\n",
    "def count_states(H):\n",
    "    dist = scipy.spatial.distance_matrix(H, H)\n",
    "    fraction = 1 / dist\n",
    "    fraction = np.sum(np.nan_to_num(fraction, nan=0.0, posinf=0.0, neginf=0.0))\n",
    "    return fraction\n",
    "\n",
    "\n",
    "for n, h in tqdm(data_hid.groupby(\"Epoch\")):\n",
    "    H = h.to_numpy()\n",
    "    counts.append(count_states(H))\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "# counts = scipy.ndimage.gaussian_filter(counts, 5)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.plot(np.array(counts) / (H.shape[0]))\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_structure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
