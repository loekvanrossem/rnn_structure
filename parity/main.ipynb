{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numba import njit\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data, gen_rand_seq\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import (\n",
    "    to_automaton_history,\n",
    "    reduce_automaton,\n",
    "    to_automaton,\n",
    "    gen_rand_automaton,\n",
    ")\n",
    "from visualization.automata import display_automata, AutomatonAnimation\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.epochs import EpochAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "import visualization.publication as publication\n",
    "from model import Model\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings parity task with relu\n",
    "settings = \"parity\"\n",
    "nonlinearity = \"relu\"\n",
    "P = 100\n",
    "L = 1\n",
    "\n",
    "gain = 0.1\n",
    "lr = 0.02\n",
    "n_epochs = 1000\n",
    "\n",
    "min_seq_len = 1 \n",
    "max_seq_len = 10\n",
    "\n",
    "symbols = [0, 1]\n",
    "encoding = OneHot(symbols)\n",
    "automaton_problem = None\n",
    "problem = lambda seq: int(np.sum(seq) % 2)\n",
    "\n",
    "thresh_factor = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Settings parity task with relu larger weights\n",
    "# settings = \"parity\"\n",
    "# nonlinearity = \"relu\"\n",
    "# P = 100\n",
    "# L = 1\n",
    "\n",
    "# gain = 0.7\n",
    "# lr = 0.02\n",
    "# n_epochs = 1000\n",
    "\n",
    "# min_seq_len = 1\n",
    "# max_seq_len = 10\n",
    "\n",
    "# symbols = [0, 1]\n",
    "# encoding = OneHot(symbols)\n",
    "# automaton_problem = None\n",
    "# problem = lambda seq: int(np.sum(seq) % 2)\n",
    "\n",
    "# thresh_factor = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Settings parity task with tanh\n",
    "# settings = \"parity_tanh\"\n",
    "# nonlinearity = \"tanh\"\n",
    "# P = 100\n",
    "# L = 1\n",
    "\n",
    "# gain = 0.8\n",
    "# lr = 0.05\n",
    "# n_epochs = 1000\n",
    "\n",
    "# min_seq_len = 1\n",
    "# max_seq_len = 10\n",
    "\n",
    "# symbols = [0, 1]\n",
    "# encoding = OneHot(symbols)\n",
    "# automaton_problem = None\n",
    "# problem = lambda seq: int(np.sum(seq) % 2)\n",
    "\n",
    "# thresh_factor = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Settings random task with relu\n",
    "# settings = \"random\"\n",
    "# nonlinearity = \"relu\"\n",
    "# P = 100\n",
    "# L = 1\n",
    "\n",
    "# gain = 0.025\n",
    "# lr = 0.05\n",
    "# n_epochs = 1000\n",
    "\n",
    "# max_problem_size = 7\n",
    "# min_seq_len = 1\n",
    "# max_seq_len = 10\n",
    "\n",
    "# symbols = [0, 1]\n",
    "# encoding = OneHot(symbols)\n",
    "# automaton_problem = gen_rand_automaton(symbols, [[1, 0], [0, 1]], max_problem_size, 0.6)\n",
    "# problem = lambda seq: int(np.argmax(automaton_problem.compute(seq)))\n",
    "\n",
    "# thresh_factor = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "# Define sequence lengths for training and validation datasets\n",
    "train_seq_lengths = list(range(min_seq_len, max_seq_len + 1))\n",
    "analysis_seq_lengths = list(range(1, max_seq_len + 1))\n",
    "val_seq_length = 50\n",
    "val_datapoints = 100\n",
    "\n",
    "long_seq_lens = [25, 100, 250, 1000, 2500, 10000]\n",
    "# long_seq_lens = list(np.logspace(0.5,  4, 50,dtype=int))\n",
    "# long_seq_lens[0] = 2\n",
    "# long_seq_lens.insert(0, 1)\n",
    "long_seq_lens = list(map(int,long_seq_lens))\n",
    "\n",
    "# Generate datasets\n",
    "training_datasets = [\n",
    "    seq_data(device, problem, encoding, seq_len=length) for length in train_seq_lengths\n",
    "]\n",
    "validation_datasets = [\n",
    "    seq_data(\n",
    "        device, problem, encoding, n_datapoints=val_datapoints, seq_len=val_seq_length\n",
    "    )\n",
    "]\n",
    "analysis_data = [\n",
    "    seq_data(device, problem, encoding, seq_len=length)\n",
    "    for length in analysis_seq_lengths\n",
    "]\n",
    "long_seq_val_datasets = [\n",
    "    seq_data(device, problem, encoding, n_datapoints=10, seq_len=length)\n",
    "    for length in long_seq_lens\n",
    "]\n",
    "tracked_datasets = (\n",
    "    validation_datasets + analysis_data + training_datasets + long_seq_val_datasets\n",
    ")\n",
    "\n",
    "\n",
    "if automaton_problem:\n",
    "    display_automata(reduce_automaton(automaton_problem))\n",
    "    publication.pub_show(save_path=f\"plots/{settings}_automaton_task_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate model\n",
    "model = Model(\n",
    "    encoding=encoding,\n",
    "    input_size=len(symbols),\n",
    "    output_size=len(symbols),\n",
    "    hidden_dim=P,\n",
    "    n_layers=L,\n",
    "    device=device,\n",
    "    nonlinearity=nonlinearity,\n",
    "    gain=gain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X =[]\n",
    "# for x,y in torch.utils.data.DataLoader(training_datasets[4], batch_size=1):\n",
    "#     X.append(x)\n",
    "\n",
    "# x_1 = X[0]\n",
    "\n",
    "# X = []\n",
    "# for x,y in torch.utils.data.DataLoader(training_datasets[8], batch_size=1):\n",
    "#     X.append(x)\n",
    "\n",
    "# x_2 = X[1]\n",
    "\n",
    "# def approx_acc():\n",
    "\n",
    "\n",
    "#     dx = (x_2-x_1).flatten(start_dim=1).T\n",
    "\n",
    "#     with torch.backends.cudnn.flags(enabled=False):\n",
    "#         J = torch.autograd.functional.jacobian(lambda x: model.rnn(x)[1], x_1)\n",
    "#         H = torch.autograd.functional.hessian(lambda x: torch.norm(model.rnn(x)[1]), x_1)\n",
    "\n",
    "#     J = torch.squeeze(J).flatten(start_dim=1)\n",
    "#     H = torch.squeeze(H).flatten(start_dim=2).flatten(end_dim=1)\n",
    "\n",
    "#     first_order_term = torch.norm(J @ dx)\n",
    "#     second_order_term = 0.5 * torch.norm(dx.T @ H @ dx)\n",
    "\n",
    "#     ratio = second_order_term / first_order_term\n",
    "\n",
    "#     return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup compiler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0, weight_decay=0)\n",
    "compiler = Compiler(model, criterion, optimizer)\n",
    "initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "hidden_function = lambda inputs: model(inputs)[1][-1]\n",
    "output_function = lambda inputs: model(inputs)[0]\n",
    "compiler.trackers = {\n",
    "    \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    \"hidden\": ActivationTracker(\n",
    "        encoding,\n",
    "        hidden_function,\n",
    "        analysis_data,\n",
    "        initial=lambda: initial_hidden,\n",
    "    ),\n",
    "    \"output\": ActivationTracker(encoding, output_function, analysis_data),\n",
    "}\n",
    "# compiler.trackers.update({f'aprox_{n}':ScalarTracker(lambda x_1=x_1,x_2=x_2:approx_acc(x_1,x_2)) for n,(x_1,x_2) in enumerate(zip(X_1,X_2))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "compiler.training_run(\n",
    "    training_datasets,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = []\n",
    "# for n in range(len(X_1)):\n",
    "#     approximation_error = [r.item() for r in compiler.trackers[f\"aprox_{n}\"]._trace]\n",
    "#     if torch.norm(model(X_1[n])[0] - model(X_2[n])[0]) < threshold:\n",
    "#         errors.append(approximation_error)\n",
    "# errors=np.array(errors)\n",
    "# mean_error_conv = np.mean(errors,axis=0)\n",
    "\n",
    "# errors = []\n",
    "# for n in range(len(X_1)):\n",
    "#     approximation_error = [r.item() for r in compiler.trackers[f\"aprox_{n}\"]._trace]\n",
    "#     if torch.norm(model(X_1[n])[0] - model(X_2[n])[0]) > threshold:\n",
    "#         errors.append(approximation_error)\n",
    "# errors=np.array(errors)\n",
    "# mean_error_div = np.mean(errors,axis=0)\n",
    "\n",
    "# plt.figure(figsize=(4,3))\n",
    "# publication.set_color_mixed()\n",
    "# publication.skip_colors(5)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Ratio second / first order term\")\n",
    "# plt.plot(mean_error_conv,label=\"Mean converging pairs\")\n",
    "# plt.plot(mean_error_div,label=\"Mean diverging pairs\")\n",
    "# plt.legend()\n",
    "# publication.plt_show(save_path=f\"plots/approximation_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize automaton dynamics\n",
    "\n",
    "# Filter and process data for visualization\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "query = \"(Dataset <= 10)\"\n",
    "data_hid = data_hid.query(query).copy()\n",
    "data_output = data_output.query(query).copy()\n",
    "\n",
    "epochs = list(set(data_hid.index.get_level_values(\"Epoch\")))\n",
    "epochs.sort()\n",
    "\n",
    "std = float(np.linalg.norm(data_hid.std()))\n",
    "threshold = thresh_factor * std\n",
    "n_points = len(data_hid.query(\"Epoch == 0\"))\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "automaton_history = to_automaton_history(\n",
    "    data_hid, data_output, merge_distance=threshold\n",
    ")\n",
    "n_states = np.array([len(automaton.states) for automaton in automaton_history])\n",
    "is_finite = np.array([automaton.is_finite() for automaton in automaton_history])\n",
    "\n",
    "n_datapoints = sum(len(dataset) for dataset in analysis_data)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(np.array(train_loss), label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.plot(n_states / n_datapoints, label=\"Number of states\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "# plt.ylim(0, 0.3)\n",
    "publication.plt_show(save_path=f\"plots/{settings}_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Long sequence validation\n",
    "long_seq_losses = [\n",
    "    loss.query(f\"Dataset=={len(tracked_datasets)-n-1}\")[0].to_numpy()\n",
    "    for n in range(len(long_seq_lens))\n",
    "]\n",
    "long_seq_losses = long_seq_losses[::-1]\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(epochs, train_loss, label=\"Train\")\n",
    "\n",
    "\n",
    "publication.set_color_gradient(1, len(long_seq_lens))\n",
    "plt.gca().set_prop_cycle(None)\n",
    "for length, loss_this_len in zip(long_seq_lens, long_seq_losses):\n",
    "    plt.plot(epochs, loss_this_len, label=length)\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "publication.plt_show(save_path=f\"plots/{settings}_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display automaton during training\n",
    "\n",
    "epoch_choices = [0]\n",
    "\n",
    "for epoch in epoch_choices:\n",
    "    automaton = automaton_history[epoch]\n",
    "    display_automata(automaton)\n",
    "    publication.pub_show(save_path=f\"plots/{settings}_automaton_epoch_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare task to learned automaton\n",
    "\n",
    "learned_automaton = to_automaton(\n",
    "    hidden_function,\n",
    "    output_function,\n",
    "    initial_hidden,\n",
    "    analysis_data,\n",
    "    encoding,\n",
    "    merge_distance_frac=threshold,\n",
    ")\n",
    "\n",
    "\n",
    "display_automata(learned_automaton)\n",
    "publication.pub_show(save_path=f\"plots/{settings}_automaton\")\n",
    "\n",
    "display_automata(reduce_automaton(learned_automaton))\n",
    "publication.pub_show(save_path=f\"plots/{settings}_automaton_reduced\")\n",
    "\n",
    "if automaton_problem:\n",
    "    display_automata(reduce_automaton(automaton_problem))\n",
    "    publication.pub_show(save_path=f\"plots/{settings}_automaton_task_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = SliderAnimation(\n",
    "    {\n",
    "        \" \": EpochAnimation(\n",
    "            graphs={\n",
    "                \"Training loss\": train_loss,\n",
    "                \"Validation loss\": val_loss,\n",
    "            },\n",
    "            unitless_graphs={\n",
    "                \"Number of states\": n_states,\n",
    "            },\n",
    "            y_bounds=(0, 1),\n",
    "        ),\n",
    "        \"Hidden representations\": ActivationsAnimation(\n",
    "            data_hid.query(\"Input != 'initial'\"), transform=\"PCA\", colors=[5] * n_points,plot_labels=False\n",
    "        ),\n",
    "        # \"Output\": ActivationsAnimation(\n",
    "        #     data_output,\n",
    "        #     transform=\"none\",\n",
    "        #     fixed_points=encoding.encoding,\n",
    "        #     colors=[6] * n_points,\n",
    "        #     plot_labels=False,\n",
    "        # ),\n",
    "        \"Automaton\": AutomatonAnimation(automaton_history, reduce_automata=False),\n",
    "\n",
    "    },\n",
    "    parameters=epochs[350:1250],\n",
    "    parameter_name=\"Epoch\",\n",
    "    fig_size=5,\n",
    ")\n",
    "# animation.to_gif(f\"plots/automaton_dynamics_{settings}\", frame_duration=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot number of diverging state mergers per sequence length pairs\n",
    "last_epoch = int(data_hid.index.get_level_values(\"Epoch\")[-1])\n",
    "data_hid_analyze = data_hid.query(\"Dataset != -1\")\n",
    "\n",
    "merger_counts = np.zeros((len(training_datasets), len(training_datasets)))\n",
    "diverging_merger_counts = np.zeros((len(training_datasets), len(training_datasets)))\n",
    "total_mergers = 0\n",
    "total_possible_mergers = 0\n",
    "total_pred_mergers = 0\n",
    "total_unpred_mergers = 0\n",
    "total_diverging_mergers = 0\n",
    "for dataset_1, hid_1_full in data_hid_analyze.groupby(\"Dataset\"):\n",
    "    print(dataset_1)\n",
    "    hid_1 = hid_1_full.query(f\"Epoch == {last_epoch}\")\n",
    "    h_1 = hid_1.to_numpy()\n",
    "    labels_1 = hid_1.index.get_level_values(\"Input\").to_numpy()\n",
    "    outputs_1 = [problem(list(map(int, x))) for x in labels_1]\n",
    "    for dataset_2, hid_2_full in data_hid_analyze.groupby(\"Dataset\"):\n",
    "        hid_2 = hid_2_full.query(f\"Epoch == {last_epoch}\")\n",
    "        if dataset_1 == 0 and dataset_2 == 0:\n",
    "            break\n",
    "        h_2 = hid_2.to_numpy()\n",
    "        H = scipy.spatial.distance_matrix(h_1, h_2)\n",
    "        mergers = H < threshold\n",
    "\n",
    "        labels_2 = hid_2.index.get_level_values(\"Input\").to_numpy()\n",
    "        outputs_2 = [problem(list(map(int, x))) for x in labels_2]\n",
    "\n",
    "        possible_mergers = np.array(\n",
    "            [[y_1 == y_2 for y_2 in outputs_2] for y_1 in outputs_1]\n",
    "        )\n",
    "        possible_mergers_sum = int(np.sum(possible_mergers) / 2)\n",
    "        if dataset_1 == dataset_2:\n",
    "            np.fill_diagonal(mergers, False)\n",
    "            possible_mergers_sum -= len(hid_1) / 2\n",
    "        count = int(np.sum(mergers) / 2)\n",
    "        merger_counts[dataset_1, dataset_2] = count / possible_mergers_sum\n",
    "\n",
    "        diverging_mergers = mergers\n",
    "        never_splits = np.full(mergers.shape, True)\n",
    "        for epoch in range(last_epoch):\n",
    "            h_1_epoch = hid_1_full.query(f\"Epoch == {epoch}\").to_numpy()\n",
    "            h_2_epoch = hid_2_full.query(f\"Epoch == {epoch}\").to_numpy()\n",
    "            H_epoch = scipy.spatial.distance_matrix(h_1_epoch, h_2_epoch)\n",
    "            mergers_epoch = H_epoch < 10 * threshold\n",
    "            never_splits = never_splits * mergers_epoch\n",
    "        diverging_merger_count = int(np.sum(mergers * (1 - never_splits)) / 2)\n",
    "        diverging_merger_counts[dataset_1, dataset_2] = (\n",
    "            diverging_merger_count / possible_mergers_sum\n",
    "        )\n",
    "\n",
    "        total_mergers += count\n",
    "        total_possible_mergers += int(possible_mergers_sum)\n",
    "        total_pred_mergers += int(np.sum(mergers * possible_mergers) / 2)\n",
    "        total_unpred_mergers += int(np.sum(mergers * np.invert(possible_mergers)) / 2)\n",
    "        total_diverging_mergers += diverging_merger_count\n",
    "\n",
    "labels = np.arange(1, len(training_datasets) + 1)\n",
    "plt.figure(figsize=(4, 4))\n",
    "publication.set_color_gradient(0)\n",
    "plt.imshow(merger_counts)\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Sequence length\")\n",
    "plt.title(\"Fraction of possible mergers\")\n",
    "publication.im_show(\n",
    "    x_labels=labels,\n",
    "    y_labels=labels,\n",
    "    save_path=f\"plots/{settings}_mergers\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "publication.set_color_gradient(0)\n",
    "plt.imshow(diverging_merger_counts / merger_counts)\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Sequence length\")\n",
    "plt.title(\"Fraction of possible mergers\")\n",
    "publication.im_show(\n",
    "    x_labels=labels,\n",
    "    y_labels=labels,\n",
    "    save_path=f\"plots/{settings}_diverging_mergers\",\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Total mergers: {total_mergers}\")\n",
    "print(f\"Total possible mergers: {total_possible_mergers}\")\n",
    "print(f\"Total unpredicted mergers: {total_unpred_mergers}\")\n",
    "print(f\"Total predicted mergers: {total_pred_mergers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot trajectories\n",
    "\n",
    "# Get data\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace().copy()\n",
    "train_loss = loss.groupby(\"Epoch\").mean()\n",
    "train_loss = train_loss.to_numpy().ravel()\n",
    "\n",
    "epochs = list(set(data_output.index.get_level_values(\"Epoch\")))\n",
    "epochs.sort()\n",
    "\n",
    "N_pairs = 1\n",
    "\n",
    "labels = set(data_hid.index.get_level_values(\"Input\"))\n",
    "labels.remove(\"initial\")\n",
    "labels = list(labels)\n",
    "potential_labels_A = labels\n",
    "potential_labels_B = labels\n",
    "\n",
    "A_label_sim = random.sample(potential_labels_A, N_pairs)\n",
    "B_label_sim = []\n",
    "for a in A_label_sim:\n",
    "    while True:\n",
    "        b = np.random.choice(potential_labels_B)\n",
    "        final_distance = np.linalg.norm(\n",
    "            np.array(data_hid.loc[last_epoch, :, a])\n",
    "            - np.array(data_hid.loc[last_epoch, :, b])\n",
    "        )\n",
    "        if (\n",
    "            problem(list(map(int, a))) == problem(list(map(int, b)))\n",
    "            and a != b\n",
    "            and b not in B_label_sim\n",
    "            and final_distance < threshold\n",
    "        ):\n",
    "            B_label_sim.append(b)\n",
    "            break\n",
    "\n",
    "h_A_sim = [\n",
    "    np.array(data.loc[epoch, :, A_label_sim])\n",
    "    for epoch, data in data_hid.groupby(\"Epoch\")\n",
    "]\n",
    "h_B_sim = [\n",
    "    np.array(data.loc[epoch, :, B_label_sim])\n",
    "    for epoch, data in data_hid.groupby(\"Epoch\")\n",
    "]\n",
    "H_sim = np.array(\n",
    "    [np.sum((h_A_sim[epoch] - h_B_sim[epoch]) ** 2, axis=1) for epoch in epochs]\n",
    ")\n",
    "\n",
    "A_label_diff = random.sample(potential_labels_A, N_pairs)\n",
    "B_label_diff = []\n",
    "for a in A_label_diff:\n",
    "    while True:\n",
    "        b = np.random.choice(potential_labels_B)\n",
    "        if (\n",
    "            problem(list(map(int, a))) != problem(list(map(int, b)))\n",
    "            and a != b\n",
    "            and b not in B_label_diff\n",
    "        ):\n",
    "            B_label_diff.append(b)\n",
    "            break\n",
    "\n",
    "h_A_diff = [\n",
    "    np.array(data.loc[epoch, :, A_label_diff])\n",
    "    for epoch, data in data_hid.groupby(\"Epoch\")\n",
    "]\n",
    "h_B_diff = [\n",
    "    np.array(data.loc[epoch, :, B_label_diff])\n",
    "    for epoch, data in data_hid.groupby(\"Epoch\")\n",
    "]\n",
    "H_diff = np.array(\n",
    "    [np.sum((h_A_diff[epoch] - h_B_diff[epoch]) ** 2, axis=1) for epoch in epochs]\n",
    ")\n",
    "\n",
    "t = np.linspace(0, len(epochs), len(epochs))\n",
    "\n",
    "## Plot trajectories\n",
    "figsize = (3.5, 2.5)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "publication.set_color_mixed()\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(epochs, train_loss, label=\"Training loss\")\n",
    "for k, h in enumerate(np.transpose(H_sim)):\n",
    "    publication.set_color_mixed()\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    publication.skip_colors(5)\n",
    "    ax.plot(t, h / np.max(h), label=f\"{A_label_sim[k]} {B_label_sim[k]}\")\n",
    "for k, h in enumerate(np.transpose(H_diff)):\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    publication.skip_colors(5)\n",
    "    publication.skip_colors(1)\n",
    "    ax.plot(t, h / np.max(h), label=f\"{A_label_diff[k]} {B_label_diff[k]}\")\n",
    "\n",
    "# plt.xlim(300, 500)\n",
    "plt.xlabel(\"Epochs\")\n",
    "publication.plt_show(save_path=f\"plots/{settings}_trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectories\n",
    "\n",
    "# Get data\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace().copy()\n",
    "train_loss = loss.groupby(\"Epoch\").mean()\n",
    "train_loss = train_loss.to_numpy().ravel()\n",
    "\n",
    "epochs = list(set(data_output.index.get_level_values(\"Epoch\")))\n",
    "epochs.sort()\n",
    "\n",
    "N_pairs = 1000\n",
    "\n",
    "labels = set(data_hid.index.get_level_values(\"Input\"))\n",
    "labels.remove(\"initial\")\n",
    "labels = list(labels)\n",
    "\n",
    "A_label = np.random.choice(labels, size=N_pairs)\n",
    "B_label = np.random.choice(labels, size=N_pairs)\n",
    "\n",
    "H_sim, H_diff = [], []\n",
    "for k in trange(0, N_pairs):\n",
    "    a, b = A_label[k], B_label[k]\n",
    "    h_A = np.array(data_hid.loc[:, :, a])\n",
    "    h_B = np.array(data_hid.loc[:, :, b])\n",
    "    H = np.linalg.norm(h_A - h_B, axis=1)\n",
    "    if problem(list(map(int, a))) == problem(list(map(int, b))):\n",
    "        H_sim.append(H)\n",
    "    else:\n",
    "        H_diff.append(H)\n",
    "\n",
    "\n",
    "t = np.linspace(0, len(epochs), len(epochs))\n",
    "\n",
    "indicators_same, indicators_diff = [], []\n",
    "for k, h in enumerate(H_sim):\n",
    "    indicator = h > threshold\n",
    "    indicators_same.append(indicator)\n",
    "for k, h in enumerate(H_diff):\n",
    "    indicator = h > threshold\n",
    "    indicators_diff.append(indicator)\n",
    "sum_same = np.sum(np.array(indicators_same), axis=0)\n",
    "sum_diff = np.sum(np.array(indicators_diff), axis=0)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "publication.set_color_mixed()\n",
    "plt.plot(t, train_loss / np.max(train_loss), label=\"Training loss\")\n",
    "publication.skip_colors(1)\n",
    "plt.plot(t, (sum_same + sum_diff) / N_pairs, label=\"Splits total\")\n",
    "publication.skip_colors(1)\n",
    "plt.plot(t, sum_same / N_pairs, label=\"Splits same\")\n",
    "plt.plot(t, sum_diff / N_pairs, label=\"Splits diff\")\n",
    "publication.plt_show(save_path=\"plots/n_states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average representational distance over time\n",
    "plt.figure(figsize=(4, 3))\n",
    "publication.set_color_mixed()\n",
    "publication.skip_colors(5)\n",
    "plt.plot(np.mean(np.concat((np.array(H_sim), np.array(H_diff))), axis=0))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean distance\")\n",
    "publication.plt_show(save_path=\"plots/mean_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define system of ODEs for theory\n",
    "@njit\n",
    "def der(t, z, eta_h_1, eta_h_2, eta_a, eta_b, N, y11, y12, y22):\n",
    "    \"\"\"Right hand side of the ode system.\"\"\"\n",
    "    h_1, h_2, bD, by_1, by_2, b2, D2, Dy_1, Dy_2 = z\n",
    "    h_mean = (h_1 + h_2) / 2\n",
    "    h2_mean = (h_1**2 + h_2**2) / 2\n",
    "    Dy_mean = (Dy_1 + Dy_2) / 2\n",
    "    by_mean = (by_1 + by_2) / 2\n",
    "    hb_mean = (h_1 * by_1 + h_2 * by_2) / 2\n",
    "    Dhy_mean = (h_1 * Dy_1 + h_2 * Dy_2) / 2\n",
    "    hy2_mean_1 = (h_1 * y11 + h_2 * y12) / 2\n",
    "    hy2_mean_2 = (h_1 * y12 + h_2 * y22) / 2\n",
    "    return [\n",
    "        -eta_h_1 * (1 / 2) * (bD + h_1 * D2 - Dy_1),\n",
    "        -eta_h_2 * (1 / 2) * (bD + h_2 * D2 - Dy_2),\n",
    "        -eta_b * (1 / N) * (bD + h_mean * D2 - Dy_mean)\n",
    "        - eta_a * (1 / N) * (h2_mean * bD + h_mean * b2 - hb_mean),\n",
    "        -eta_b * (1 / N) * (by_1 + h_mean * Dy_1 - (y11 + y12) / 2),\n",
    "        -eta_b * (1 / N) * (by_2 + h_mean * Dy_2 - (y22 + y12) / 2),\n",
    "        -eta_b * (2 / N) * (b2 + h_mean * bD - by_mean),\n",
    "        -eta_a * (2 / N) * (h2_mean * D2 + h_mean * bD - Dhy_mean),\n",
    "        -eta_a * (1 / N) * (h2_mean * Dy_1 + h_mean * by_1 - hy2_mean_1),\n",
    "        -eta_a * (1 / N) * (h2_mean * Dy_2 + h_mean * by_2 - hy2_mean_2),\n",
    "    ]\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_train_loss(h_1, h_2, bD, by_1, by_2, b2, D2, Dy_1, Dy_2, y11, y22):\n",
    "    return 0.5 * (\n",
    "        b2\n",
    "        + D2 * (h_1**2 + h_2**2) / 2\n",
    "        + (y11 + y22) / 2\n",
    "        + 2 * bD * (h_1 + h_2) / 2\n",
    "        - 2 * (h_1 * Dy_1 + h_2 * Dy_2) / 2\n",
    "        - 2 * (by_1 + by_2) / 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot theory trajectories\n",
    "\n",
    "gain = data_hid.query(\"Epoch == 0 and Dataset == 0\").std().std()\n",
    "\n",
    "## Data variables\n",
    "N = 1\n",
    "len_1, len_2 = 1, 10\n",
    "y_1, y_2 = 1, 1\n",
    "y11, y12, y22 = y_1**2, y_1 * y_2, y_2**2\n",
    "\n",
    "## Initial parameters\n",
    "h_1_0, h_2_0, D2_0, b2_0 = (\n",
    "    np.random.normal(0, gain),\n",
    "    np.random.normal(0, gain),\n",
    "    np.random.normal(0, gain) ** 2,\n",
    "    np.random.normal(0, gain) ** 2,\n",
    ")\n",
    "theta_bD, theta_by_1, theta_by_2, theta_Dy_1, theta_Dy_2 = (\n",
    "    2 * np.random.beta(2, 2, 5) - 1\n",
    ")\n",
    "theta_bD, theta_by_1, theta_by_2, theta_Dy_1, theta_Dy_2 = (0, 0, 0, 0, 0)\n",
    "bD_0, by_1_0, by_2_0, Dy_1_0, Dy_2_0 = (\n",
    "    theta_bD * np.sqrt(b2_0 * D2_0),\n",
    "    theta_by_1 * np.sqrt(b2_0 * y_1**2),\n",
    "    theta_by_2 * np.sqrt(b2_0 * y_2**2),\n",
    "    theta_Dy_1 * np.sqrt(D2_0 * y_1**2),\n",
    "    theta_Dy_2 * np.sqrt(D2_0 * y_2**2),\n",
    ")\n",
    "\n",
    "## Effective learning rates\n",
    "eta_h_1, eta_h_2, eta_a, eta_b = (\n",
    "    lr * len_1,\n",
    "    lr * len_2,\n",
    "    lr,\n",
    "    0.25 * lr,\n",
    ")\n",
    "\n",
    "sol = scipy.integrate.solve_ivp(\n",
    "    der,\n",
    "    [0, n_epochs],\n",
    "    [h_1_0, h_2_0, bD_0, by_1_0, by_2_0, b2_0, D2_0, Dy_1_0, Dy_2_0],\n",
    "    args=(eta_h_1, eta_h_2, eta_a, eta_b, N, y11, y12, y22),\n",
    "    dense_output=True,\n",
    ")\n",
    "t = np.linspace(0, n_epochs, n_epochs)\n",
    "z = sol.sol(t)\n",
    "h_1, h_2, bD, by_1, by_2, b2, D2, Dy_1, Dy_2 = z\n",
    "train_loss = get_train_loss(h_1, h_2, bD, by_1, by_2, b2, D2, Dy_1, Dy_2, y11, y22)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "plt.plot(t, train_loss, label=\"train loss\")\n",
    "publication.skip_colors(4)\n",
    "plt.plot(t, (h_2 - h_1) ** 2, label=\"dh\")\n",
    "plt.ylim(0)\n",
    "# plt.legend()\n",
    "publication.plt_show(save_path=\"plots/theory_trajectories\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_structure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
