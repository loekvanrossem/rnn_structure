{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fae9700e970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import to_automaton_history, reduce_automaton, to_automaton\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "from visualization.automata import AutomatonAnimation, display_automata\n",
    "from visualization.epochs import EpochAnimation\n",
    "\n",
    "from model import Model\n",
    "import publication\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load settings\n",
    "settings = \"rich\"\n",
    "\n",
    "(nonlinearity, gain, lr, P, L, n_epochs, max_seq_len) = (\n",
    "    pd.read_csv(\"model settings/rnn.txt\", sep=\" \", header=0).loc[settings].to_numpy()\n",
    ")\n",
    "P, L, n_epochs, max_seq_len = int(P), int(L), int(n_epochs), int(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0000001\n",
    "n_epochs = 2000\n",
    "max_seq_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "# Define problem and data encoding\n",
    "symbols = [0, 1]\n",
    "encoding = OneHot(symbols)\n",
    "problem = lambda seq: np.sum(seq) % 2  # XOR problem\n",
    "# problem = lambda seq: (np.sum(seq) % 3) % 2\n",
    "\n",
    "# Define sequence lengths for training and validation datasets\n",
    "train_seq_lengths = list(range(1, max_seq_len + 1))\n",
    "analysis_seq_lengths = train_seq_lengths\n",
    "# analysis_seq_lengths = list(range(1, 8 + 1))\n",
    "val_seq_length = 50\n",
    "val_datapoints = 100\n",
    "\n",
    "# Generate datasets\n",
    "training_datasets = [\n",
    "    seq_data(device, problem, encoding, seq_len=length) for length in train_seq_lengths\n",
    "]\n",
    "validation_datasets = [\n",
    "    seq_data(\n",
    "        device, problem, encoding, n_datapoints=val_datapoints, seq_len=val_seq_length\n",
    "    )\n",
    "]\n",
    "analysis_data = [\n",
    "    seq_data(device, problem, encoding, seq_len=length)\n",
    "    for length in analysis_seq_lengths\n",
    "]\n",
    "tracked_datasets = validation_datasets + analysis_data + training_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loek/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import Encoding\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    encoding : Encoding\n",
    "        An encoding from input symbols to neural activities\n",
    "    input_size : int\n",
    "        The size of one input symbol\n",
    "    output_size : int\n",
    "        The size of the output\n",
    "    hidden_dim : int\n",
    "        The number of hidden neurons\n",
    "    n_layers : int\n",
    "        The number of RNN layers\n",
    "    device : device\n",
    "        The device to put the model on\n",
    "    nonlinearity : str\n",
    "        Nonlinearity used in recurrent layer\n",
    "    gain : float, default 0.1\n",
    "        The gain of the initial rnn weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding: Encoding,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        device: torch.device,\n",
    "        nonlinearity: str = \"tanh\",\n",
    "        gain: float = 0.1,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.encoding = encoding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Defining the layers\n",
    "        # self.rnn = nn.RNN(\n",
    "        #     input_size,\n",
    "        #     hidden_dim,\n",
    "        #     n_layers,\n",
    "        #     batch_first=True,\n",
    "        #     nonlinearity=nonlinearity,\n",
    "        # )\n",
    "        self.fc_in = nn.Linear(input_size, hidden_dim, bias=True)\n",
    "        self.ReLU = nn.LeakyReLU()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim, nhead=50, num_encoder_layers=4, bias=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size, bias=True)\n",
    "\n",
    "        # for par in self.parameters():\n",
    "        #     if len(par.shape) == 2:\n",
    "        #         nn.init.xavier_normal_(par, gain=gain)\n",
    "        #     if len(par.shape) == 1:\n",
    "        #         nn.init.zeros_(par)\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name and param.data.dim() == 2:\n",
    "                nn.init.kaiming_uniform_(param)\n",
    "            # if param.data.dim() == 2:\n",
    "            #     nn.init.xavier_normal_(param, gain=1)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the self and obtaining outputs\n",
    "        # y = torch.stack([y * 0] * (x.shape[1] - 1) + [y], axis=1)  # pad y with zeros\n",
    "        # out = self.transformer(x, y)\n",
    "\n",
    "        out = self.ReLU(self.fc_in(x))\n",
    "        out = self.transformer.encoder(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.ReLU(self.fc_out(out))\n",
    "\n",
    "        hidden = out\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def predict(self, x):\n",
    "        out = self(x, x)[0]\n",
    "        # out = out[:, -1, :]\n",
    "        prediction = torch.argmax(out).item()\n",
    "        return prediction\n",
    "\n",
    "    def train_step(self, optimizer: Optimizer, criterion, dataloader):\n",
    "        self.train()\n",
    "        av_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, outputs = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = self(inputs, outputs)\n",
    "\n",
    "            # output = output[:, -1, :]\n",
    "\n",
    "            # print(output)\n",
    "            # print(outputs)\n",
    "\n",
    "            loss = criterion(torch.squeeze(output), torch.squeeze(outputs))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            av_loss += loss / len(dataloader)\n",
    "        return av_loss\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    encoding=encoding,\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    hidden_dim=P,\n",
    "    n_layers=L,\n",
    "    device=device,\n",
    "    nonlinearity=nonlinearity,\n",
    "    gain=gain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup compiler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.001, amsgrad=True, weight_decay=0.00\n",
    ")\n",
    "compiler = Compiler(model, criterion, optimizer)\n",
    "# initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "# hidden_function = lambda inputs: model(inputs)[1][-1]\n",
    "output_function = lambda inputs: model(inputs)[0]\n",
    "compiler.trackers = {\n",
    "    \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    # \"hidden\": ActivationTracker(\n",
    "    #     encoding,\n",
    "    #     hidden_function,\n",
    "    #     analysis_data,\n",
    "    #     initial=lambda: initial_hidden,\n",
    "    # ),\n",
    "    # \"output\": ActivationTracker(encoding, output_function, analysis_data),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3457776606082916\n",
      "7.63991916179657\n",
      "1.6997771859169006\n",
      "1.7022745609283447\n",
      "1.7033833861351013\n",
      "1.6786170601844788\n",
      "1.6684730648994446\n",
      "1.6386250853538513\n",
      "1.549298882484436\n",
      "1.5482019484043121\n",
      "1.6395772695541382\n",
      "1.6503614783287048\n",
      "1.6542935967445374\n",
      "1.6512359976768494\n",
      "1.6563953757286072\n",
      "1.650999665260315\n",
      "1.6407458782196045\n",
      "1.6375731229782104\n",
      "1.6339682936668396\n",
      "1.6218538880348206\n",
      "1.610079824924469\n",
      "1.5957909226417542\n",
      "1.5890021324157715\n",
      "1.5752001404762268\n",
      "1.5536794662475586\n",
      "1.5328725576400757\n",
      "1.6181593835353851\n",
      "1.54664808511734\n",
      "1.5715550780296326\n",
      "1.537929743528366\n",
      "1.323664128780365\n",
      "1.4066768884658813\n",
      "1.8239650130271912\n",
      "1.391015350818634\n",
      "1.6206921935081482\n",
      "1.4324889779090881\n",
      "1.4085412621498108\n",
      "1.423372358083725\n",
      "1.5963194668293\n",
      "1.0845849215984344\n",
      "1.5248377323150635\n",
      "1.3357012867927551\n",
      "1.3718405961990356\n",
      "1.2523531317710876\n",
      "1.3439454138278961\n",
      "1.7875691056251526\n",
      "1.4513601064682007\n",
      "1.5186070501804352\n",
      "1.6382784843444824\n",
      "1.044702410697937\n",
      "1.2577766478061676\n",
      "1.3340481221675873\n",
      "1.1519176661968231\n",
      "1.3107880651950836\n",
      "1.3664598166942596\n",
      "1.302600473165512\n",
      "1.1802627742290497\n",
      "1.24602010846138\n",
      "1.2777203023433685\n",
      "1.4626545906066895\n",
      "1.2486970722675323\n",
      "1.2802873253822327\n",
      "1.1666115522384644\n",
      "1.4190237522125244\n",
      "1.4601072669029236\n",
      "1.3018074929714203\n",
      "1.163909673690796\n",
      "1.290457159280777\n",
      "1.1630593240261078\n",
      "1.3811697363853455\n",
      "1.1176600456237793\n",
      "1.3164117336273193\n",
      "1.1648395657539368\n",
      "1.2709584534168243\n",
      "1.2321017980575562\n",
      "1.1822147965431213\n",
      "1.2778663039207458\n",
      "1.0807024240493774\n",
      "1.005797117948532\n",
      "0.9177644401788712\n",
      "0.7805347144603729\n",
      "1.043081745505333\n",
      "0.9235225170850754\n",
      "1.2387049198150635\n",
      "1.2393906712532043\n",
      "1.1690943241119385\n",
      "1.2878338992595673\n",
      "1.1622092127799988\n",
      "1.4309006035327911\n",
      "1.2609180510044098\n",
      "1.1814424395561218\n",
      "1.2599762380123138\n",
      "1.0703245103359222\n",
      "1.2895364165306091\n",
      "1.2412831783294678\n",
      "1.191315621137619\n",
      "1.1968460381031036\n",
      "1.198459953069687\n",
      "1.2715759873390198\n",
      "1.3015779852867126\n",
      "1.0576640963554382\n",
      "1.1130704879760742\n",
      "1.1940295696258545\n",
      "1.106266051530838\n",
      "1.2098883986473083\n",
      "1.0900896191596985\n",
      "1.1507259607315063\n",
      "1.1391546726226807\n",
      "1.1553554832935333\n",
      "1.242398589849472\n",
      "1.1305493414402008\n",
      "1.26615971326828\n",
      "1.208040028810501\n",
      "1.1108008921146393\n",
      "1.1188108026981354\n",
      "1.199830174446106\n",
      "1.2551220059394836\n",
      "1.304107904434204\n",
      "1.1103556454181671\n",
      "1.1586239039897919\n",
      "1.207486242055893\n",
      "0.9998031258583069\n",
      "0.9878453612327576\n",
      "0.9741376042366028\n",
      "0.9529714286327362\n",
      "0.6911530941724777\n",
      "0.8472123444080353\n",
      "0.8120873868465424\n",
      "0.950698584318161\n",
      "1.1093583703041077\n",
      "0.7187956720590591\n",
      "0.9696515202522278\n",
      "0.8182293176651001\n",
      "0.9386664032936096\n",
      "0.7827024161815643\n",
      "0.6599617153406143\n",
      "0.9787798821926117\n",
      "0.6283385157585144\n",
      "0.859586626291275\n",
      "0.8069088160991669\n",
      "0.8379852175712585\n",
      "0.8235826194286346\n",
      "0.9692011177539825\n",
      "0.8061437755823135\n",
      "0.785314217209816\n",
      "0.9282547831535339\n",
      "0.6293283402919769\n",
      "0.8063724488019943\n",
      "0.837181881070137\n",
      "0.7086181193590164\n",
      "0.6568544954061508\n",
      "0.8191628158092499\n",
      "0.8876457810401917\n",
      "0.9001136124134064\n",
      "0.7283359467983246\n",
      "0.76521897315979\n",
      "0.7326931059360504\n",
      "0.7332438230514526\n",
      "0.7406238615512848\n",
      "0.9036040008068085\n",
      "0.5329395830631256\n",
      "0.6960000544786453\n",
      "0.5787924528121948\n",
      "0.6989001035690308\n",
      "0.7855620682239532\n",
      "0.7735139429569244\n",
      "0.762052372097969\n",
      "0.8374318182468414\n",
      "0.7691402435302734\n",
      "0.8229791522026062\n",
      "0.7110351026058197\n",
      "0.6541230529546738\n",
      "0.7863509356975555\n",
      "0.6816302388906479\n",
      "0.910849466919899\n",
      "0.7354088574647903\n",
      "0.8789177536964417\n",
      "0.8418879061937332\n",
      "0.8240142464637756\n",
      "0.911221832036972\n",
      "0.6865536123514175\n",
      "0.7923988401889801\n",
      "0.7902495712041855\n",
      "0.9449624121189117\n",
      "0.6300467550754547\n",
      "0.758930429816246\n",
      "0.7878486961126328\n",
      "0.7232907265424728\n",
      "0.765711098909378\n",
      "0.6974615156650543\n",
      "0.8151272535324097\n",
      "0.8663730174303055\n",
      "0.8679660856723785\n",
      "0.8796161264181137\n",
      "1.093470275402069\n",
      "0.8048869967460632\n",
      "0.8659846186637878\n",
      "0.8501446545124054\n",
      "0.7473553270101547\n",
      "0.7490713447332382\n",
      "0.8893315494060516\n",
      "0.8918139934539795\n",
      "0.997759073972702\n",
      "0.8260466158390045\n",
      "1.0161163210868835\n",
      "0.9020278453826904\n",
      "0.8471654206514359\n",
      "0.8950762152671814\n",
      "0.7848912179470062\n",
      "0.6198613345623016\n",
      "0.708415612578392\n",
      "0.6069526672363281\n",
      "0.7256119549274445\n",
      "0.8479719758033752\n",
      "0.6587416380643845\n",
      "0.8790256679058075\n",
      "0.6077004373073578\n",
      "0.844694197177887\n",
      "0.8157423138618469\n",
      "0.7871092855930328\n",
      "0.9076533019542694\n",
      "0.8396086245775223\n",
      "0.8747397363185883\n",
      "0.8181579411029816\n",
      "0.7219989746809006\n",
      "0.8871312141418457\n",
      "0.7194714248180389\n",
      "0.7147819101810455\n",
      "0.828428715467453\n",
      "0.964098334312439\n",
      "0.7627345025539398\n",
      "0.7539438754320145\n",
      "0.705877959728241\n",
      "0.7344312518835068\n",
      "0.7388418912887573\n",
      "0.8002562522888184\n",
      "0.8821548074483871\n",
      "0.7226021438837051\n",
      "0.7227502018213272\n",
      "0.7592370510101318\n",
      "0.7900377213954926\n",
      "0.7399758100509644\n",
      "0.7409893572330475\n",
      "0.7398285567760468\n",
      "0.7253864258527756\n",
      "0.7201565653085709\n",
      "0.6045264303684235\n",
      "0.7199724614620209\n",
      "0.8240774273872375\n",
      "0.8521838188171387\n",
      "0.7048397064208984\n",
      "0.7401942312717438\n",
      "0.8303817510604858\n",
      "0.7503994554281235\n",
      "0.8303856253623962\n",
      "0.8284903466701508\n",
      "0.7650811970233917\n",
      "0.8014670759439468\n",
      "0.8724642097949982\n",
      "0.8747600018978119\n",
      "0.8763224184513092\n",
      "0.7908527106046677\n",
      "0.779551550745964\n",
      "0.8006058037281036\n",
      "0.8274520635604858\n",
      "0.7858043611049652\n",
      "0.7848547101020813\n",
      "0.874216765165329\n",
      "0.7476706653833389\n",
      "0.8427840322256088\n",
      "0.7850097119808197\n",
      "0.7225705981254578\n",
      "1.0297557413578033\n",
      "0.7750377953052521\n",
      "0.7178100943565369\n",
      "0.7425282299518585\n",
      "0.8432018458843231\n",
      "0.7441992908716202\n",
      "0.853076159954071\n",
      "0.8735250234603882\n",
      "0.7975833863019943\n",
      "0.6402461975812912\n",
      "0.9015178382396698\n",
      "0.8282373547554016\n",
      "0.7682565450668335\n",
      "0.7180588692426682\n",
      "0.7823389321565628\n",
      "0.8758633732795715\n",
      "0.7373795062303543\n",
      "0.7981660068035126\n",
      "0.8310660421848297\n",
      "0.6640324294567108\n",
      "0.7848379462957382\n",
      "0.712531179189682\n",
      "0.7113147675991058\n",
      "0.6554765850305557\n",
      "0.8087962716817856\n",
      "0.7016251087188721\n",
      "0.7519283443689346\n",
      "0.716336190700531\n",
      "0.8778640031814575\n",
      "0.7587167769670486\n",
      "0.7874352335929871\n",
      "0.8781175017356873\n",
      "0.914926141500473\n",
      "0.8380182981491089\n",
      "0.761615440249443\n",
      "0.7512436360120773\n",
      "0.8266830295324326\n",
      "0.8726932108402252\n",
      "0.7622926086187363\n",
      "0.8126996159553528\n",
      "0.8948590606451035\n",
      "0.7623489052057266\n",
      "0.7909126877784729\n",
      "0.7921386659145355\n",
      "0.6893173605203629\n",
      "0.863898292183876\n",
      "0.7603258490562439\n",
      "0.72500379383564\n",
      "0.8132016360759735\n",
      "0.773718073964119\n",
      "0.6806980073451996\n",
      "0.7793052494525909\n",
      "0.7485839426517487\n",
      "0.7106170803308487\n",
      "0.7436726987361908\n",
      "0.8588637113571167\n",
      "0.7719873189926147\n",
      "0.6839977651834488\n",
      "0.7526557743549347\n",
      "0.7265327274799347\n",
      "0.9879656434059143\n",
      "0.8788626790046692\n",
      "0.846860945224762\n",
      "0.9152711033821106\n",
      "0.8338732719421387\n",
      "0.9191773533821106\n",
      "0.7225499749183655\n",
      "0.7490071207284927\n",
      "0.8376449197530746\n",
      "0.7333192527294159\n",
      "0.7450260818004608\n",
      "0.7873189449310303\n",
      "0.8443399220705032\n",
      "0.8664407581090927\n",
      "0.7852786034345627\n",
      "0.8438943028450012\n",
      "0.8905466794967651\n",
      "0.8192836940288544\n",
      "0.7333403527736664\n",
      "0.7933919131755829\n",
      "0.8480541706085205\n",
      "0.7808319628238678\n",
      "0.7288265228271484\n",
      "0.7813301384449005\n",
      "0.8553077280521393\n",
      "0.7684665471315384\n",
      "0.720242902636528\n",
      "0.6709459871053696\n",
      "0.7343679219484329\n",
      "0.7042259573936462\n",
      "0.680979773402214\n",
      "0.6770831495523453\n",
      "0.7432414591312408\n",
      "0.7519100159406662\n",
      "0.8704946041107178\n",
      "0.7164455950260162\n",
      "0.8117617517709732\n",
      "0.6749719232320786\n",
      "0.8138801157474518\n",
      "0.758428543806076\n",
      "0.7173791229724884\n",
      "0.7789872884750366\n",
      "0.7991067916154861\n",
      "0.64539635181427\n",
      "0.7204529196023941\n",
      "0.8218043446540833\n",
      "0.7718953490257263\n",
      "0.7422017604112625\n",
      "0.7077132910490036\n",
      "0.7264073640108109\n",
      "0.7103935778141022\n",
      "0.8524159342050552\n",
      "0.8151700645685196\n",
      "0.7931514084339142\n",
      "0.8746004402637482\n",
      "0.8153165876865387\n",
      "0.69762222468853\n",
      "0.7242111712694168\n",
      "0.7326819449663162\n",
      "0.780623197555542\n",
      "0.8762619495391846\n",
      "0.7756188064813614\n",
      "0.7772384136915207\n",
      "0.8060587048530579\n",
      "0.85735023021698\n",
      "0.7359872758388519\n",
      "0.7695928812026978\n",
      "0.7165911346673965\n",
      "0.8048681020736694\n",
      "0.7044822722673416\n",
      "0.7525727152824402\n",
      "0.7896415591239929\n",
      "0.7754551023244858\n",
      "0.7809457331895828\n",
      "0.7312440276145935\n",
      "0.7892042398452759\n",
      "0.7548105418682098\n",
      "0.763425201177597\n",
      "0.8978639841079712\n",
      "0.8291692435741425\n",
      "0.7925771474838257\n",
      "0.7414335906505585\n",
      "0.859094887971878\n",
      "0.8023753762245178\n",
      "0.7994144856929779\n",
      "0.7368935644626617\n",
      "0.7796135097742081\n",
      "0.8060951381921768\n",
      "0.6478741317987442\n",
      "0.7992098927497864\n",
      "0.8074123561382294\n",
      "0.7882810682058334\n",
      "0.7910236120223999\n",
      "0.8530887067317963\n",
      "0.7805262506008148\n",
      "0.7162429243326187\n",
      "0.9165215790271759\n",
      "0.9012041687965393\n",
      "0.8688596487045288\n",
      "0.7703246176242828\n",
      "0.6752481013536453\n",
      "0.7064620852470398\n",
      "0.7560611367225647\n",
      "0.7932064980268478\n",
      "0.7830376029014587\n",
      "0.8971335589885712\n",
      "0.7506970465183258\n",
      "0.7548403739929199\n",
      "0.7631569355726242\n",
      "0.6873445808887482\n",
      "0.7449259608983994\n",
      "0.8353566527366638\n",
      "0.7643131464719772\n",
      "0.8490090668201447\n",
      "1.066907376050949\n",
      "0.8307589292526245\n",
      "0.8648350238800049\n",
      "0.8134099394083023\n",
      "0.7861125767230988\n",
      "0.7635409086942673\n",
      "0.7747367322444916\n",
      "0.8011694550514221\n",
      "0.7344524711370468\n",
      "0.8254286050796509\n",
      "0.814953550696373\n",
      "0.8229465484619141\n",
      "0.817989319562912\n",
      "0.7491193413734436\n",
      "0.7296034246683121\n",
      "0.7280751913785934\n",
      "0.6829222589731216\n",
      "0.8392998427152634\n",
      "0.8131667673587799\n",
      "0.7964783608913422\n",
      "0.7079607546329498\n",
      "0.7913030385971069\n",
      "0.7380961775779724\n",
      "0.77805095911026\n",
      "0.8211064636707306\n",
      "0.8067703694105148\n",
      "0.6841740757226944\n",
      "0.703825831413269\n",
      "0.7527621239423752\n",
      "0.7632828056812286\n",
      "0.8021982908248901\n",
      "0.6473534852266312\n",
      "0.911420077085495\n",
      "0.8244986236095428\n",
      "0.7455353289842606\n",
      "0.8512397408485413\n",
      "0.8115070462226868\n",
      "0.7001864314079285\n",
      "0.7076803892850876\n",
      "0.8068058490753174\n",
      "0.7993167638778687\n",
      "0.7653731405735016\n",
      "0.7367280870676041\n",
      "0.7526603490114212\n",
      "0.7206078618764877\n",
      "0.9924949705600739\n",
      "0.7228344678878784\n",
      "0.7371646910905838\n",
      "0.6959268152713776\n",
      "0.7940996587276459\n",
      "0.7437621057033539\n",
      "0.7926691770553589\n",
      "0.7616098821163177\n",
      "0.7476603835821152\n",
      "0.7898817211389542\n",
      "0.7066794335842133\n",
      "0.7644785642623901\n",
      "0.8057355433702469\n",
      "0.7792864888906479\n",
      "0.8568108081817627\n",
      "0.7370548844337463\n",
      "0.7704804390668869\n",
      "0.749430701136589\n",
      "0.7771321088075638\n",
      "0.7744105607271194\n",
      "0.7244351208209991\n",
      "0.7470692545175552\n",
      "0.7373898923397064\n",
      "0.7388461828231812\n",
      "0.7983793020248413\n",
      "0.7874119281768799\n",
      "0.7678190618753433\n",
      "0.716736689209938\n",
      "0.7470749318599701\n",
      "0.8069552481174469\n",
      "0.7907798290252686\n",
      "0.7622600197792053\n",
      "0.7279880195856094\n",
      "0.9167502820491791\n",
      "0.7486772835254669\n",
      "0.6700072139501572\n",
      "0.7554299384355545\n",
      "0.7863817811012268\n",
      "0.7015828490257263\n",
      "0.7531805634498596\n",
      "0.784412294626236\n",
      "0.774434357881546\n",
      "0.6549750715494156\n",
      "0.8473946154117584\n",
      "0.7287140339612961\n",
      "0.7024757564067841\n",
      "0.7910882234573364\n",
      "0.6778101474046707\n",
      "0.7968491315841675\n",
      "0.7371741235256195\n",
      "0.7886539697647095\n",
      "0.748045951128006\n",
      "0.7319505363702774\n",
      "0.7313487082719803\n",
      "0.7903841137886047\n",
      "0.7217162400484085\n",
      "0.7308284491300583\n",
      "0.9720318019390106\n",
      "0.7533972710371017\n",
      "0.7859323620796204\n",
      "0.8416175544261932\n",
      "0.7652567923069\n",
      "0.7776269614696503\n",
      "0.8001234978437424\n",
      "0.7548228800296783\n",
      "0.7411831170320511\n",
      "0.7793179154396057\n",
      "0.8095167279243469\n",
      "0.7397024482488632\n",
      "0.9551031291484833\n",
      "0.8279053568840027\n",
      "0.7875081598758698\n",
      "0.8019585460424423\n",
      "0.7994121313095093\n",
      "0.7998893409967422\n",
      "0.7641486972570419\n",
      "0.7397930175065994\n",
      "0.7975957095623016\n",
      "0.794291079044342\n",
      "0.7723873555660248\n",
      "0.7910637855529785\n",
      "0.7767979800701141\n",
      "0.734221413731575\n",
      "0.7289977967739105\n",
      "0.8160759210586548\n",
      "0.8091478198766708\n",
      "0.6937047243118286\n",
      "0.737805962562561\n",
      "0.6151551306247711\n",
      "0.7723446190357208\n",
      "0.8009391129016876\n",
      "0.7568090856075287\n",
      "0.6938451528549194\n",
      "0.8075430989265442\n",
      "0.6624318510293961\n",
      "0.7282195836305618\n",
      "0.9169896394014359\n",
      "0.7256081104278564\n",
      "0.7638382315635681\n",
      "0.7423856109380722\n",
      "0.7271024584770203\n",
      "0.7803312093019485\n",
      "0.7091140002012253\n",
      "0.7761518359184265\n",
      "0.799472987651825\n",
      "0.8310349881649017\n",
      "0.7405582517385483\n",
      "0.8280598819255829\n",
      "0.8173669874668121\n",
      "0.7108097076416016\n",
      "0.7804596424102783\n",
      "0.6986259669065475\n",
      "0.7458609640598297\n",
      "0.7969498783349991\n",
      "0.6890785992145538\n",
      "0.7778158783912659\n",
      "0.7730153650045395\n",
      "0.6656669229269028\n",
      "0.7548762410879135\n",
      "0.7396802753210068\n",
      "0.7764113992452621\n",
      "0.7931354939937592\n",
      "0.7202921509742737\n",
      "0.7166287302970886\n",
      "0.7475112676620483\n",
      "0.8451582044363022\n",
      "0.8777481615543365\n",
      "0.823074996471405\n",
      "0.7893192023038864\n",
      "0.7766589671373367\n",
      "0.6876693218946457\n",
      "0.7614416480064392\n",
      "0.8130850493907928\n",
      "0.7925166934728622\n",
      "0.8209488391876221\n",
      "0.8273879289627075\n",
      "0.7239499539136887\n",
      "0.7039793580770493\n",
      "0.7707763612270355\n",
      "0.7786022424697876\n",
      "0.6662188172340393\n",
      "0.7393757700920105\n",
      "0.7531099617481232\n",
      "0.7889590263366699\n",
      "0.871147632598877\n",
      "0.8203919231891632\n",
      "0.6862464398145676\n",
      "0.7848448604345322\n",
      "0.7335657030344009\n",
      "0.8616799712181091\n",
      "0.7272424399852753\n",
      "0.7568583190441132\n",
      "0.7577253878116608\n",
      "0.7175929248332977\n",
      "0.7962913811206818\n",
      "0.6885776817798615\n",
      "0.738490417599678\n",
      "0.7025564908981323\n",
      "0.7741651237010956\n",
      "0.7319506704807281\n",
      "0.7933744788169861\n",
      "0.7832663655281067\n",
      "0.7495272308588028\n",
      "0.8316085487604141\n",
      "0.9033617079257965\n",
      "0.7689867913722992\n",
      "0.7686414867639542\n",
      "0.7594098746776581\n",
      "0.8284021019935608\n",
      "0.7255132347345352\n",
      "0.7552042007446289\n",
      "0.7914921641349792\n",
      "0.7760480493307114\n",
      "0.7907313406467438\n",
      "0.8203630745410919\n",
      "0.7295731008052826\n",
      "0.7902615368366241\n",
      "0.7242016941308975\n",
      "0.7143285572528839\n",
      "0.7324564307928085\n",
      "0.7800802290439606\n",
      "0.6264288425445557\n",
      "0.7187995314598083\n",
      "0.8387642502784729\n",
      "0.8636943101882935\n",
      "0.805007591843605\n",
      "0.7051285058259964\n",
      "0.7488988190889359\n",
      "0.8233802020549774\n",
      "0.8507599532604218\n",
      "0.8525263369083405\n",
      "0.840368390083313\n",
      "0.761609211564064\n",
      "0.7679950147867203\n",
      "0.6904042959213257\n",
      "0.7925475090742111\n",
      "0.6875855773687363\n",
      "0.7066455185413361\n",
      "0.7851241230964661\n",
      "0.7936344742774963\n",
      "0.8130618333816528\n",
      "0.7855269014835358\n",
      "0.8227997720241547\n",
      "0.7300695925951004\n",
      "0.7279727607965469\n",
      "0.7280777245759964\n",
      "0.7506398856639862\n",
      "0.689021036028862\n",
      "0.7787835448980331\n",
      "0.73121377825737\n",
      "0.7094215452671051\n",
      "0.8011362999677658\n",
      "0.8244893252849579\n",
      "0.8464783430099487\n",
      "0.7961126863956451\n",
      "0.799372136592865\n",
      "0.7724188268184662\n",
      "0.8118185997009277\n",
      "0.7860240340232849\n",
      "0.7936928272247314\n",
      "0.7440660148859024\n",
      "0.8058147728443146\n",
      "0.8002063632011414\n",
      "0.806730717420578\n",
      "0.7977501899003983\n",
      "0.7576338350772858\n",
      "0.7788950800895691\n",
      "0.7806978821754456\n",
      "0.7925345003604889\n",
      "0.7617069333791733\n",
      "0.7398475408554077\n",
      "0.7877196073532104\n",
      "0.7979303747415543\n",
      "0.7356036007404327\n",
      "0.7640626728534698\n",
      "0.8172397017478943\n",
      "0.7645376026630402\n",
      "0.7626598179340363\n",
      "0.8081347942352295\n",
      "0.812765508890152\n",
      "0.7409962713718414\n",
      "0.7362511605024338\n",
      "0.7750257551670074\n",
      "0.7692082971334457\n",
      "0.7556197792291641\n",
      "0.7643163502216339\n",
      "0.8225462883710861\n",
      "0.7540368288755417\n",
      "0.7276828736066818\n",
      "0.8485235273838043\n",
      "0.7578842490911484\n",
      "0.7051385194063187\n",
      "0.7679695785045624\n",
      "0.8182585537433624\n",
      "0.7672769129276276\n",
      "0.7799194306135178\n",
      "0.8398623168468475\n",
      "0.7748878598213196\n",
      "0.7987357378005981\n",
      "0.7470490634441376\n",
      "0.6878358274698257\n",
      "0.7894613444805145\n",
      "0.7836431264877319\n",
      "0.7729002982378006\n",
      "0.7603465616703033\n",
      "0.8246757388114929\n",
      "0.733979806303978\n",
      "0.749700054526329\n",
      "0.7428727000951767\n",
      "0.7642882764339447\n",
      "0.7880530506372452\n",
      "0.7635401338338852\n",
      "0.7079048901796341\n",
      "0.7374847829341888\n",
      "0.7414218485355377\n",
      "0.7505109012126923\n",
      "0.7816466689109802\n",
      "0.7432248592376709\n",
      "0.7825881242752075\n",
      "0.744571790099144\n",
      "0.7811691462993622\n",
      "0.7547130882740021\n",
      "0.7110103964805603\n",
      "0.7364669740200043\n",
      "0.747322604060173\n",
      "0.7925349771976471\n",
      "0.7605945765972137\n",
      "0.7628885954618454\n",
      "0.7412136644124985\n",
      "0.7754040658473969\n",
      "0.7229780852794647\n",
      "0.7777905911207199\n",
      "0.7940805852413177\n",
      "0.7814523428678513\n",
      "0.7492678910493851\n",
      "0.7472399324178696\n",
      "0.7150995135307312\n",
      "0.7362511605024338\n",
      "0.7972219884395599\n",
      "0.7396405339241028\n",
      "0.8103945255279541\n",
      "0.805152416229248\n",
      "0.7811949998140335\n",
      "0.8310264945030212\n",
      "0.7029287219047546\n",
      "0.7628797739744186\n",
      "0.7661543190479279\n",
      "0.7942211478948593\n",
      "0.7772195786237717\n",
      "0.7360274195671082\n",
      "0.7207011282444\n",
      "0.8283308148384094\n",
      "0.791132852435112\n",
      "0.7868641316890717\n",
      "0.777231365442276\n",
      "0.8008044362068176\n",
      "0.8068217039108276\n",
      "0.7943401038646698\n",
      "0.7496868669986725\n",
      "0.7221193611621857\n",
      "0.7573965787887573\n",
      "0.7683424055576324\n",
      "0.7438922226428986\n",
      "0.7550982385873795\n",
      "0.7542088627815247\n",
      "0.6975593864917755\n",
      "0.8091744482517242\n",
      "0.788613423705101\n",
      "0.7214846163988113\n",
      "0.7428858876228333\n",
      "0.74306221306324\n",
      "0.7854316532611847\n",
      "0.7770747542381287\n",
      "0.7421085089445114\n",
      "0.7541523575782776\n",
      "0.7144472151994705\n",
      "0.7637084722518921\n",
      "0.7532360404729843\n",
      "0.7232233285903931\n",
      "0.7446680665016174\n",
      "0.6929284930229187\n",
      "0.806177020072937\n",
      "0.7487793266773224\n",
      "0.8698437511920929\n",
      "0.7647511661052704\n",
      "0.7043090760707855\n",
      "0.7800461053848267\n",
      "0.7481765151023865\n",
      "0.6461713314056396\n",
      "0.8165724277496338\n",
      "0.8323411047458649\n",
      "0.7497683018445969\n",
      "0.6726753413677216\n",
      "0.7251556813716888\n",
      "0.7945985198020935\n",
      "0.711461216211319\n",
      "0.7842740416526794\n",
      "0.7808426469564438\n",
      "0.7403309345245361\n",
      "0.7797251045703888\n",
      "0.7436477690935135\n",
      "0.7552079558372498\n",
      "0.7536784708499908\n",
      "0.7884916961193085\n",
      "0.8106405138969421\n",
      "0.7591978907585144\n",
      "0.750372052192688\n",
      "0.7171762585639954\n",
      "0.7445191740989685\n",
      "0.7584072947502136\n",
      "0.7735104113817215\n",
      "0.68682661652565\n",
      "0.7618825137615204\n",
      "0.7298299670219421\n",
      "0.6830697953701019\n",
      "0.8336836397647858\n",
      "0.7032318562269211\n",
      "0.8147310614585876\n",
      "0.7433071583509445\n",
      "0.7220977246761322\n",
      "0.7006316632032394\n",
      "0.7011342346668243\n",
      "0.7692035734653473\n",
      "0.7089002430438995\n",
      "0.8246212899684906\n",
      "0.7314116954803467\n",
      "0.7623624205589294\n",
      "0.8295712172985077\n",
      "0.7129800021648407\n",
      "0.7816191166639328\n",
      "0.7111860662698746\n",
      "0.7490016669034958\n",
      "0.8669527173042297\n",
      "0.7747417688369751\n",
      "0.8505227863788605\n",
      "0.7158828675746918\n",
      "0.7520043849945068\n",
      "0.6833430528640747\n",
      "0.7381442934274673\n",
      "0.7324242889881134\n",
      "0.7410534471273422\n",
      "0.691180944442749\n",
      "0.7597901225090027\n",
      "0.7782657742500305\n",
      "0.8054837584495544\n",
      "0.859018474817276\n",
      "0.8612874746322632\n",
      "0.7687584757804871\n",
      "0.7145940512418747\n",
      "0.8843625783920288\n",
      "0.7496030032634735\n",
      "0.8419933021068573\n",
      "0.8114609718322754\n",
      "0.7323548644781113\n",
      "0.7916292250156403\n",
      "0.7171152383089066\n",
      "0.7877189368009567\n",
      "0.7637544423341751\n",
      "0.8478643447160721\n",
      "0.8452548384666443\n",
      "0.7359579652547836\n",
      "0.7196821123361588\n",
      "0.7365765422582626\n",
      "0.731516569852829\n",
      "0.7502452433109283\n",
      "0.7763508558273315\n",
      "0.7795795202255249\n",
      "0.7771643847227097\n",
      "0.7288216203451157\n",
      "0.7455016672611237\n",
      "0.7588271051645279\n",
      "0.7983807623386383\n",
      "0.7932932674884796\n",
      "0.7762469947338104\n",
      "0.7484070658683777\n",
      "0.7493906319141388\n",
      "0.8038929998874664\n",
      "0.738317534327507\n",
      "0.7666223645210266\n",
      "0.7099013477563858\n",
      "0.6082346066832542\n",
      "0.8225788325071335\n",
      "0.7442693412303925\n",
      "0.7240088582038879\n",
      "0.7450760155916214\n",
      "0.7381519079208374\n",
      "0.783613920211792\n",
      "0.7335600852966309\n",
      "0.693109393119812\n",
      "0.7764317393302917\n",
      "0.740703821182251\n",
      "0.7181137204170227\n",
      "0.7540780901908875\n",
      "0.7269887775182724\n",
      "0.837079256772995\n",
      "0.854696050286293\n",
      "1.0942560583353043\n",
      "0.6983270943164825\n",
      "0.7910909950733185\n",
      "0.7602701336145401\n",
      "0.7916221916675568\n",
      "0.7872672528028488\n",
      "0.8202759325504303\n",
      "0.7412117421627045\n",
      "0.7004828006029129\n",
      "0.7522455453872681\n",
      "0.6914971768856049\n",
      "0.6593524366617203\n",
      "0.7032252997159958\n",
      "0.7374023646116257\n",
      "0.7173817604780197\n",
      "0.7028250843286514\n",
      "0.7426905035972595\n",
      "0.7545270174741745\n",
      "0.7909587323665619\n",
      "0.7528761029243469\n",
      "0.756070926785469\n",
      "0.7516312599182129\n",
      "0.775373637676239\n",
      "0.7129591703414917\n",
      "0.7012477815151215\n",
      "0.8631762266159058\n",
      "0.8246737718582153\n",
      "0.8105433285236359\n",
      "0.8124508410692215\n",
      "0.7518353909254074\n",
      "0.7240580320358276\n",
      "0.7535732388496399\n",
      "0.8310938775539398\n",
      "0.7372270822525024\n",
      "0.7857779711484909\n",
      "0.7258144468069077\n",
      "0.8126721084117889\n",
      "0.789834588766098\n",
      "0.847220778465271\n",
      "0.7487172186374664\n",
      "0.8004360646009445\n",
      "0.7641470730304718\n",
      "0.7807961404323578\n",
      "0.8232139647006989\n",
      "0.7406484335660934\n",
      "0.7305277287960052\n",
      "0.8304159641265869\n",
      "0.7760635912418365\n",
      "0.7050962150096893\n",
      "0.6643430292606354\n",
      "0.7784590870141983\n",
      "0.7575892806053162\n",
      "0.7841635197401047\n",
      "0.7748051285743713\n",
      "0.7836417406797409\n",
      "0.7472580671310425\n",
      "0.7494082748889923\n",
      "0.7538917362689972\n",
      "0.7519394308328629\n",
      "0.8154148459434509\n",
      "0.7827532291412354\n",
      "0.8932363241910934\n",
      "0.7864025831222534\n",
      "0.8037428557872772\n",
      "0.7070734947919846\n",
      "0.7441981881856918\n",
      "0.7727129012346268\n",
      "0.7799171507358551\n",
      "0.7701136767864227\n",
      "0.8005560338497162\n",
      "0.7236393541097641\n",
      "0.7782875299453735\n",
      "0.7794294059276581\n",
      "0.8129172921180725\n",
      "0.7233428210020065\n",
      "0.7202754318714142\n",
      "0.7917567193508148\n",
      "0.7867285311222076\n",
      "0.7294667661190033\n",
      "0.7573274374008179\n",
      "0.7452229708433151\n",
      "0.7154626846313477\n",
      "0.7654790878295898\n",
      "0.7990820407867432\n",
      "0.7341886162757874\n",
      "0.7672224342823029\n",
      "0.8137342631816864\n",
      "0.7530923783779144\n",
      "0.7528561651706696\n",
      "0.7803130745887756\n",
      "0.7391536980867386\n",
      "0.7398764342069626\n",
      "0.8081148862838745\n",
      "0.7629527598619461\n",
      "0.7153682559728622\n",
      "0.7425825893878937\n",
      "0.7789692282676697\n",
      "0.7817716151475906\n",
      "0.8303479105234146\n",
      "0.7528775930404663\n",
      "0.6782936006784439\n",
      "0.7334522902965546\n",
      "0.7622784227132797\n",
      "0.743680939078331\n",
      "0.7280285805463791\n",
      "0.7760981023311615\n",
      "0.7839787304401398\n",
      "0.7360306680202484\n",
      "0.7310028523206711\n",
      "0.7530903816223145\n",
      "0.7685477137565613\n",
      "0.7188100218772888\n",
      "0.8480232059955597\n",
      "0.7586515247821808\n",
      "0.7440734207630157\n",
      "0.7246858775615692\n",
      "0.7770287096500397\n",
      "0.8346707373857498\n",
      "0.7459944188594818\n",
      "0.6651613563299179\n",
      "0.760320395231247\n",
      "0.925543487071991\n",
      "0.7203648388385773\n",
      "0.7572275400161743\n",
      "0.7272275984287262\n"
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    av_loss = 0\n",
    "    for dataset in training_datasets[::-1]:\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "        loss = model.train_step(optimizer, criterion, dataloader).item()\n",
    "        av_loss += loss / len(dataloader)\n",
    "    print(av_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.2735, 0.7299]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.2735, 0.7299]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.0463, 0.4782]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.0463, 0.4782]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[0.1268, 0.6649]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.1268, 0.6649]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.6569, 0.3831]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.6569, 0.3831]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[-3.0066e-04,  7.1979e-01]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[-3.0066e-04,  7.1979e-01]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.5798, 0.5708]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.5798, 0.5708]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]], device='cuda:0')\n",
      "(tensor([[0.7813, 0.2302]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[0.7813, 0.2302]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n",
      "tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "(tensor([[-0.0050,  0.6222]], device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[-0.0050,  0.6222]], device='cuda:0', grad_fn=<LeakyReluBackward0>))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(training_datasets[-1], batch_size=1)\n",
    "count = 0\n",
    "for x, y in dataloader:\n",
    "    target = torch.argmax(y).item()\n",
    "    print(x)\n",
    "    print(model(x, y))\n",
    "    prediction = model.predict(x)\n",
    "    if prediction == target:\n",
    "        count += 1\n",
    "\n",
    "accuracy = count / len(dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2000 [00:00<?, ?steps/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Transformer.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Training run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:228\u001b[0m, in \u001b[0;36mCompiler.training_run\u001b[0;34m(self, training_datasets, n_epochs, batch_size, progress_bar, conv_thresh)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tracker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m    231\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, trainloader)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m/\u001b[39m n_train_data)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trainloader, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(trainloaders, training_datasets)\n\u001b[1;32m    235\u001b[0m )\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:83\u001b[0m, in \u001b[0;36mScalarTracker.track\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# initial_hidden = model.init_hidden(batch_size=1)[-1]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# hidden_function = lambda inputs: model(inputs)[1][-1]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m output_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m compiler\u001b[38;5;241m.\u001b[39mtrackers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: ScalarTracker(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_datasets\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# \"hidden\": ActivationTracker(\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#     encoding,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#     hidden_function,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#     analysis_data,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#     initial=lambda: initial_hidden,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# \"output\": ActivationTracker(encoding, output_function, analysis_data),\u001b[39;00m\n\u001b[1;32m     19\u001b[0m }\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/compilation.py:180\u001b[0m, in \u001b[0;36mCompiler.validation\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    178\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    179\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(\n\u001b[0;32m--> 180\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39msqueeze(outputs)\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqueeze(loss_this_dataset)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([loss_this_dataset], [i])\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn_structure/lib/python3.12/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "## Training run\n",
    "compiler.training_run(\n",
    "    training_datasets,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute percentage merged as function of seq len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ON TRANSFORMER TEST BEHAVIOUR ON ARBITRAY LONG RANGE SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize automaton dynamics\n",
    "\n",
    "# Filter and process data for visualization\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "data_output = compiler.trackers[\"output\"].get_trace()\n",
    "query = \"(Dataset <10) \"\n",
    "data_hid = data_hid.query(query).copy()\n",
    "data_output = data_output.query(query).copy()\n",
    "\n",
    "epochs = list(set(data_hid.index.get_level_values(\"Epoch\")))\n",
    "epochs.sort()\n",
    "\n",
    "std = float(np.linalg.norm(data_hid.std()))\n",
    "n_points = len(data_hid.query(\"Epoch == 0\"))\n",
    "\n",
    "automaton_history = to_automaton_history(\n",
    "    data_hid, data_output, merge_distance=0.1 * std\n",
    ")\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "n_states = np.array([len(automaton.states) for automaton in automaton_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot number of reduced states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = sum(len(dataset) for dataset in analysis_data)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "# plt.plot(n_states / n_datapoints, label=\"Number of states\", zorder=0, color=\"0.5\")\n",
    "plt.plot(n_states / n_datapoints, label=\"Number of states\", zorder=0)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "if save:\n",
    "    publication.plt_show(save_path=f\"plots/automaton_dynamics/{settings}_loss\")\n",
    "else:\n",
    "    publication.plt_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = sum(len(dataset) for dataset in training_datasets)\n",
    "\n",
    "publication.set_color_mixed()\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "if save:\n",
    "    publication.plt_show(save_path=f\"plots/automaton_dynamics/{settings}_loss_small\")\n",
    "else:\n",
    "    publication.plt_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for large sequence length generalization\n",
    "\n",
    "# Evaluate model generalization for sequences of varying lengths\n",
    "N = 100\n",
    "stepsize = 1\n",
    "val_acc = []\n",
    "for n in trange(1, N + 1, stepsize):\n",
    "    val_data = seq_data(device, problem, encoding, n_datapoints=10, seq_len=n)\n",
    "    dataloader = torch.utils.data.DataLoader(val_data, batch_size=1)\n",
    "    count = 0\n",
    "    for x, y in dataloader:\n",
    "        target = torch.argmax(y).item()\n",
    "        # out = model.transformer.encoder(x)[0][:, -1, :]\n",
    "        # prediction = torch.argmax(out).item()\n",
    "        prediction = model.predict(x)\n",
    "        if prediction == target:\n",
    "            count += 1\n",
    "\n",
    "    accuracy = count / len(dataloader)\n",
    "    val_acc.append(accuracy)\n",
    "\n",
    "# Visualize validation error for varying sequence lengths\n",
    "publication.set_color_mixed()\n",
    "fig = plt.figure(figsize=(6, 2))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.bar(np.arange(1, N + 1, stepsize), val_acc, color=\"skyblue\")\n",
    "ax.set_xlabel(\"Sequence lengths\")\n",
    "ax.set_ylabel(\"Validation accuracy\")\n",
    "# ax.set_title(\"Model Generalization Across Sequence Lengths\")\n",
    "ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "if save:\n",
    "    publication.pub_show(save_path=f\"plots/automaton_dynamics/{settings}_validation\")\n",
    "else:\n",
    "    publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match settings:\n",
    "    case \"rich\":\n",
    "        epoch_choices = [0, 1000, 1999]\n",
    "    case \"intermediate\":\n",
    "        epoch_choices = [0, 350, 660, 1999]\n",
    "    case \"lazy\":\n",
    "        epoch_choices = [0, 1999]\n",
    "    case \"low_data\":\n",
    "        epoch_choices = [0, 600,1999]\n",
    "\n",
    "for epoch in epoch_choices:\n",
    "    automaton = automaton_history[epoch]\n",
    "    display_automata(automaton)\n",
    "    if save:\n",
    "        publication.pub_show(\n",
    "            save_path=f\"plots/automaton_dynamics/{settings}_automaton_epoch_{epoch}\"\n",
    "        )\n",
    "    else:\n",
    "        publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automaton = to_automaton(\n",
    "    hidden_function,\n",
    "    output_function,\n",
    "    initial_hidden,\n",
    "    training_datasets,\n",
    "    encoding,\n",
    "    merge_distance_frac=0.1,\n",
    ")\n",
    "display_automata(reduce_automaton(automaton))\n",
    "if save:\n",
    "    publication.pub_show(\n",
    "        save_path=f\"plots/automaton_dynamics/{settings}_automaton_reduced\"\n",
    "    )\n",
    "else:\n",
    "    publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation\n",
    "publication.set_color_mixed()\n",
    "animation = SliderAnimation(\n",
    "    {\n",
    "        \"Hidden representations\": ActivationsAnimation(\n",
    "            data_hid, transform=\"PCA\", colors=[5] * n_points\n",
    "        ),\n",
    "        \"Output\": ActivationsAnimation(\n",
    "            data_output,\n",
    "            transform=\"none\",\n",
    "            fixed_points=encoding.encoding,\n",
    "            colors=[6] * n_points,\n",
    "        ),\n",
    "        \"Automaton\": AutomatonAnimation(automaton_history, reduce_automata=False),\n",
    "        \"Loss\": EpochAnimation(\n",
    "            graphs={\n",
    "                \"Training loss\": train_loss,\n",
    "                \"Validation loss\": val_loss,\n",
    "            },\n",
    "            unitless_graphs={\n",
    "                \"Number of states\": n_states,\n",
    "            },\n",
    "            y_bounds=(0, 1),\n",
    "        ),\n",
    "    },\n",
    "    parameters=epochs,\n",
    "    parameter_name=\"Epoch\",\n",
    "    fig_size=4,\n",
    ")\n",
    "\n",
    "# Optionally export the animation\n",
    "# if save:\n",
    "#     animation.to_gif(f\"plots/automaton_dynamics_{settings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_hid = compiler.trackers[\"hidden\"].get_trace()\n",
    "counts = []\n",
    "# for n, h in tqdm(data_hid.groupby(\"Epoch\")):\n",
    "#     H = h.to_numpy()\n",
    "#     if n == 0:\n",
    "#         dist = scipy.spatial.distance_matrix(H, H)\n",
    "#         threshold = 0.01 * np.max(dist)\n",
    "#     clustering = AgglomerativeClustering(\n",
    "#         n_clusters=None, distance_threshold=threshold\n",
    "#     ).fit(H)\n",
    "#     count = max(clustering.labels_)\n",
    "#     counts.append(count)\n",
    "\n",
    "\n",
    "def count_states(H):\n",
    "    dist = scipy.spatial.distance_matrix(H, H)\n",
    "    fraction = 1 / dist\n",
    "    fraction = np.sum(np.nan_to_num(fraction, nan=0.0, posinf=0.0, neginf=0.0))\n",
    "    return fraction\n",
    "\n",
    "\n",
    "for n, h in tqdm(data_hid.groupby(\"Epoch\")):\n",
    "    H = h.to_numpy()\n",
    "    counts.append(count_states(H))\n",
    "\n",
    "loss = compiler.trackers[\"loss\"].get_trace()\n",
    "val_loss = loss.query(\"Dataset==0\")[0].to_numpy()\n",
    "train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean()\n",
    "# counts = scipy.ndimage.gaussian_filter(counts, 5)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.plot(np.array(counts) / (H.shape[0]))\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e02af9847f8f14625728f2f7147d07d87bda9043f1b0a8cf0822fa7c64756065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
