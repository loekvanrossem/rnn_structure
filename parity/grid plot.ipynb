{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9d4f086730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import trange,tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import to_automaton_history, reduce_automaton, to_automaton\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "from visualization.automata import AutomatonAnimation, display_automata\n",
    "from visualization.epochs import EpochAnimation\n",
    "\n",
    "from model import Model\n",
    "import publication\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearity, lr, P, L, n_epochs = \"tanh\", 0.001, 100, 1, 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(gain, seq_len):\n",
    "    ## Generate data\n",
    "\n",
    "    # Define problem and data encoding\n",
    "    symbols = [0, 1]\n",
    "    encoding = OneHot(symbols)\n",
    "    problem = lambda seq: np.sum(seq) % 2  # XOR problem\n",
    "\n",
    "    # Define sequence lengths for training and validation datasets\n",
    "    train_seq_lengths = list(range(1, seq_len + 1))\n",
    "    analysis_seq_lengths = train_seq_lengths\n",
    "    val_seq_length = 100\n",
    "    val_datapoints = 30\n",
    "\n",
    "    # Generate datasets\n",
    "    training_datasets = [\n",
    "        seq_data(device, problem, encoding, seq_len=length)\n",
    "        for length in train_seq_lengths\n",
    "    ]\n",
    "    validation_datasets = [\n",
    "        seq_data(\n",
    "            device,\n",
    "            problem,\n",
    "            encoding,\n",
    "            n_datapoints=val_datapoints,\n",
    "            seq_len=val_seq_length,\n",
    "        )\n",
    "    ]\n",
    "    analysis_data = [\n",
    "        seq_data(device, problem, encoding, seq_len=length)\n",
    "        for length in analysis_seq_lengths\n",
    "    ]\n",
    "    tracked_datasets = validation_datasets + analysis_data + training_datasets\n",
    "\n",
    "    ## Instantiate model\n",
    "    model = Model(\n",
    "        encoding=encoding,\n",
    "        input_size=2,\n",
    "        output_size=2,\n",
    "        hidden_dim=P,\n",
    "        n_layers=L,\n",
    "        device=device,\n",
    "        nonlinearity=nonlinearity,\n",
    "        gain=gain,\n",
    "    )\n",
    "\n",
    "    ## Setup compiler\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=lr, amsgrad=True, weight_decay=0\n",
    "    )\n",
    "    compiler = Compiler(model, criterion, optimizer)\n",
    "    compiler.trackers = {\n",
    "        \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    }\n",
    "\n",
    "    ## Training run\n",
    "    compiler.training_run(\n",
    "        training_datasets,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=128,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    ## Collect data\n",
    "    loss = compiler.trackers[\"loss\"].get_trace()\n",
    "    val_loss = loss.query(\"Dataset==0\")[0].to_numpy()[-1]\n",
    "    train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean().to_numpy()[-1]\n",
    "    initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "    hidden_function = lambda inputs: model(inputs)[1][-1]\n",
    "    output_function = lambda inputs: model(inputs)[0]\n",
    "    automaton = to_automaton(\n",
    "        hidden_function,\n",
    "        output_function,\n",
    "        initial_hidden,\n",
    "        training_datasets,\n",
    "        encoding,\n",
    "        merge_distance_frac=0.1,\n",
    "    )\n",
    "    n_states = len(automaton.states)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(validation_datasets[0], batch_size=128)\n",
    "    correct = 0\n",
    "    for x, y in val_loader:\n",
    "        predictions = torch.argmax(model(x)[0], axis=1)\n",
    "        targets = torch.argmax(y, axis=1)\n",
    "        correct += sum(predictions == targets)\n",
    "    val_acc = (correct / len(validation_datasets[0])).item()\n",
    "\n",
    "    return train_loss, val_loss, val_acc,n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m seq_lens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, N_len, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      5\u001b[0m gain_data, seq_len_data, train_loss_data, val_loss_data,val_acc_data, n_states_data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m     [],\n\u001b[1;32m      7\u001b[0m     [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     [],\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gain \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(gains):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq_len \u001b[38;5;129;01min\u001b[39;00m seq_lens:\n\u001b[1;32m     17\u001b[0m         train_loss, val_loss, val_acc,n_states \u001b[38;5;241m=\u001b[39m trial(gain, seq_len)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "N_gain, N_len = 10, 10\n",
    "\n",
    "gains = np.linspace(0.1, 2.0, N_gain)\n",
    "seq_lens = np.linspace(1, 10, N_len, dtype=int)\n",
    "gain_data, seq_len_data, train_loss_data, val_loss_data,val_acc_data, n_states_data = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for gain in tqdm(gains):\n",
    "    for seq_len in seq_lens:\n",
    "\n",
    "        train_loss, val_loss, val_acc,n_states = trial(gain, seq_len)\n",
    "        for data, item in zip(\n",
    "            (gain_data, seq_len_data, train_loss_data, val_loss_data,val_acc_data, n_states_data),\n",
    "            (gain, seq_len, train_loss, val_loss,val_acc, n_states),\n",
    "        ):\n",
    "            data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, name in zip(\n",
    "    (gain_data, seq_len_data, train_loss_data, val_loss_data,val_acc_data, n_states_data),\n",
    "    (\"gain\", \"seq_len\", \"train_loss\", \"val_loss\",\"val_acc\", \"n_states\"),\n",
    "):\n",
    "    np.save(\"plots/\" + name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states_total = np.array([ sum(2**i for i in range(len)) for len in seq_len_data])\n",
    "n_states_norm = n_states_data/n_states_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot results\n",
    "publication.set_color_gradient(0)\n",
    "\n",
    "train_loss_grid = np.array(train_loss_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(train_loss_grid)\n",
    "publication.im_show(colorbar=True, x_labels=seq_lens,y_labels=np.around(gains, 1),save_path=\"plots/train_loss_grid\")\n",
    "\n",
    "val_loss_grid = np.array(val_loss_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(val_loss_grid)\n",
    "publication.im_show(colorbar=True, x_labels=seq_lens,y_labels=np.around(gains, 1),save_path=\"plots/val_loss_grid\")\n",
    "\n",
    "val_acc_grid = np.array(val_acc_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(val_acc_grid)\n",
    "publication.im_show(colorbar=True, x_labels=seq_lens,y_labels=np.around(gains, 1),save_path=\"plots/val_acc_grid\")\n",
    "\n",
    "n_states_grid = np.array(n_states_norm).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(n_states_grid)\n",
    "publication.im_show(colorbar=True, x_labels=np.around(gains, 1), y_labels=seq_lens,save_path=\"plots/n_states_grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(val_loss_data, n_states_data)\n",
    "publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, name in zip(\n",
    "#     (gain_data, seq_len_data, train_loss_data, val_loss_data,val_acc_data, n_states_data),\n",
    "#     (\"gain\", \"seq_len\", \"train_loss\", \"val_loss\",\"val_acc\", \"n_states\"),\n",
    "# ):\n",
    "#     data = np.load(f\"plots/{name}.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_structure",
   "language": "python",
   "name": "rnn_structure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e02af9847f8f14625728f2f7147d07d87bda9043f1b0a8cf0822fa7c64756065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
