{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f11fc026e90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "\n",
    "from data import seq_data\n",
    "from preprocessing import OneHot\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "from data_analysis.automata import (\n",
    "    to_automaton_history,\n",
    "    reduce_automaton,\n",
    "    to_automaton,\n",
    "    has_all_transitions,\n",
    ")\n",
    "from visualization.animation import SliderAnimation\n",
    "from visualization.activations import ActivationsAnimation\n",
    "from visualization.automata import AutomatonAnimation, display_automata\n",
    "from visualization.epochs import EpochAnimation\n",
    "\n",
    "from model import Model\n",
    "import publication\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearity, lr, P, L, n_epochs = \"tanh\", 0.0003, 100, 1, 12500\n",
    "\n",
    "N_gain, N_p = 9, 9\n",
    "max_seq_len = 8\n",
    "max_gain = 2.0\n",
    "max_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = 0.000000001\n",
    "lr = 0.001\n",
    "n_epochs = 500\n",
    "P = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gain = 0.000000001\n",
    "# lr = 0.0003\n",
    "# n_epochs = 5000\n",
    "# P = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(gain, p):\n",
    "    ## Generate data\n",
    "\n",
    "    # Define problem and data encoding\n",
    "    symbols = [0, 1]\n",
    "    encoding = OneHot(symbols)\n",
    "    problem = lambda seq: np.sum(seq) % 2  # XOR problem\n",
    "\n",
    "    # Define sequence lengths for training and validation datasets\n",
    "    train_seq_lengths = list(range(1, max_seq_len + 1))\n",
    "    val_seq_length = 100\n",
    "    val_datapoints = 30\n",
    "    analysis_seq_lengths = list(range(1, max_seq_len + 1))\n",
    "\n",
    "    # Generate datasets\n",
    "    # training_datasets = [\n",
    "    #     seq_data(device, problem, encoding, seq_len=length)\n",
    "    #     for length in train_seq_lengths\n",
    "    # ]\n",
    "    training_datasets = []\n",
    "    for length in range(1, max_seq_len + 1):\n",
    "        n = np.random.binomial(2**length, p)\n",
    "        data = seq_data(device, problem, encoding, seq_len=length, n_datapoints=n)\n",
    "        training_datasets.append(data)\n",
    "    validation_datasets = [\n",
    "        seq_data(\n",
    "            device,\n",
    "            problem,\n",
    "            encoding,\n",
    "            n_datapoints=val_datapoints,\n",
    "            seq_len=val_seq_length,\n",
    "        )\n",
    "    ]\n",
    "    analysis_data = [\n",
    "        seq_data(device, problem, encoding, seq_len=length)\n",
    "        for length in analysis_seq_lengths\n",
    "    ]\n",
    "    tracked_datasets = validation_datasets + analysis_data + training_datasets\n",
    "\n",
    "    ## Instantiate model\n",
    "    model = Model(\n",
    "        encoding=encoding,\n",
    "        input_size=2,\n",
    "        output_size=2,\n",
    "        hidden_dim=P,\n",
    "        n_layers=L,\n",
    "        device=device,\n",
    "        nonlinearity=nonlinearity,\n",
    "        gain=gain,\n",
    "    )\n",
    "\n",
    "    ## Setup compiler\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=lr, amsgrad=True, weight_decay=0\n",
    "    )\n",
    "    compiler = Compiler(model, criterion, optimizer)\n",
    "    compiler.trackers = {\n",
    "        \"loss\": ScalarTracker(lambda: compiler.validation(tracked_datasets)),\n",
    "    }\n",
    "\n",
    "    ## Training run\n",
    "    compiler.training_run(\n",
    "        training_datasets,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=128,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    ## Collect data\n",
    "    loss = compiler.trackers[\"loss\"].get_trace()\n",
    "    val_loss = loss.query(\"Dataset==0\")[0].to_numpy()[-1]\n",
    "    train_loss = loss.query(\"Dataset>0\").groupby(\"Epoch\").mean().to_numpy()[-1]\n",
    "    initial_hidden = model.init_hidden(batch_size=1)[-1]\n",
    "    hidden_function = lambda inputs: model(inputs)[1][-1]\n",
    "    output_function = lambda inputs: model(inputs)[0]\n",
    "    automaton = to_automaton(\n",
    "        hidden_function,\n",
    "        output_function,\n",
    "        initial_hidden,\n",
    "        analysis_data,\n",
    "        encoding,\n",
    "        merge_distance_frac=0.1,\n",
    "    )\n",
    "    n_states = len(automaton.states)\n",
    "    is_finite = automaton.is_finite()\n",
    "    n_exit = sum(\n",
    "        not has_all_transitions(state, automaton.transition_function)\n",
    "        for state in automaton.states\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(validation_datasets[0], batch_size=128)\n",
    "    correct = 0\n",
    "    for x, y in val_loader:\n",
    "        predictions = torch.argmax(model(x)[0], axis=1)\n",
    "        targets = torch.argmax(y, axis=1)\n",
    "        correct += sum(predictions == targets)\n",
    "    val_acc = (correct / len(validation_datasets[0])).item()\n",
    "\n",
    "    return train_loss, val_loss, val_acc, n_states, is_finite, n_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/nfs/ghome/live/vanrosseml/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "\n",
      "Computing automata:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\u001b[A\n",
      "\n",
      "Computing automata:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\u001b[A\n",
      "\n",
      "Computing automata:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\u001b[A\n",
      "\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
      "\n",
      "Computing automata:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Computing automata: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "  0%|          | 0/9 [03:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gain \u001b[38;5;129;01min\u001b[39;00m tqdm(gains):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq_len \u001b[38;5;129;01min\u001b[39;00m seq_lens:\n\u001b[0;32m---> 17\u001b[0m         train_loss, val_loss, val_acc, n_states, is_finite, n_exit \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     21\u001b[0m             (\n\u001b[1;32m     22\u001b[0m                 gain_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m             (gain, seq_len, train_loss, val_loss, val_acc, n_states, is_finite, n_exit),\n\u001b[1;32m     32\u001b[0m         ):\n\u001b[1;32m     33\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(item)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mtrial\u001b[0;34m(gain, p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     22\u001b[0m     n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlength, p)\n\u001b[0;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mseq_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_datapoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     training_datasets\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m     25\u001b[0m validation_datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     seq_data(\n\u001b[1;32m     27\u001b[0m         device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/rnn_structure/parity/../source/data.py:60\u001b[0m, in \u001b[0;36mseq_data\u001b[0;34m(device, problem, encoding, n_datapoints, seq_len, symbols)\u001b[0m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m gen_rand_seq(seq_len, symbols, n_datapoints)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Compute outputs\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Prepare for torch\u001b[39;00m\n\u001b[1;32m     63\u001b[0m inputs, outputs \u001b[38;5;241m=\u001b[39m encoding(inputs), encoding(outputs)\n",
      "File \u001b[0;32m~/mambaforge/envs/rnn_structure/lib/python3.12/site-packages/numpy/lib/_shape_base_impl.py:366\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m arr \u001b[38;5;241m=\u001b[39m conv[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    365\u001b[0m nd \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m--> 366\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# arr, with the iteration axis at the end\u001b[39;00m\n\u001b[1;32m    369\u001b[0m in_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(nd))\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "gains = np.linspace(0.1, max_gain, N_gain)\n",
    "seq_lens = np.linspace(1, max_p, N_p)\n",
    "(\n",
    "    gain_data,\n",
    "    seq_len_data,\n",
    "    train_loss_data,\n",
    "    val_loss_data,\n",
    "    val_acc_data,\n",
    "    n_states_data,\n",
    "    is_finite_data,\n",
    "    n_exit_data,\n",
    ") = [[] for _ in range(8)]\n",
    "\n",
    "for gain in tqdm(gains):\n",
    "    for seq_len in seq_lens:\n",
    "\n",
    "        train_loss, val_loss, val_acc, n_states, is_finite, n_exit = trial(\n",
    "            gain, seq_len\n",
    "        )\n",
    "        for data, item in zip(\n",
    "            (\n",
    "                gain_data,\n",
    "                seq_len_data,\n",
    "                train_loss_data,\n",
    "                val_loss_data,\n",
    "                val_acc_data,\n",
    "                n_states_data,\n",
    "                is_finite_data,\n",
    "                n_exit_data,\n",
    "            ),\n",
    "            (gain, seq_len, train_loss, val_loss, val_acc, n_states, is_finite, n_exit),\n",
    "        ):\n",
    "            data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, name in zip(\n",
    "    (\n",
    "        gain_data,\n",
    "        seq_len_data,\n",
    "        train_loss_data,\n",
    "        val_loss_data,\n",
    "        val_acc_data,\n",
    "        n_states_data,\n",
    "        is_finite_data,\n",
    "        n_exit_data,\n",
    "    ),\n",
    "    (\n",
    "        \"gain\",\n",
    "        \"seq_len\",\n",
    "        \"train_loss\",\n",
    "        \"val_loss\",\n",
    "        \"val_acc\",\n",
    "        \"n_states\",\n",
    "        \"is_finite\",\n",
    "        \"n_exit\",\n",
    "    ),\n",
    "):\n",
    "    np.save(\"plots/\" + name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states_total = np.array([sum(2**i for i in range(len + 1)) for len in seq_len_data])\n",
    "n_states_norm = n_states_data / n_states_total\n",
    "\n",
    "## Plot results\n",
    "publication.set_color_gradient(2)\n",
    "\n",
    "train_loss_grid = np.array(train_loss_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(train_loss_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/train_loss_grid\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "val_loss_grid = np.array(val_loss_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(val_loss_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/val_loss_grid\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "val_acc_grid = np.array(val_acc_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(val_acc_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/val_acc_grid\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "n_states_grid = np.array(n_states_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(n_states_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/n_states_grid\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "n_exit_grid = np.array(n_exit_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(n_exit_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/n_exit_grid\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "is_finite_grid = np.array(is_finite_data).reshape(N_gain, N_len)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.imshow(is_finite_grid)\n",
    "plt.xlabel(\"Max training sequence length\")\n",
    "plt.ylabel(\"Initial weight scale\")\n",
    "# publication.im_show(\n",
    "#     colorbar=True,\n",
    "#     x_labels=seq_lens,\n",
    "#     y_labels=np.around(gains, 1),\n",
    "#     save_path=\"plots/is_finite_grid\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(n_states_data, val_acc_data)\n",
    "publication.pub_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gain_data,\n",
    "    seq_len_data,\n",
    "    train_loss_data,\n",
    "    val_loss_data,\n",
    "    val_acc_data,\n",
    "    n_states_data,\n",
    "    is_finite_data,\n",
    "    n_exit_data,\n",
    ") = [\n",
    "    np.load(f\"plots/{name}.npy\")\n",
    "    for name in (\n",
    "        \"gain\",\n",
    "        \"seq_len\",\n",
    "        \"train_loss\",\n",
    "        \"val_loss\",\n",
    "        \"val_acc\",\n",
    "        \"n_states\",\n",
    "        \"is_finite\",\n",
    "        \"n_exit\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_structure",
   "language": "python",
   "name": "rnn_structure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e02af9847f8f14625728f2f7147d07d87bda9043f1b0a8cf0822fa7c64756065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
