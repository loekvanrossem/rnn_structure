{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from tqdm import trange, tqdm\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from numba import njit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "from data import fun_data, grid_data\n",
    "from preprocessing import Direct, Encoding, OneHot\n",
    "from compilation import Compiler, Tracker, ScalarTracker, ActivationTracker\n",
    "from activations import get_activations\n",
    "from data_analysis.visualization.animation import SliderAnimation\n",
    "from data_analysis.visualization.activations import (\n",
    "    ActivationsAnimation,\n",
    "    FunctionAnimation,\n",
    "    PointAnimation,\n",
    ")\n",
    "from data_analysis.visualization.automata import AutomatonAnimation\n",
    "from data_analysis.visualization.epochs import EpochAnimation\n",
    "import data_analysis.visualization.publication as publication\n",
    "import simulate\n",
    "import two_points\n",
    "\n",
    "import models as models\n",
    "from models import MLP, CNN, ResNet\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "traj_path = \"plots/2 points/comparisons/\"\n",
    "\n",
    "publication.set_color_mixed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = pd.read_csv(\"model_settings/2 points.txt\", sep=\" \", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(setting, save=True):\n",
    "    ## Load settings\n",
    "    (\n",
    "        model_type,\n",
    "        nonlinearity,\n",
    "        gain,\n",
    "        lr,\n",
    "        P,\n",
    "        L,\n",
    "        n_epochs,\n",
    "        hidden_layer,\n",
    "        dx2,\n",
    "        dy2,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "    ) = settings.loc[setting].to_numpy()\n",
    "    model_type = getattr(models, model_type)\n",
    "    if nonlinearity == \"discontinuous\":\n",
    "        nonlinearity = simulate.Discontinuous.apply\n",
    "    elif nonlinearity == \"none\":\n",
    "        nonlinearity = None\n",
    "    else:\n",
    "        nonlinearity = getattr(torch.nn.functional, nonlinearity)\n",
    "\n",
    "    threshold = 1e-4\n",
    "    n_epochs_plot = 20000\n",
    "\n",
    "    ## Load data\n",
    "    def load_data(data_name):\n",
    "        data_path = f\"{traj_path}{data_name}.pkl\"\n",
    "        if os.path.exists(data_path):\n",
    "            with open(data_path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    etas_h_dic, etas_y_dic, G_hs_dic, G_ys_dic = [\n",
    "        load_data(name) for name in [\"etas_h\", \"etas_y\", \"G_hs\", \"G_ys\"]\n",
    "    ]\n",
    "\n",
    "    if setting in etas_h_dic.keys():\n",
    "        eta_h_opts = etas_h_dic[setting]\n",
    "        eta_y_opts = etas_y_dic[setting]\n",
    "        G_hs = G_hs_dic[setting]\n",
    "        G_ys = G_ys_dic[setting]\n",
    "        print(\"Fit parameters succesfully loaded from previous run.\")\n",
    "    else:\n",
    "        ## Fit effective learning rates\n",
    "        print(\"Could not find previous fit parameters, fitting...\")\n",
    "        eta_h_opts, eta_y_opts = [], []\n",
    "        G_hs, G_ys = [], []\n",
    "        for _ in range(10):\n",
    "            data, encoding = two_points.data_set(dx2, dy2, 1, 1, device)\n",
    "\n",
    "            model = model_type(\n",
    "                encoding=encoding,\n",
    "                input_size=1,\n",
    "                output_size=1,\n",
    "                hidden_dim=P,\n",
    "                n_hid_layers=L,\n",
    "                device=device,\n",
    "                init_std=gain,\n",
    "                non_linearity=nonlinearity,\n",
    "            )\n",
    "\n",
    "            # Compute G\n",
    "            input_1 = data[0][0]\n",
    "            input_2 = data[0][1]\n",
    "            hid_1 = model(input_1)[1][hidden_layer]\n",
    "            hid_2 = model(input_2)[1][hidden_layer]\n",
    "            pred_1 = model(input_1)[0]\n",
    "            pred_2 = model(input_2)[0]\n",
    "            input_1, input_2, hid_1, hid_2, pred_1, pred_2 = [\n",
    "                a.detach().numpy()\n",
    "                for a in (input_1, input_2, hid_1, hid_2, pred_1, pred_2)\n",
    "            ]\n",
    "            G_h = (\n",
    "                np.linalg.norm(hid_2 - hid_1) ** 2\n",
    "                / np.linalg.norm(input_2 - input_1) ** 2\n",
    "            )\n",
    "            G_y = (\n",
    "                np.linalg.norm(pred_2 - pred_1) ** 2\n",
    "                / np.linalg.norm(hid_2 - hid_1) ** 2\n",
    "            )\n",
    "\n",
    "            criterion = lambda x, y: 0.5 * nn.functional.mse_loss(x, y)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "            compiler = Compiler(model, criterion, optimizer)\n",
    "            compiler.trackers = {\n",
    "                \"loss\": ScalarTracker(lambda: compiler.validation([data])),\n",
    "                \"hidden\": ActivationTracker(\n",
    "                    model,\n",
    "                    lambda inputs: model(inputs)[1][hidden_layer],\n",
    "                    datasets=[data],\n",
    "                ),\n",
    "                \"output\": ActivationTracker(\n",
    "                    model, lambda inputs: model(inputs)[0], datasets=[data]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            h0, y0, w0 = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            with io.capture_output() as captured:\n",
    "                compiler.training_run([data], [data], n_epochs=n_epochs, batch_size=100)\n",
    "\n",
    "            if compiler.trackers[\"loss\"].get_entry(-1)[0][0] > 1e-2:\n",
    "                continue\n",
    "\n",
    "            data_hid = compiler.trackers[\"hidden\"].get_trace().copy()\n",
    "            data_output = compiler.trackers[\"output\"].get_trace().copy()\n",
    "            h_A = [\n",
    "                np.array(data.loc[epoch, 0, \"A\"])\n",
    "                for epoch, data in data_hid.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            h_B = [\n",
    "                np.array(data.loc[epoch, 0, \"B\"])\n",
    "                for epoch, data in data_hid.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            y_A = [\n",
    "                np.array(data.loc[epoch, 0, \"A\"])\n",
    "                for epoch, data in data_output.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            y_B = [\n",
    "                np.array(data.loc[epoch, 0, \"B\"])\n",
    "                for epoch, data in data_output.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            epochs = np.arange(0, len(h_A))\n",
    "            y_true_A, y_true_B = data[0][1].numpy(), data[1][1].numpy()\n",
    "            dy2 = np.sum((y_true_B - y_true_A) ** 2)\n",
    "            h2 = np.array([np.sum((h_A[epoch] - h_B[epoch]) ** 2) for epoch in epochs])\n",
    "            y2 = np.array([np.sum((y_A[epoch] - y_B[epoch]) ** 2) for epoch in epochs])\n",
    "            w = np.array(\n",
    "                [\n",
    "                    y2[epoch] - np.dot(y_true_A - y_true_B, y_A[epoch] - y_B[epoch])\n",
    "                    for epoch in epochs\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            eta_h_opt, eta_y_opt = simulate.optimize_eta(h2, y2, w, dx2, dy2)\n",
    "\n",
    "            G_hs.append(G_h)\n",
    "            G_ys.append(G_y)\n",
    "            eta_h_opts.append(eta_h_opt)\n",
    "            eta_y_opts.append(eta_y_opt)\n",
    "\n",
    "        ## Save data\n",
    "        datas = [eta_h_opts, eta_y_opts, G_hs, G_ys]\n",
    "        data_dics = [etas_h_dic, etas_y_dic, G_hs_dic, G_ys_dic]\n",
    "        data_names = [\"etas_h\", \"etas_y\", \"G_hs\", \"G_ys\"]\n",
    "        for data, data_dic in zip(datas, data_dics):\n",
    "            data_dic[setting] = data\n",
    "\n",
    "        for data_dic, data_name in zip(data_dics, data_names):\n",
    "            data_path = f\"{traj_path}{data_name}.pkl\"\n",
    "            with open(data_path, \"wb\") as f:\n",
    "                pickle.dump(data_dic, f)\n",
    "\n",
    "    eta_h, eta_y = np.mean(eta_h_opts), np.mean(eta_y_opts)\n",
    "    G_h, G_y = np.mean(G_hs), np.mean(G_ys)\n",
    "\n",
    "    print(f\"eta_h, eta_y, G_h, G_y = {eta_h}, {eta_y}, {G_h}, {G_y}\")\n",
    "\n",
    "    dx2 = 1\n",
    "\n",
    "    h0s, y0s, w0s, hs, ys, ws = [], [], [], [], [], []\n",
    "    dy2s = []\n",
    "\n",
    "    N = 50\n",
    "    m = 1\n",
    "\n",
    "    print(\"Computing dy plot...\")\n",
    "    for dy in np.linspace(0, 1, N):\n",
    "        dy2 = dy**2\n",
    "        variables = []\n",
    "        for _ in range(m):\n",
    "            ## Generate data\n",
    "            data, encoding = two_points.data_set(dx2, dy2, 1, 1, device)\n",
    "\n",
    "            ## Instantiate model\n",
    "            model = model_type(\n",
    "                encoding=encoding,\n",
    "                input_size=1,\n",
    "                output_size=1,\n",
    "                hidden_dim=P,\n",
    "                n_hid_layers=L,\n",
    "                device=device,\n",
    "                init_std=gain,\n",
    "                non_linearity=nonlinearity,\n",
    "            )\n",
    "\n",
    "            criterion = lambda x, y: 0.5 * nn.functional.mse_loss(x, y)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr / 3)\n",
    "            compiler = Compiler(model, criterion, optimizer)\n",
    "            compiler.trackers = {\n",
    "                \"loss\": ScalarTracker(lambda: compiler.validation([data]))\n",
    "            }\n",
    "\n",
    "            h0, y0, w0 = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            ## Training run\n",
    "            with io.capture_output() as captured:\n",
    "                compiler.training_run(\n",
    "                    [data],\n",
    "                    [],\n",
    "                    n_epochs=n_epochs_plot,\n",
    "                    batch_size=100,\n",
    "                    conv_thresh=threshold,\n",
    "                )\n",
    "\n",
    "            if compiler.trackers[\"loss\"].get_entry(-1)[0][0] > 1e-2:\n",
    "                break\n",
    "\n",
    "            h, y, w = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            variables.append([h0, y0, w0, h, y, w])\n",
    "\n",
    "        if len(variables) > 0:\n",
    "            variables = np.mean(np.array(variables), axis=0)\n",
    "            for i, array in enumerate((h0s, y0s, w0s, hs, ys, ws)):\n",
    "                array.append(variables[i])\n",
    "            dy2s.append(dy2)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            fig = plt.figure(figsize=(3, 3))\n",
    "            plt.scatter(np.sqrt(dy2s), hs)\n",
    "            plt.xlabel(\"$||y_2-y_1||$\")\n",
    "            plt.ylabel(\"$||dh||^2$\")\n",
    "            plt.ylim(0, 1.1 * max(hs))\n",
    "            A_low = np.sqrt(eta_h / eta_y) * np.sqrt(dy2s) * np.sqrt(dx2)\n",
    "            A_high = (G_h - G_y * eta_h / eta_y) * dx2\n",
    "            plt.plot(\n",
    "                np.sqrt(dy2s),\n",
    "                0.5 * A_high + np.sqrt(0.25 * A_high**2 + A_low**2),\n",
    "                linestyle=\"--\",\n",
    "                label=\"Theory\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            publication.plt_show(colors=\"contrast\")\n",
    "\n",
    "    def plot_y():\n",
    "        A_low = np.sqrt(eta_h / eta_y) * np.sqrt(dy2s) * np.sqrt(dx2)\n",
    "        A_high = (G_h - G_y * eta_h / eta_y) * dx2\n",
    "        fig = plt.figure(figsize=(2.5, 2.5))\n",
    "        plt.scatter(np.sqrt(dy2s), hs)\n",
    "        plt.xlabel(\"$||y_2-y_1||$\")\n",
    "        plt.ylabel(\"$||dh||^2$\")\n",
    "        plt.ylim(0, 1.1 * max(hs))\n",
    "        plt.plot(\n",
    "            np.sqrt(dy2s),\n",
    "            0.5 * A_high + np.sqrt(0.25 * A_high**2 + A_low**2),\n",
    "            linestyle=\"--\",\n",
    "            label=\"Theory\",\n",
    "        )\n",
    "\n",
    "    plot_y()\n",
    "    publication.plt_show(\n",
    "        colors=\"contrast\",\n",
    "        save_path=traj_path + \"dh_vs_y_\" + setting + \"_no_legend\" + \".png\",\n",
    "    )\n",
    "    plot_y()\n",
    "    plt.legend()\n",
    "    publication.plt_show(\n",
    "        colors=\"contrast\",\n",
    "        save_path=traj_path + \"dh_vs_y_\" + setting + \".png\",\n",
    "    )\n",
    "\n",
    "    dy2 = 0.5\n",
    "\n",
    "    h0s, y0s, w0s, hs, ys, ws = [], [], [], [], [], []\n",
    "    dx2s = []\n",
    "\n",
    "    N = 50\n",
    "    m = 1\n",
    "\n",
    "    print(\"Computing dx plot...\")\n",
    "    for dx in np.linspace(0.2, 1, N):\n",
    "        dx2 = dx**2\n",
    "        variables = []\n",
    "        for _ in range(m):\n",
    "            ## Generate data\n",
    "            data, encoding = two_points.data_set(dx2, dy2, 1, 1, device)\n",
    "\n",
    "            ## Instantiate model\n",
    "            model = model_type(\n",
    "                encoding=encoding,\n",
    "                input_size=1,\n",
    "                output_size=1,\n",
    "                hidden_dim=P,\n",
    "                n_hid_layers=L,\n",
    "                device=device,\n",
    "                init_std=gain,\n",
    "                non_linearity=nonlinearity,\n",
    "            )\n",
    "\n",
    "            criterion = lambda x, y: 0.5 * nn.functional.mse_loss(x, y)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr / 3)\n",
    "            compiler = Compiler(model, criterion, optimizer)\n",
    "            compiler.trackers = {\n",
    "                \"loss\": ScalarTracker(lambda: compiler.validation([data]))\n",
    "            }\n",
    "\n",
    "            h0, y0, w0 = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            ## Training run\n",
    "            with io.capture_output() as captured:\n",
    "                compiler.training_run(\n",
    "                    [data],\n",
    "                    [],\n",
    "                    n_epochs=n_epochs_plot,\n",
    "                    batch_size=100,\n",
    "                    conv_thresh=threshold,\n",
    "                )\n",
    "\n",
    "            if compiler.trackers[\"loss\"].get_entry(-1)[0][0] > 1e-2:\n",
    "                break\n",
    "\n",
    "            h, y, w = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            variables.append([h0, y0, w0, h, y, w])\n",
    "\n",
    "        if len(variables) > 0:\n",
    "            variables = np.mean(np.array(variables), axis=0)\n",
    "            for i, array in enumerate((h0s, y0s, w0s, hs, ys, ws)):\n",
    "                array.append(variables[i])\n",
    "            dx2s.append(dx2)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            fig = plt.figure(figsize=(2.5, 2.5))\n",
    "            plt.scatter(np.sqrt(dx2s), hs)\n",
    "            plt.xlabel(\"$||x_2-x_1||$\")\n",
    "            plt.ylabel(\"$||dh||^2$\")\n",
    "            plt.ylim(0, 1.1 * max(hs))\n",
    "            A_low = np.sqrt(eta_h / eta_y) * np.sqrt(dy2) * np.sqrt(dx2s)\n",
    "            A_high = (G_h - G_y * eta_h / eta_y) * dx2\n",
    "            plt.plot(\n",
    "                np.sqrt(dx2s),\n",
    "                0.5 * A_high + np.sqrt(0.25 * A_high**2 + A_low**2),\n",
    "                linestyle=\"--\",\n",
    "                label=\"Theory\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            publication.plt_show()\n",
    "\n",
    "    def plot_x():\n",
    "        fig = plt.figure(figsize=(2.5, 2.5))\n",
    "        plt.scatter(np.sqrt(dx2s), hs)\n",
    "        plt.xlabel(\"$||x_2-x_1||$\")\n",
    "        plt.ylabel(\"$||dh||^2$\")\n",
    "        plt.ylim(0, 1.1 * max(hs))\n",
    "        A_low = np.sqrt(eta_h / eta_y) * np.sqrt(dy2) * np.sqrt(dx2s)\n",
    "        A_high = (G_h - G_y * eta_h / eta_y) * np.array(dx2s)\n",
    "        plt.plot(\n",
    "            np.sqrt(dx2s),\n",
    "            0.5 * A_high + np.sqrt(0.25 * A_high**2 + A_low**2),\n",
    "            linestyle=\"--\",\n",
    "            label=\"Theory\",\n",
    "        )\n",
    "\n",
    "    plot_x()\n",
    "    publication.plt_show(\n",
    "        save_path=traj_path + \"dh_vs_x_\" + setting + \"_no_legend\" + \".png\",\n",
    "    )\n",
    "    plot_x()\n",
    "    plt.legend()\n",
    "    publication.plt_show(save_path=traj_path + \"dh_vs_x_\" + setting + \".png\")\n",
    "\n",
    "    return eta_h_opts, eta_y_opts, G_hs, G_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t-----DEFAULT-----\n",
      "Could not find previous fit parameters, fitting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/loek/projects/rnn/DNN/compare theory plotter.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m setting \u001b[39min\u001b[39;00m settings\u001b[39m.\u001b[39mindex:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m-----\u001b[39m\u001b[39m{\u001b[39;00msetting\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m-----\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     eta_h_opts, eta_y_opts, G_h, G_y \u001b[39m=\u001b[39m make_plots(setting, save\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/loek/projects/rnn/DNN/compare theory plotter.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m h0, y0, w0 \u001b[39m=\u001b[39m two_points\u001b[39m.\u001b[39mget_h_y_w(data, model, hidden_layer)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mcapture_output() \u001b[39mas\u001b[39;00m captured:\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     compiler\u001b[39m.\u001b[39;49mtraining_run([data], [data], n_epochs\u001b[39m=\u001b[39;49mn_epochs, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mif\u001b[39;00m compiler\u001b[39m.\u001b[39mtrackers[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget_entry(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1e-2\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/loek/projects/rnn/DNN/compare%20theory%20plotter.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rnn/DNN/../source/compilation.py:225\u001b[0m, in \u001b[0;36mCompiler.training_run\u001b[0;34m(self, training_datasets, tracked_datasets, n_epochs, batch_size, conv_thresh)\u001b[0m\n\u001b[1;32m    223\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m trainloader, dataset \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(trainloaders, training_datasets):\n\u001b[0;32m--> 225\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_step(\n\u001b[1;32m    226\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion, trainloader\n\u001b[1;32m    227\u001b[0m     ) \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(dataset) \u001b[39m/\u001b[39m n_train_data)\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     val_loss \u001b[39m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrackers[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m         \u001b[39m.\u001b[39mget_entry(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    232\u001b[0m         \u001b[39m.\u001b[39mquery(\u001b[39m\"\u001b[39m\u001b[39mDataset==0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m         \u001b[39m.\u001b[39mto_numpy()[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m    234\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/rnn/DNN/models.py:93\u001b[0m, in \u001b[0;36mMLP.train_step\u001b[0;34m(self, optimizer, criterion, dataloader)\u001b[0m\n\u001b[1;32m     90\u001b[0m     loss \u001b[39m=\u001b[39m criterion(torch\u001b[39m.\u001b[39msqueeze(output), torch\u001b[39m.\u001b[39msqueeze(outputs))\n\u001b[1;32m     92\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 93\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     94\u001b[0m     av_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m av_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn/lib/python3.10/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     77\u001b[0m     d_p_list,\n\u001b[1;32m     78\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     79\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     80\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     81\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     83\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     85\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     86\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     88\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn/lib/python3.10/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m func(params,\n\u001b[1;32m    223\u001b[0m      d_p_list,\n\u001b[1;32m    224\u001b[0m      momentum_buffer_list,\n\u001b[1;32m    225\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    226\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    229\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    230\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnn/lib/python3.10/site-packages/torch/optim/sgd.py:265\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         d_p \u001b[39m=\u001b[39m buf\n\u001b[0;32m--> 265\u001b[0m param\u001b[39m.\u001b[39;49madd_(d_p, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "etas_h, etas_y, G_hs, G_ys = {}, {}, {}, {}\n",
    "for setting in settings.index:\n",
    "    print(f\"\\t\\t\\t\\t\\t-----{setting.upper()}-----\")\n",
    "    eta_h_opts, eta_y_opts, G_h, G_y = make_plots(setting, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
